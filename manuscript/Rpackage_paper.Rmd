---
title             : "multibridge: An R Package To Evaluate Informed Hypotheses in Binomial and Multinomial Models" 
shorttitle        : "multibridge"

author:
  - name: Alexandra Sarafoglou
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Data Curation
    #   - Formal Analysis
    #   - Funding Acquisition
    #   - Methodology
    #   - Project Administration
    #   - Software
    #   - Validation
    #   - Visualization
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing
    corresponding: yes
  - name: Frederik Aust
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Visualization
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing
  - name: Maarten Marsman
    affiliation: ' '
    # role:
    #   - Funding Acquisition
    #   - Conceptualization
    #   - Methodology
    #   - Supervision
    #   - Validation
    #   - Writing - Review & Editing
  - name: Frantisek Bartos
    affiliation: ' '
    # role:
    #   - Software
  - name: Eric-Jan Wagenmakers
    affiliation: ' '
    # role:
    #   - Funding Acquisition
    #   - Methodology
    #   - Supervision
    #   - Validation
    #   - Writing - Review & Editing
  - name: Julia M. Haaf
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Formal Analysis
    #   - Methodology
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing

affiliation:
  - id: ' '
    institution: University of Amsterdam
    
note: | 
  Correspondence concerning this article should be addressed to: Alexandra Sarafoglou, Department of Psychology, PO Box 15906, 1001 NK Amsterdam, The Netherlands, E-mail: alexandra.sarafoglou@gmail.com

abstract: |
  The \textbf{multibridge} \texttt{R} package allows a Bayesian evaluation of informed hypotheses \(\mathcal{H}_r\) applied to frequency data from an independent binomial or multinomial distribution. \textbf{multibridge} uses bridge sampling to efficiently compute Bayes factors for the following hypotheses concerning the latent category proportions \(\boldsymbol{\theta}\): (a) hypotheses that postulate equality constraints (e.g., \(\theta_1 = \theta_2 = \theta_3\)); (b) hypotheses that postulate inequality constraints (e.g., \(\theta_1 < \theta_2 < \theta_3\) or \(\theta_1 > \theta_2 > \theta_3\)); (c) hypotheses that postulate \rev{combinations} of inequality constraints and equality constraints (e.g., \(\theta_1 < \theta_2 = \theta_3\)); and (d) hypotheses that postulate \rev{combinations} of (a)--(c) (e.g., \(\theta_1 < (\theta_2 = \theta_3) , \theta_4\)). Any informed hypothesis \(\mathcal{H}_r\) may be compared against the encompassing hypothesis \(\mathcal{H}_e\) that all category proportions vary freely, or against the null hypothesis \(\mathcal{H}_0\) that all category proportions are equal. \textbf{multibridge} facilitates the fast and accurate comparison of large models with many constraints and models for which relatively little posterior mass falls in the restricted parameter space. This paper describes the underlying methodology and illustrates the use of \textbf{multibridge} through fully reproducible examples.

bibliography      : "../inst/REFERENCES.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
numbersections    : false

documentclass     : "apa6"
classoption       : "man"
biblio-style      : "apa"
output            : papaja::apa6_pdf
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{nicefrac}
   - \usepackage{caption}
   - \usepackage{xcolor}
   - \definecolor{mypink}{RGB}{255, 230, 255}
   - \definecolor{myWheat}{RGB}{245, 222, 179}
   - \definecolor{myGreen}{RGB}{27, 158, 119}
   - \usepackage{todonotes}
   - \usepackage[toc]{appendix}
   - \newcommand{\Julia}[1]{\todo[inline, color=mypink]{#1}}
   - \newcommand{\Frederik}[1]{\todo[inline, color=myWheat]{#1}}
   - \newcommand{\Alex}[1]{\todo[inline, color=myGreen]{#1}}
   - \newcommand{\rev}[1]{\textcolor{blue}{#1}}
---

```{r, echo = FALSE, warning=FALSE}
library(plyr)
library(knitr)
library(stats)
library(ggplot2)
library(RColorBrewer)
library(multibridge)

.correlation_plot <- function(x=x, 
                              y=y, 
                              yrange=c(-1, 1),
                              xrange=c(-1, 1), 
                              binwidth = 0.05,
                              xlab=expression(theta[1]), ylab= expression(theta[2]),
                              title= 'Plot', 
                              alpha=1/2,
                              col = '#03A89E'){
  
  data <- data.frame(x=x,  y=y)

  blankPlot <- ggplot() +
    geom_blank(aes(1,1)) +
    theme(line = element_blank(),
          text  = element_blank(),
          title = element_blank(),
          plot.background = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())
  
  scatterP <- ggplot(data, aes(x=x, y=y)) +
    geom_point(color="black", fill=col, shape=21, size=2, alpha=alpha) +
    theme_classic() +
    scale_x_continuous(name = xlab, limits= xrange) +
    scale_y_continuous(name = ylab, limits= yrange) +
    geom_segment(aes(y=-Inf,yend=-Inf,x=xrange[1],xend=xrange[2])) +
    geom_segment(aes(y=yrange[1],yend=yrange[2],x=-Inf,xend=-Inf)) +
    theme(axis.line=element_blank()) +
    theme(legend.position="none", plot.margin = unit(c(0,0,2,2), "lines"), 
          axis.text.x=element_text(size=14),  
          axis.text.y=element_text(size=14), 
          axis.title.x=element_text(size=14, vjust=-1.6), 
          axis.title.y=element_text(size=14, vjust=2.6)) 
  
  yHist <- ggplot(data, aes(x=y)) +
    geom_histogram(color="black", fill=col, binwidth = binwidth) +
    xlim(xrange[1],xrange[2]) +
    labs(title="", x="", y="")+
    theme_void() +
    coord_flip() + 
    theme(legend.position = "none", 
          plot.margin = unit(c(0,0,5,0), "lines")) 
  
  xHist <- ggplot(data, aes(x=x)) +
    geom_histogram(color="black", fill=col, binwidth = binwidth) +
    xlim(xrange[1],xrange[2]) +
    labs(title=title, x="", y="")+
    theme_void() +
    theme(legend.position = "none", 
          plot.margin = unit(c(0,0,0,5), "lines")) 
  
  p <- cowplot::plot_grid(xHist, blankPlot, scatterP,
                          yHist, nrow = 2, 
                          labels=c(' '), label_size = 18,
                          rel_heights = c(1, 2), rel_widths = c(2, 1))
  return(p)
}
```

# Introduction

The most common way to analyze categorical variables is to conduct either binomial tests, multinomial tests, or chi-square goodness of fit tests. These tests compare the encompassing hypothesis to a null hypothesis that all underlying category proportions are either exactly equal, or follow a specific distribution. Accordingly, these tests are suitable when theories predict either the invariance of all category proportions or specific values. For instance, chi-square goodness of fit tests are commonly used to test Benford's law, which predicts the distribution of leading digits in empirical datasets [@benford1938law; @newcomb1881note]. Often, however, the predictions that researchers are interested in are of a different kind. Consider for instance the weak-order mixture model of decision-making  [@regenwetter2012behavioral]. The theory predicts that individuals' choice preferences are weakly ordered at all times, that is, if they prefer choice \(A\) over \(B\) and \(B\) over \(C\) then they will also prefer \(A\) over \(C\) [@regenwetter2011transitivity]---a well-constrained prediction of behavior. The theory is, however, silent about the exact values of each choice preference. Hence, the standard tests that compare \(\mathcal{H}_e\) to \(\mathcal{H}_0\) are unsuited to test the derived predictions. Instead, the predictions need to be translated into an informed hypothesis \(\mathcal{H}_r\) that reflects the predicted ordinal relations among the parameters. Only then is it possible to adequately test whether the theory of weakly-ordered preference describes participants' choice behavior. Of course, researchers may be interested in more complex hypotheses, including ones that feature combinations of equality constraints, inequality constraints, and unconstrained category proportions. For instance, @nuijten2016prevalence hypothesized that articles published in social psychology journals would have higher error rates than articles published in other psychology journals. As in the previous example, the authors had no expectations about the exact error rate distribution across journals. Here, again, the standard tests are inadequate. Generally, by specifying informed hypotheses researchers and practitioners are able to ``add theoretical expectations to the traditional alternative hypothesis'' [@hoijtink2008bayesian, p. 2] and thus test hypotheses that relate more closely to their theories [@haaf2019capturngPreprint; @rijkeboer2008psychologists].

In the Bayesian framework, researchers may test hypotheses of interest by means of Bayes factors [@jeffreys1935some; @kass1995bayes]. Bayes factors quantify the extent to which the data change the prior model odds to the posterior model odds, that is, the extent to which one hypothesis outpredicts the other. Specifically, Bayes factors are the ratio of marginal likelihoods of the respective hypotheses. For instance, the Bayes factor for the informed hypothesis versus the encompassing hypothesis is defined as:
\begin{align*}
\text{BF}_{re} = \cfrac{\overbrace{p(\mathbf{x}\mid \mathcal{H}_r)}^{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_r$}}}}{\underbrace{p(\mathbf{x}\mid \mathcal{H}_e)}_{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_e$}}}},
\end{align*}
where the subscript \(r\) denotes the informed hypothesis and \(e\) denotes the encompassing hypothesis. Several available \texttt{R} packages compute Bayes factors for informed hypotheses. For instance, the package \textbf{multinomineq} [@heck2019multinomial] evaluates informed hypotheses for multinomial models as well as models that feature independent binomials. The package \textbf{BFpack} [@mulderBfpackInPress] evaluates informed hypotheses for statistical models such as univariate and multivariate normal linear models, generalized linear models, special cases of linear mixed models, survival models, and relational event models. The package \textbf{BAIN} [@gu2019bain] evaluates informed hypotheses for structural equation models. Outside of \texttt{R}, the Fortran 90 program \textbf{BIEMS} [@mulder2012biems] evaluates informed hypotheses for multivariate linear models such as MANOVA, repeated measures, and multivariate regression. All these packages rely on one of two implementations of the encompassing prior approach [@klugkist2005bayesian; @sedransk1985bayesian] to approximate order constrained Bayes factors: the unconditional encompassing method [@klugkist2005bayesian ; @hoijtink2008bayesian; @hoijtink2011informative] and the conditional encompassing method [@gu2014bayesian; @laudy2006bayesian; @mulder2009bayesian; @mulder2014prior; @mulder2016bayes]. Even though the encompassing prior approach is currently the most common method to evaluate informed hypotheses, it becomes increasingly unreliable and inefficient as the number of restrictions increases or the parameter space of the restricted model decreases [@sarafoglou2020evaluatingPreprint]. \rev{For instance, simulation studies conducted by @sarafoglou2020evaluatingPreprint have illustrated that the encompassing prior approach is not able to produce Bayes factors when hypotheses with a large number of constrained parameters are considered (i.e., they considered 18 categories). For hypotheses with less number of categories (i.e., 5 or 6), the method worked well when the data were not extreme and provided either weak or moderate evidence in favor of or against the informed hypothesis. However, when the data provided extreme evidence against the predicted constraints, the method again failed to compute Bayes factors.}

As alternative to the encompassing prior approach, @sarafoglou2020evaluatingPreprint recently proposed a bridge sampling routine [@bennett1976efficient; @meng1996simulating] that computes Bayes factors for informed hypotheses more reliably and efficiently. This routine is implemented in \textbf{multibridge} (\url{https://CRAN.R-project.org/package=multibridge}) and is suitable to evaluate inequality constraints for multinomial and binomial models \rev{as well as combinations between equality and inequality constraints}. 

\rev{Here we showcase how the proposed bridge sampling routine by} @sarafoglou2020evaluatingPreprint \rev{can be performed with \textbf{multibridge}. In the remainder of this article, we will introduce the package and its functionalities and describe the methods used to compute the informed hypotheses in binomial and multinomial models. We will illustrate its core functions using three examples and end with a brief discussion and future directions.} 

# Multibridge

The general workflow of \textbf{multibridge} is illustrated in Figure \ref{fig:scheme-multibridge}. The core functions of \textbf{multibridge}, that is \(\texttt{mult\_bf\_informed}\) and \(\texttt{binom\_bf\_informed}\), return the Bayes factor estimate in favor of or against the informed hypothesis. To compute a Bayes factor, the core functions require the observed counts, the informed hypothesis, the parameters of the prior distribution under \(\mathcal{H}_e\), and the category labels. An overview of \rev{the basic required} arguments \rev{of the two core functions} are provided in Table \ref{table:arguments}.

\rev{When calling \texttt{mult\_bf\_informed} or \texttt{binom\_bf\_informed}, the user specifies the data values (\texttt{x} and \texttt{n} for binomial models and \texttt{x} for multinomial models, respectively), the informed hypothesis (\texttt{Hr}), the \(\alpha\) and \(\beta\) parameters of the binomial prior distributions (\texttt{a} and \texttt{b}) or the concentration parameters for the Dirichlet prior distribution (\texttt{a}), respectively, and the category labels of the factor levels (\texttt{factor\_levels}). The functions then return the estimated Bayes factor for the informed hypothesis relative to the encompassing \rev{hypothesis that imposes no constraints on the category proportions} or the null hypothesis \rev{which states that all category proportions are equal}. Based on these results different S3 methods can be used to get more detailed information on the individual components. For instance, users can extract the Bayes factor with the \texttt{bayes\_factor}-method,} visualize the posterior parameter estimates under the encompassing hypothesis using the \texttt{plot}-method, or get more detailed information on how the Bayes factor is composed using the \texttt{summary}-method. Table \ref{table:s3_methods} summarizes all S3 methods currently available in \textbf{multibridge}.

(ref:scheme-multibridge-caption) The \textbf{multibridge} workflow. The functions \texttt{mult\_bf\_informed} or \texttt{binom\_bf\_informed} return the estimated Bayes factor for the informed hypothesis relative to the encompassing or the null hypothesis. Based on these results different S3 methods can be used to get more detailed information on the individual components of the analysis (e.g., \texttt{summary}, \texttt{bayes\_factor}), and parameter estimates of the encompassing distribution (\texttt{plot}).
```{r scheme-multibridge, fig.cap='(ref:scheme-multibridge-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/scheme-multibridge.png", auto_pdf = TRUE)
``` 

## Supported Hypotheses

\rev{The following hypotheses are supported in \textbf{multibridge}. Users can test hypotheses on ordinal relations and equality constraints among parameters (left panel in Figure \ref{fig:hypotheses}). Additionally, \textbf{multibridge} supports the computation of Bayes factors for multiple independent constraints (middle panel in Figure \ref{fig:hypotheses}), for instance, the simultaneous evaluation of inequality constraints on the first three category proportions and an equality constraints on the fifth and sixth category proportion. The package also supports the evaluation of combinations of equality constraints, inequality constraints, and free parameters (right panel in Figure \ref{fig:hypotheses}). As an example, consider an ordinal hypothesis that identifies a smallest and a largest parameter, and equates the remaining parameters.}

\rev{When} an informed hypothesis includes \rev{combinations} of equality and inequality constraints, the core functions in \textbf{multibridge} split the hypothesis to compute Bayes factors separately for imposed equality constraints (for which the Bayes factor has an analytic solution) and inequality constraints (for which the Bayes factor is estimated using bridge sampling). \rev{Hence, f}or hypotheses that \rev{include combinations of} equality and inequality constraints the \texttt{bayes\_factor} method separately returns the Bayes factor for the equality constraints and the conditional Bayes factor for the inequality constraints given the equality constraints. 

(ref:hypotheses-caption) \textbf{multibridge} supports informed hypotheses including inequality and equality constraints (left), independent hypotheses (middle), and combinations of inequality and equality constraints and free parameters (right). 
```{r hypotheses, fig.cap='(ref:hypotheses-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/supported-hyps.png", auto_pdf = TRUE)
``` 


\rev{An important requirement for the hypotheses supported in \textbf{multibridge} is that the constrained parameters share upper and lower bounds. That is, if the constraint was to be drawn as a Hasse diagram or specified as a character vector, the constrained parameters should string together like a chain, ranging from the smallest parameter to the largest. We refer to these hypotheses as "stick-hypotheses". Conversely, "branched-hypotheses", that is, hypotheses that do not share common upper and lower bounds are currently not supported in \textbf{multibridge}. Examples for branched-hypotheses are shown in Figure \ref{fig:branch}. Researchers whose theories give rise to branched-hypotheses and wish to test them can do so using one of the alternative \texttt{R} packages, for instance, \textbf{multinomineq} by} @heck2019multinomial.

(ref:branch-caption) A prerequisite of \textbf{multibridge} is that informed hypotheses can be arranged as a stick. Branched hypotheses are currently not supported in the package.
```{r branch, fig.cap='(ref:branch-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/branched-hyps.png", auto_pdf = TRUE)
``` 

 \rev{The informed hypothesis \texttt{Hr} can be conveniently specified as a string or a character vector describing the relations among the category proportions. For instance, a simple ordering of three category proportions, \(\theta_1 > \theta_2 > \theta_3 \), can be specified either as `c("t1", ">", "t2", ">", "t3")`, or as `"t1 > t2 > t3"`. To assign labels of the parameters, they must be passed to the argument \texttt{factor\_levels}. \textbf{multibridge} then assumes that the order within the category labels correspond to the order of the data vector. Alternatively, the informed hypotheses can be specified using indices (e.g., `"1 > 2 > 3"`). To avoid circularity, an index or category label can be used only once within an informed hypothesis.}
 
\begin{table}[H]
\caption{To estimate the Bayes factor in favor for or against the specified informed hypothesis, the user provides the core functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} with the basic required arguments listed below.}
\label{table:arguments}
\begin{center}
\begin{tabular}{p{4cm}p{12cm}}
        \toprule
Argument & Description \\\midrule
\texttt{x} & \texttt{numeric}. Vector with data (for multinomial models) or a vector of counts of successes, or a two-dimensional table (or matrix) with 2 columns, giving the counts of successes and failures, respectively (for binomial models).  \\
\texttt{n} &  \texttt{numeric}. Vector with counts of trials. Must be the same length as \texttt{x}. Ignored if \texttt{x} is a matrix or a table. Included only in \texttt{binom\_bf\_informed}. \\
\texttt{Hr} & \texttt{string} or \texttt{character}. String or vector with the user specified informed hypothesis. Parameters may be referenced by the specified \texttt{factor\_levels} or by numerical indices.\\
\texttt{a} & \texttt{numeric}. Vector with concentration parameters of Dirichlet distribution (for multinomial models) or $\alpha$ parameters for independent beta distributions (for binomial models). Must be the same length as \texttt{x}. Default sets all parameters to 1. \\
\texttt{b} & \texttt{numeric}. Vector with $\beta$ parameters. Must be the same length as \texttt{x}. Default sets all $\beta$ parameters to 1. Included only in \texttt{binom\_bf\_informed}.\\
\texttt{factor\_levels} &  \texttt{character}. Vector with category labels. Must be the same length as \texttt{x}.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
 
\rev{Permitted signs to specify informed hypotheses are the `"<"`-sign and `">"`-sign for inequality constraints, the `"="`-sign for equality constraints, and the `","`-sign for parameters to vary freely within a constraint. For instance, `"t1 > t2 , t3 , t4"` states that \texttt{t1} is bigger than (\texttt{t2}, \texttt{t3}, \texttt{t4}) and that no constraints are imposed among \texttt{t2}, \texttt{t3}, and \texttt{t4}, thus they vary freely. Lastly, users can connect multiple independent restrictions using the `"\&"`-sign, for instance, `"t1 > t2 > t3 \& t5 = t6"`.}

```{r, eval = FALSE}
x <- c(2, 1, 5, 4, 1, 8)
n <- c(10, 7, 13, 7, 9, 14)
a <- b <- c(1, 1, 1, 1, 1, 1)

# Testing ordinal relations and equality constraints
mult_bf_informed(x=x, a=a, Hr='1 > 2 > 3')
mult_bf_informed(x=x, a=a, Hr='1 = 2 = 3')

binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 > 2 > 3')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 = 2 = 3')

# Testing combinations of ordinal constraints, 
# equality constraints, and free parameters
mult_bf_informed(x=x, a=a, Hr='1 = 2 = 3 > 4 > 5 = 6')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 < 2 , 3 , 4 < 5 = 6')

# Testing combinations of independent constraints
mult_bf_informed(x=x, a=a, Hr='1 > 2 > 3 & 4 , 5 = 6')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 > 2 > 3 & 5 = 6')
```

\rev{When evaluating equality constraints, it should be noted that there is a difference between assuming equality of category proportions and assuming that categories can be merged, that is, the hypothesis $\mathcal{H}_r: \theta_1 = \theta_2 < \theta_3 = \theta_4$ is not the same as $\mathcal{H}_r: \theta_1 + \theta_2 < \theta_3 + \theta_4$. In the first case the hypotheses concerns four categories of which two are expected to have equal category proportions. As a result, we assign priors to each of these four categories. In the second case, the hypothesis concerns only two categories since we assume that $\theta_1$ and $\theta_2$ belong to one group and $\theta_3$ and $\theta_4$ belong to the other. Consequently, one would assign prior to only two categories instead of four. If the goal is to merge observations of different categories, one can combine the counts and use the new data to conduct the analysis on.}

```{r, eval = FALSE}
# Merging categories or setting them equal do not yield the same results
# Hr: t1 = t2 < t3 = t4
x <- c(20, 7, 5, 9)
a <- c(1, 1, 1, 1)
summary(mult_bf_informed(x=x, a=a, Hr='1 = 2 > 3 = 4'))$bf

# Hr: t1 + t2 < t3 + t4
x <- c(20 + 7, 5 + 9)
a <- c(1, 1)
summary(mult_bf_informed(x=x, a=a, Hr='1 > 2'))$bf
```

\begin{table}[H]
\caption {S3 methods available in $\textbf{multibridge}$.}
\label{table:s3_methods}
\begin{center}
\begin{tabular}{p{4cm}p{3.5cm}p{9cm}}
        \toprule
Function Name(s) & S3 Method & Description \\\midrule
$\texttt{mult\_bf\_informed}$, $\texttt{binom\_bf\_informed}$ & $\texttt{print}$ & Prints model specifications and descriptives. \\
 & $\texttt{summary}$ &  Prints and returns the Bayes factor and associated hypotheses for the full model, and all equality and inequality constraints.\\
  & $\texttt{plot}$ & Plots the posterior median and credible interval of the parameter estimates of the encompassing model. Default sets credible interval to 95\%.\\
 & $\texttt{bayes\_factor}$ & Contains all Bayes factors and log marginal likelihood estimates for inequality constraints.\\
 & $\texttt{samples}$ & Extracts prior and posterior samples from constrained densities (if bridge sampling was applied). \\
& $\texttt{bridge\_output}$  &  Extracts bridge sampling output and associated error measures.\\
& $\texttt{restriction\_list}$ & Extracts restriction list and associated informed hypothesis. \\
$\texttt{mult\_bf\_inequality}$, $\texttt{binom\_bf\_inequality}$  & $\texttt{print}$ & Prints the bridge sampling estimate for the log marginal likelihood and the corresponding percentage error. \\
& $\texttt{summary}$ & Prints and returns the bridge sampling estimate for the log marginal likelihood and associated error terms.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\rev{\textbf{multibridge} is designed such that the functions \texttt{mult\_bf\_informed} or \texttt{binom\_bf\_informed} combine most supported functionalities of the package. Other available functions compute Bayes factors for hypotheses that postulate only equality or only inequality constraints, and draw from constrained multinomial distributions and distributions of multiple independent binomials.} A list of all currently available functions and data sets is given in Table \ref{table:core_functions}.

\begin{table}[H]
\caption {Core functions available in $\textbf{multibridge}$.}
\label{table:core_functions}
\begin{center}
\begin{tabular}{p{5.5cm}p{10.5cm}}
        \toprule
Function Name(s) & Description \\\midrule
$\texttt{mult\_bf\_informed}$ & Evaluates informed hypotheses on multinomial parameters.  \\
$\texttt{mult\_bf\_inequality}$ & Estimates the marginal likelihood of a constrained prior or posterior Dirichlet distribution.  \\
$\texttt{mult\_bf\_equality}$ & Computes Bayes factor for equality constrained multinomial parameters using the standard Bayesian multinomial test.  \\
$\texttt{mult\_tsampling}$ & Samples from constrained prior or posterior Dirichlet density.\\
$ \texttt{lifestresses}, \texttt{peas}$ & Data sets associated with informed hypotheses in multinomial models.\\\midrule
$\texttt{binom\_bf\_informed}$ & Evaluates informed hypotheses on binomial parameters.  \\
$\texttt{binom\_bf\_inequality}$ & Estimates the marginal likelihood of constrained prior or posterior beta distributions.\\
$\texttt{binom\_bf\_equality}$ & Computes Bayes factor for equality constrained binomial parameters. \\
$\texttt{binom\_tsampling}$ & Samples from constrained prior or posterior beta densities.\\
$ \texttt{journals}$ & Data set associated with informed hypotheses in binomial models.\\\midrule
$ \texttt{generate\_restriction\_list}$ & Encodes the informed hypothesis.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

# Methodological Background

\rev{In this section we provide background information on the methods implemented in \textbf{multibridge}. Specifically, this section formalizes multinomial models and models that feature independent binomial probabilities, defines Bayes factors for the Bayesian multinomial and binomial test. Furthermore, the section discusses the influence of priors on the Bayes factors, illustrates how to compute posterior model probabilites and how to compare two informed hypotheses with each other, and provides a non-technical introduction into the bridge sampling routine implemented in \textbf{multibridge}. Mathematical details of the methods and principles discussed here can be found in} @sarafoglou2020evaluatingPreprint and @gronau2017tutorial.

In the binomial model, we assume that the elements in the vector of successes \textbf{x} and the elements in the vector of total number of observations \textbf{n} in the \(K\) categories follow independent binomial distributions \rev{$\textbf{x}  \sim \prod_{k = 1}^K \text{Binomial}(\theta_k, n_k)$, from which we can derive the likelihood of the data given the parameters:}
\[
p(\mathbf{x} \mid \boldsymbol{\theta}) = \prod_{k=1}^K {{n_k}\choose{x_k}}\theta_k^{x_k}(1-\theta_k)^{n_k-x_k}.
\]

The parameter vector of the binomial success probabilities \(\boldsymbol{\theta}\) contains the underlying category proportions and assume that categories are independent. Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is a vector of independent beta distributions with parameters \(\boldsymbol{\alpha}\) and \(\boldsymbol{\beta}\)\rev{, thus $\boldsymbol{\theta} \sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k)$. The prior density is given by:}
\[
p(\boldsymbol{\theta}) = \prod_{k=1}^K \frac{ \theta_k^{\alpha_k - 1}(1-\theta_k)^{\beta_k - 1}}{\text{B}(\alpha_k\text{, }\beta_k)},
\]
\rev{where B$(\alpha_k\text{, }\beta_k)$ is the beta function:}
\[
\text{B}(\alpha_k\text{, }\beta_k) = \frac{\Gamma(\alpha_k)\Gamma(\beta_k)}{\Gamma(\alpha_k + \beta_k)}.
\]

\rev{The multinomial model constitutes a generalization of the binomial model (for \(K \geq 2\)). In this model,} we assume that the vector of observations \textbf{x} in the \(K\) categories follows a multinomial distribution in which the parameters of interest, \(\boldsymbol{\theta}\), represent the underlying category proportions\rev{, thus $\textbf{x} \sim \text{Multinomial}(x_+, \boldsymbol{\theta})$, where $x_+ = \sum_{k=1}^K x_k$.}

Since the \(K\) categories are dependent, the vector of probability parameters is constrained to sum to one, such that \(\sum_{k = 1}^K (\theta_1, \cdots, \theta_K) = 1\). Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is the Dirichlet distribution with concentration parameter vector \(\boldsymbol{\alpha}\), $\boldsymbol{\theta} \sim \text{Dirichlet}(\boldsymbol{\alpha}):$
\[
p(\boldsymbol{\theta}) = \frac{1}{\text{B}(\boldsymbol{\alpha})}\, \prod_{k=1}^K\, \theta_k^{\alpha_k-1},
\]
\rev{where B$(\boldsymbol{\alpha})$ is the multivariate beta function:}
\[
\text{B}(\boldsymbol{\alpha}) = \cfrac{\prod_{k = 1}^K \Gamma(\alpha_k)}{\Gamma \left( \sum_{k = 1}^K \alpha_k \right)}. 
\]

## Developing Suitable Prior Distributions

\rev{In the binomial and multinomial model, the concentration parameters have an intuitive interpretation. In the binomial model, the parameters $\alpha_k$ can be interpreted as vector of \emph{a priori} successes that observations fall within the various categories and $\beta_k$ can be interpreted as vector of \emph{a priori} failures. Likewise, in the multinomial model, \(\alpha_k\) can be interpreted as vector of \emph{a priori} category counts. It follows, that the higher the number of concentration parameters is, the information the prior contains and the more influence it has on parameter estimation and hypothesis testing.}

\rev{To assign adequate priors for the multiple binomials and multinomial model, we recommend one of the following approaches. If researchers possess no knowledge or expectations about the plausible parameter values, a uniform distribution can be assigned across the parameter space. This prior assumes that before seeing the data, each category contains one observation, that is, all concentration parameters are set to one. A uniform prior distribution, puts equal probability mass on all permitted parameter values, similar to the adjusted priors proposed by} @heck2016adjusted \rev{(see Figure \ref{fig:prior}). In contrast to the method proposed in} @heck2016adjusted \rev{, however, \textbf{multibridge} allows priors to be set on the original scale.}

(ref:prior-caption) The development of a prior distribution should be accompanied by a visual inspection of the prior predictive. Here we display three prior distributions on two binomial probabilities that are constrained to be \(\theta_1 < \theta_2\). The uniform distribution (panel a) assigns equal mass to all permissible values of the constrained space. A symmetric prior (panel b) concentrates the mass in the center distribution. A prior describing a constraint in the opposite direction (panel c), puts most of the along the diagonal.
```{r prior, echo = FALSE, message = FALSE, fig.cap = "(ref:prior-caption)", out.width = "60%", warning=FALSE, fig.align = 'center'}
knitr::include_graphics("prior.png", auto_pdf = TRUE)
# set.seed(4491)
# x <- c(10, 10)
# n <- c(20, 20)
# a1 <- c(1, 1)
# b1 <- c(1, 1)
# a2 <- c(10, 10)
# b2 <- c(10, 10)
# a3 <- c(15, 5)
# b3 <- c(5, 15)
# factor_levels <- c('t1', 't2')
# Hr <- c('t1 < t2')
# He <- c('t1 , t2')
# N <- 1.5e3
# 
# # generate restriction list 
# inequalities1 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a1, b=b1, 
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr1 <- binom_tsampling(inequalities1, niter = N, prior=TRUE)
# 
# inequalities2 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a2, b=b2, 
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr2 <- binom_tsampling(inequalities2, niter = N, prior=TRUE)
# 
# inequalities3 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a3, b=b3, 
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr3 <- binom_tsampling(inequalities3, niter = N, prior=TRUE)
# 
# xrange <- yrange <- c(0, 1)
# 
# 
# p1 <- .correlation_plot(x = prior_samples_Hr1[, 1],
#                         y = prior_samples_Hr1[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title='t1 ~ Beta(1, 1)  t2 ~ Beta(1, 1)',
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# p2 <- .correlation_plot(x = prior_samples_Hr2[, 1],
#                         y = prior_samples_Hr2[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title='t1 ~ Beta(10, 10)  t2 ~ Beta(10, 10)',
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# p3 <- .correlation_plot(x = prior_samples_Hr3[, 1],
#                         y = prior_samples_Hr3[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title='t1 ~ Beta(5, 15)  t2 ~ Beta(15, 5)',
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# 
# p <- cowplot::plot_grid(p1, p2, p3, nrow = 3, 
#                           labels=c('a)', 'b)', 'c)'), label_size = 12,
#                         rel_heights = c(10), rel_widths = c(1))
``` 

\rev{We recommend incorporating priors knowledge into the models whenever possible. Based on theories, expert knowledge, or informed guesses, researchers often have expectations about plausible and implausible parameter values. In these cases, the prior should match these expectations} [@lee2018determining]\rev{. For instance, in the case of informed hypotheses, prior counts can be chosen to match a particular expected ordinal trend. To determine whether the chosen priors are consistent with the theory, researchers can visualize and assess prior predictive distributions, that is, the distribution of the model parameters and data patterns predicted by the priors} [@gabry2019visualization; @schad2021toward; @wagenmakers2021seven]\rev{. The developed priors should reflect the theory and make reasonable preductions, but not be too informative to influence on posterior parameter estimates.}

\rev{Furthermore, one can choose the observed category counts of previous studies as priors for the current one, as is often suggested for replication studies and referred to as ``Bayesian learning''} [e.g., @verhagen2014bayesian]\rev{. This approach constructs highly informative priors; instead of describing the new data as precisely as possible, the goal with this approach is quantify the additional knowledge gained by the new data. Finally, priors can be constructed using a fraction of the likelihood of the data while centering it on the the mean of the parameter range} [@mulder2014prior; @gu2018approximated].

## Bayes factor

\textbf{multibridge} features two different methods to compute Bayes factors: one method computes Bayes factors for equality constrained parameters \rev{(which can be computed analytically)} and one method computes Bayes factors for inequality constrained parameters \rev{(which needs to be approximated)}. In cases where informed hypotheses feature \rev{combinations} between inequality and equality constraints, \textbf{multibridge} computes the overall Bayes factor \(\text{BF}_{re}\) by multiplying the individual Bayes factors for both constraint types. This is motivated by the fact that the Bayes factor for \rev{combinations} will factor into a Bayes factor for the equality constraints and a conditional Bayes factor for the inequality constraints given the equality constraints [see @sarafoglou2020evaluatingPreprint, for the proof].

### Testing Equality Constraints

\rev{For equality constrained binomial models \textbf{multibridge} supports two kinds of null hypotheses, one which states that all parameters are equal and one which states that all parameters are equal and equal to a specific value. Both null hypotheses are tested against an encompassing hypothesis. Under the encompassing hypothesis, we specify a Beta$(\alpha_k\text{, }\beta_k)$ prior on each of the $\theta_k$ that yields the following marginal likelihood:}
\[
p(\mathbf{x} \mid \mathcal{H}_e) = \frac{\prod_{k=1}^K {{n_k}\choose{x_k}} \times \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}.
\]
<!-- The reason for this is theoretical: we believe that such hypotheses are better tested using a hierarchical structure (thus modeling the binomial probabilities as dependent). -->

\rev{Under the first null hypothesis which states that all binomial probabilities are set equal without them being constrained further, we collapse all individual Beta$(\alpha_k\text{, }\beta_k)$ priors and corrects for the change in categories; if $K$ categories are collapsed, $K-1$ is subtracted from the concentration parameters. A Beta$(1\text{, }1)$ prior on the individual category proportions thus also yields to a Beta$(1\text{, }1)$ prior when all categories are collapsed. Hence, we yield a Beta$(\alpha_+ - K - 1\text{, }\beta_+ - K - 1)$ prior on $\theta$, where $\alpha_+ = \sum_{k=1}^K \alpha_k$ and $\beta_+ = \sum_{k=1}^K \beta_k$. This yields the following marginal likelihood:}
\[
p(\mathbf{x} \mid \mathcal{H}_{01}) = \frac{ \prod_{k=1}^K{{n_k}\choose{x_k}} \times \text{B}(x_+ + \alpha_+ - K - 1\text{, }n_+-x_+ +\beta_+ - K - 1)}{\text{B}(\alpha_+ - K - 1\text{, }\beta_+ - K - 1)}.
\]

\rev{We can now compute the Bayes factor $\text{BF}_{01e}$ as follows:}
\begin{align*}
\text{BF}_{0e} &= \frac{p(\mathbf{x} \mid \mathcal{H}_0)}{p(\mathbf{x} \mid \mathcal{H}_e)} \\
&= \frac{\frac{ \prod_{k=1}^K{{n_k}\choose{x_k}} \times \text{B}(x_+ + \alpha_+ - K - 1\text{, }n_+-x_+ +\beta_+ - K - 1)}{\text{B}(\alpha_+ - K - 1\text{, }\beta_+ - K - 1)}
}{\frac{\prod_{k=1}^K {{n_k}\choose{x_k}} \times \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}} \\
&=\frac{ \prod_{k=1}^K \text{B}(x_+ + \alpha_+ - K - 1\text{, }n_+-x_+ +\beta_+ - K - 1)
}{\prod_{k=1}^K \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)} \times \frac{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}{\text{B}(\alpha_+ - K - 1\text{, }\beta_+ - K - 1)}
\end{align*}
<!-- \begin{align*} -->
<!-- \text{BF}_{0e} &= \frac{p(\mathbf{x} \mid \mathcal{H}_0)}{p(\mathbf{x} \mid \mathcal{H}_e)} \\ -->
<!-- &= \frac{\frac{\text{B}(x_+ + \alpha\text{, }n_+-x_+ +\beta)}{\text{B}(\alpha\text{, }\beta)} \times \prod_{k=1}^K{{n_k}\choose{x_k}} -->
<!-- }{\frac{ \prod_{k=1}^K {{n_k}\choose{x_k}} \times \text{B}(x_k + \alpha\text{, }n_k - x_k + \beta)}{ \text{B}(\alpha\text{, }\beta)^K}}\\ -->
<!-- &=\frac{\text{B}(\alpha\text{, }\beta)^{K - 1} \, \text{B}(x_+ + \alpha\text{, }n_+ - x_+ + \beta)}{\prod_{k=1}^K \text{B}(x_k + \alpha\text{, }n_k - x_k + \beta)} -->
<!-- \end{align*} -->

\rev{The second null hypothesis states that all binomial probabilities in a model are assumed to be exactly equal \textit{and} equal to a predicted value $\theta_0$. Under this hypothesis, the prior reduces to a single point and the marginal likelihood simplifies to the likelihood:}
\[
p(\mathbf{x} \mid \mathcal{H}_{02}) = \theta_0^{x_+}(1 - \theta_0)^{n_+ - x_+} \times \prod_{k=1}^K{{n_k}\choose{x_k}}.
\]

\rev{The Bayes factor for the second null hypothesis hypothesis is then defined as:} 
\[
\text{BF}_{02e}
= \cfrac{\prod_{k=1}^K \text{B}(\alpha_k \text{, } \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k + x_k\text{, } \beta_k + n_k - x_k)} \times \theta_{0}^{x_+} (1 - \theta_{0})^{n_+ - x_+}.
\]
Note that \textbf{multibridge} only supports the specification of one predicted value for all binomial probabilities.

```{r, eval=FALSE, echo=TRUE}
x <- c(3, 4, 10, 11)
n <- c(15, 12, 12, 12)
a <- c(1, 1, 1, 1)
b <- c(1, 1, 1, 1)
# assuming all binomial proportions are equal
binom_bf_equality(x=x, n=n, a=a, b=b)
# assuming all binomial proportions are equal 
# and equal to a predicted value
binom_bf_equality(x=x, n=n, a=a, b=b, p = 0.5)
```

\rev{For multinomial models, assuming that all category proportions in a model are equality constrained, the Bayes factor \(\text{BF}_{0e}\) is defined as:}
\[
\text{BF}_{0e} =  \frac{
 \text{B}\left(\alpha_{1}\text{, }\dots\text{, }\alpha_K\right)}{\text{B}\left(\alpha_1+x_1\text{, }\dots\text{, }\alpha_K+x_K\right)} \, \times 
\frac{\text{B}(\boldsymbol{\alpha}+\mathbf{x})}{\text{B}(\boldsymbol{\alpha})} \, \times  \prod_{k=1}^K \theta_{0k}^{x_k},
\]
\rev{where \(\theta_{0k}\) represent the predicted category proportions} [see @sarafoglou2020evaluatingPreprint for the derivation]. For multinomial models, under the null hypothesis, category probabilities can either all be set equal (i.e., all category probabilities are \(\frac{1}{K}\)) or can replaced with the user-specified predicted values.}

```{r, eval=FALSE, echo=TRUE}
x <- c(3, 4, 10, 11)
a <- c(1, 1, 1, 1)
# assuming all category proportions are exactly equal
mult_bf_equality(x=x, a=a)
# specifying predicted values
mult_bf_equality(x=x, a=a, p = c(0.1, 0.1, 0.3, 0.5))
```

### Testing Inequality Constraints

\rev{For inequality constrained binomial and multinomial models, users can specify informed hypotheses that are either tested against a null hypothesis postulating that all parameters are equal or against the encompassing hypothesis which lets all parameters free to vary. Generally, to obtain the marginal likelihood of the informed hypothesis, it is necessary to integrate over the restricted parameter space, which is difficult to compute.} As a solution to the problem of computing marginal likelihood of the informed hypothesis, @klugkist2005bayesian derived an identity that defines the Bayes factor \(\text{BF}_{re}\) as the ratio of proportions of posterior and prior parameter space consistent with the restriction. This identity forms the basis of the encompassing prior approach. Recently, @sarafoglou2020evaluatingPreprint highlighted that these proportions can be reinterpreted as the marginal likelihoods (i.e., the normalizing constants) of the constrained posterior and constrained prior distribution.}

\rev{The constrained prior distribution of the parameters subject to an informed hypothesis $\mathcal{H}_r$ take the following form:}
\[
p(\boldsymbol{\theta} \mid \mathcal{H}_r)
= \frac{p(\boldsymbol{\theta} \mid \mathcal{H}_e)\, \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)}{\int_{\mathcal{R}_e}\, p(\boldsymbol{\theta}\mid\mathcal{H}_r)\,\text{d}\boldsymbol{\theta}}.
\]

\rev{The constrained posterior distribution of the parameters under the informed hypothesis can be represented in the same way.}
\[
p(\boldsymbol{\theta} \mid \mathbf{x}\text{, }\mathcal{H}_r)
= \frac{p(\boldsymbol{\theta} \mid \mathbf{x}\text{, } \mathcal{H}_e)\, \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)}{\int_{\mathcal{R}_e}\, p(\mathbf{x} \mid \boldsymbol{\theta})\, p(\boldsymbol{\theta}\mid\mathcal{H}_r)\,\text{d}\boldsymbol{\theta}},
\]
\rev{where $\mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)$ is an indicator function that is one for parameter values in the that obey the constrained and zero otherwise. The} @klugkist2005bayesian \rev{identity is thus:} 

\begin{align}
\label{Eq:klugkistIdentity}
\text{BF}_{re} &= \cfrac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Marginal likelihood of}\\\text{constrained posterior distribution}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Marginal likelihood of}\\\text{constrained prior distribution}}}}.
\end{align}

\rev{This reformulation of the} @klugkist2005bayesian \rev{identity as a ratio of marginal likelihoods, made it possible to utilize numerical sampling methods such as bridge sampling to compute the Bayes factor. The following section provides a conceptual introduction to bridge sampling how it is used in the context of evaluating informed hypotheses.}

## Bridge Sampling Routine

\rev{The bridge sampling routine implemented in \textbf{multibridge} is a numerical method to estimate the marginal likelihood of a target density} [cf., @gronau2017tutorial; @overstall2010default]. \rev{The identity used in bridge sampling is displayed in Equation \ref{Eq:bridgeidentity}; it considers the unnormalized target density, a proposal density with known normalizing constant, and an arbitrary bridge function. The numerator in Equation \ref{Eq:bridgeidentity} describes the expected value of the unnormalized target density evaluated with samples from the proposal density. The denominator is the expected value of the proposal density and a bridge function evaluated with samples from the target density. The bridge function serves the purpose of increasing the overlap between the two densities, thus increasing the efficiency and accuracy of the method. The bridge sampling identity can then be expressed as follows:}

\begin{align}
    p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e) = \cfrac{\mathbb{E}_{g(\boldsymbol{\theta})}\left(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)h(\boldsymbol{\theta})\right)}{\mathbb{E}_{\text{prior}} \left(g(\boldsymbol{\theta})h(\boldsymbol{\theta})\right)},
    \label{Eq:bridgeidentity}
\end{align}
where the term \(h(\boldsymbol{\theta})\) refers to the bridge function proposed by @meng1996simulating, \(g(\boldsymbol{\theta})\) refers to a so-called proposal distribution, and \(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)\) \rev{is the unnormalized target density; in this case it represents} the part of the prior parameter space under the encompassing hypothesis that is in accordance with the constraint. \rev{In the conventional application of bridge sampling, the marginal likelihoods of the two competing hypotheses are estimated, that is, the marginal likelihood of the informed hypothesis and the marginal likelihood of the encompassing hypothesis. But on the basis of  Equation \ref{Eq:klugkistIdentity}, the routine implemented in \textbf{multibridge} estimates the marginal likelihoods of the restricted prior and restricted posterior densities.}

It should be noted that the bridge sampling algorithm implemented in \textbf{multibridge} is an adapted version of the algorithm implemented in the \texttt{R} package \textbf{bridgesampling} [@gronau2017bridgesampling] and allows for the specification of informed hypotheses on probability vectors.^[In addition, the function to compute the relative mean square error for bridge sampling estimates in \textbf{multibridge} is based on the code of the \texttt{error\_measures}-function from the \textbf{bridgesampling} package.]

\rev{A schematic representation of the bridge sampling routine is displayed in Figure \ref{fig:bridge}.} To estimate the marginal likelihood, bridge sampling requires samples from the target distribution, that is, the constrained Dirichlet distribution for multinomial models and constrained beta distributions for binomial models, and samples from the proposal distribution which in principle can be any distribution with a known marginal likelihood; in \textbf{multibridge} the proposal distribution is the multivariate normal distribution. Samples from the target distribution are generated using the Gibbs sampling algorithms proposed by @damien2001sampling. For binomial models, we apply the suggested Gibbs sampling algorithm for constrained beta distributions. In the case of the multinomial models, we apply an algorithm that simulates values from constrained Gamma distributions which are then transformed into Dirichlet random variables. To sample efficiently from these distributions, \textbf{multibridge} provides a \texttt{C++} implementation of this algorithm. Samples from the proposal distribution are generated using the standard \texttt{rmvnorm}-function from the \texttt{R} package \textbf{mvtnorm} [@mvtnorm].

(ref:bridge-caption) A schematic illustration of the bridge sampling routine to estimate the marginal likelihood of a constrained prior distribution. 
```{r bridge, fig.cap='(ref:bridge-caption)', out.width = "60%", message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/bridge-sampling.png", auto_pdf = TRUE)
``` 

\rev{Despite the bridge function, the} efficiency of the bridge sampling method is optimal only if the target and proposal distribution operate on the same parameter space and have sufficient overlap. We therefore probit transform the samples of the constrained distributions to move the samples from the probability space to the entire real line. Subsequently, we use half of these draws to construct the proposal distribution using the method of moments. \rev{Then, samples are drawn from the proposal density and transformed back into the probability space, ensuring that the samples correspond to the informed hypothesis. These transformed samples are then used to evaluate the unnormalized target density.}

The numerator in Equation \ref{Eq:bridgeidentity} evaluates the unnormalized density for the constrained prior distribution with samples from the proposal distribution. The denominator evaluates the normalized proposal distribution with samples from the constrained prior distribution. Using this identity, we obtain the bridge sampling estimator for the marginal likelihood of the constrained prior distribution by applying the iterative scheme proposed by @meng1996simulating. \rev{\textbf{multibridge}} then runs the iterative scheme until the tolerance criterion suggested by @gronau2017tutorial is reached. The sampling from the target and proposal distribution, the transformations and computational steps are performed automatically within the core functions of \textbf{multibridge}. The user only needs to provide the functions with the data, a prior and a specification of the informed hypothesis. As part of the standard output of \texttt{binom\_bf\_informed} and \texttt{mult\_bf\_informed}, the functions return the bridge sampling estimate for the log marginal likelihood of the target distribution, its associate relative mean square error \rev{and} the number of iterations \rev{needed to until the bridge sampling estimator reached the tolerance criterion}.

\rev{To summarize, to} implement the bridge sampling method we only need to be able to sample from the constrained densities. Crucially, when using bridge sampling, it does not matter how small the constrained parameter space is in proportion to the encompassing density. This gives the method a decisive advantage over the encompassing prior approach in terms of accuracy and efficiency especially (1) when binomial and multinomial models with moderate to high number of categories (i.e., \(K > 10\)) are evaluated and (2) when relatively little posterior mass falls in the constrained parameter space.

## Stick-Breaking Transformation

\rev{The bridge sampling routine in \textbf{multibridge} uses the multivariate normal distribution as proposal distribution, which requires moving the target distribution to the real line. Crucially, the transformation needs to retain the ordering of the parameters, that is, it needs to take into account the lower bound and the upper bound of each parameter. To meet these requirements, \textbf{multibridge} uses a probit transformation, as proposed in} @sarafoglou2020evaluatingPreprint\rev{, and subsequently transforms the elements in the parameter vector, moving from its lowest to its highest value. A schematic illustration of the stick-breaking transformation is given in Figure \ref{fig:stick}, detailed technical details of the transformation are provided in the appendix.}

(ref:stick-caption) A schematic illustration of the stick-breaking transformation for the ordered probability vector $\theta_{1} < \theta_{2} < \theta_{3}$. The stick-breaking transformation moves from the smallest to the largest value and determines the bounds of the parameters using a unit-length stick. 
```{r stick, fig.cap='(ref:stick-caption)', out.width = "60%", message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/stick-breaking.png", auto_pdf = TRUE)
``` 

\rev{To perform the transformation from a parameter vector on the real line to an ordered probability vector, we need to determine the lower and upper bound of each parameter. Consider an increasing trend of four parameters, that is, $\theta_{1} < \theta_{2} < \theta_{3} < \theta_{4}$. The lower bound for for the smallest element in the parameter vector, $\theta_{1}$, is 0. For $\theta_{2}$, $\theta_{3}$, and $\theta_{4}$ the lower bound is the preceding element in the vector. That is, the lower bound for $\theta_{2}$ is $\theta_1$, lower bound for $\theta_{3}$ is $\theta_2$, and the lower bound for $\theta_{4}$ is $\theta_3$.}

\rev{This definition holds for both binomial models and multinomial models. Differences in these two models appear only when determining the upper bound for each parameter. For binomial models, the upper bound for each parameter is $1$. For multinomial models, due to the sum-to-one constraint the upper bounds need to be computed differently. As proposed in} @frigyik2010introduction and @stan2020 \rev{we represent $\boldsymbol{\theta}$ as unit-length stick which we subsequently divide into as many elements as there are parameters in the constraint} [@frigyik2010introduction, @stan2020]\rev{. In this approach, the upper bounds are derived from on the values of smaller elements as well as on the number of remaining larger parameters in the stick. Concretely, for the smallest element in the parameter vector, $\theta_{1}$, the upper bound is $\cfrac{1}{4}$; if this element were larger than that it would be impossible to create a probability vector with increasing values. For $\theta_{2}$, $\theta_{3}$, and $\theta_{4}$ the upper bound is the proportion of the unit-length stick that has not yet been accounted for in the transformation divided by the number of parameters in the remaining stick. For instance, the upper bound for $\theta_{2}$ is defined as $\cfrac{1 - \theta_1}{3}$. This transformation allows us to effectively transform elements from the real line to an constrained probability space and is therefore a main component of the bridge sampling algorithm.}

\rev{One drawback of this transformation is, however, that it can only be performed if all parameters in the constraint can be stringed together like a chain, thus, only works for "stick-hypotheses". For hypotheses in which parameters do not share common lower and upper bounds, the assumption is violated that for a given parameter smaller elements and the number of parameters in the remaining stick determine their upper bound.}

## Poster Model Probabilites, and Bayes Factor Transitivity

\rev{Consider a scenario where one has a whole set of hypotheses that they want to compare with each other, for instance, two informed hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) as well as a null hypothesis \(\mathcal{H}_{0}\) and the encompassing hypothesis \(\mathcal{H}_{e}\).} An overview of the relative plausibility of all $M=4$ models simultaneously may be obtained by presenting the posterior model probabilities \rev{for all $i = 1, \cdots, 4$} hypotheses, $p(\mathcal{H}_i \, | \, x)$ @berger2005posterior. \rev{The computation of posterior model probabilities are not automatically computed in \textbf{multibridge}, however, after computing the individual Bayes factors, the posterior model probabilities can be derived using the following equation.} Denoting the prior model probability for \rev{hypothesis} $\mathcal{H}_{r1}$ by $p(\mathcal{H}_{r1})$, the posterior model probability $p(\mathcal{H}_{r1} \mid \mathbf{x})$ is given by:

\[ p(\mathcal{H}_{r1} \mid \mathbf{x}) = \cfrac{\cfrac{p(\mathbf{x} \mid \mathcal{H}_{r1})}{p(\mathbf{x} \mid \mathcal{H}_e)} \times p(\mathcal{H}_{r1})}{\displaystyle\sum\limits_{i = 1}^M \cfrac{p(\mathbf{x} \mid \mathcal{H}_i)}{p(\mathbf{x} \mid \mathcal{H}_e)} \times p(\mathcal{H}_i)}.\]

 When all hypotheses are equally likely \emph{a priori}, this simplifies to:
\[
p(\mathcal{H}_{r1} \mid \mathbf{x}) = \cfrac{\text{BF}_{r1e}}{\text{BF}_{r1e} + \text{BF}_{r2e} + \text{BF}_{0e} + \text{BF}_{ee}} .
\]

```{r, eval = FALSE}
# posterior model probability of Hr1 given three alternative hypotheses
p_Hr1_x <- bfr1e/(bfr1e + bfr2e + bf0e + 1) # bfee = 1
```

\rev{Posterior model probabilities are useful for comparing multiple hypotheses with each other. However, it should be noted that posterior model probabilities are relative quantities and can change depending on which hypotheses are included in the comparison. Thus, hypotheses that describe the data poorly may have high posterior model probabilities if the other hypotheses in the comparison set are even worse descriptions of the data. In order to gain insight into whether a hypothesis describes the data adequately, we therefore include so-called bookend hypotheses in addition to the theory-informed hypotheses, that is, a hypothesis that maximally constrains the parameter space (such as a point-null hypothesis \(\mathcal{H}_{0}\)) and the encompassing hypothesis \(\mathcal{H}_{e}\) that does not constrain the parameter space} [in this case, that makes no ordinal predictions; @lee2018determining]\rev{. A hypothesis is then considered adequate if it can outperform these bookend models.}

Instead of posterior model probabilities, Bayes factors can also be calculated directly between two informed hypotheses. The comparison of any two informed hypotheses with one another follows from the fact that Bayes factors are transitive. For instance, the Bayes factor comparison \rev{between two informed hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\)} can be obtained by first computing \(\text{BF}_{er1}\) and \(\text{BF}_{er2}\), and then dividing out the common hypothesis \(\mathcal{H}_{e}\):
\[\text{BF}_{r1r2} = \frac{\text{BF}_{er1}}{\text{BF}_{er2}}.\]

## Prior Sensitivity

\rev{One of the main criticisms of Bayesian hypothesis testing is that the priors exert too much influence on the Bayes factors} [e.g., @kass1995bayes]\rev{. That is, even if the data are informative enough to overwhelm the prior for parameter estimation, priors can still influence the Bayes factors. The development of suitable priors is thus an important part of Bayesian hypothesis testing.} 

\rev{But even priors that are justified by theory are to a certain degree arbitrary. For instance, if one expects an increasing trend in the data, the parameters in the prior can be chosen to reflect that trend. The exact number of \emph{a priori} category counts, however, is at the discretion of the analyst. It is therefore considered good research practice to conduct a sensitivity analysis on the final results. In a sensitivity analysis, a set of plausible priors are determined in addition to the prior chosen in the main analysis for which the Bayes factors are calculated. The range of Bayes factors then gives an indication of the extend to which the results are fragile or robust to different modeling choices. In general, the prior on which the final analysis is performed as well as the set of priors used to conduct the sensitivity analysis should be determined and preregistered before seeing the data to ensure a fair comparison of the hypotheses of interest.} 

# Usage and Examples

In the following, we will outline \rev{three} examples on how to use \textbf{multibridge} to compare an informed hypothesis to a null or encompassing hypothesis. The first example concerns multinomial data and the second \rev{and third} example concerns independent binomial data. Additional examples are available as vignettes (see \texttt{vignette(package\ =\ "multibridge")}). 


```{r, echo = FALSE}
library('multibridge')
```

The two core functions of \textbf{multibridge}---\texttt{mult\_bf\_informed} and the \texttt{binom\_bf\_informed}---can be illustrated schematically as follows:

```{r, eval=FALSE, echo=TRUE}
mult_bf_informed(x, Hr, a, factor_levels)
binom_bf_informed(x, n, Hr, a, b, factor_levels)
```

## Example 1: Applying A Benford Test to Greek Fiscal Data

The first-digit phenomenon, otherwise known as Benford's law [@benford1938law; @newcomb1881note] states that the expected proportion of leading digits in empirical data can be formalized as follows: for any given leading digit \(d, d = (1, \cdots, 9)\) the expected proportion is approximately equal to \[\mathbb{E}_{\theta_d}= \text{log}_{10}((d + 1)/d).\] This means that in an empirical data set, numbers with smaller leading digits are more common than numbers with larger leading digits. Specifically, a number has leading digit \(1\) in \(30.1 \%\) of the cases, and leading digit \(2\) in \(17.61 \%\) of the cases; leading digit \(9\) is the least frequent digit with an expected proportion of only \(4.58 \%\) (see Table \ref{Tab:benford} for an overview of the expected proportions). Empirical data for which this relationship holds include population sizes, death rates, baseball statistics, atomic weights of elements, and physical constants [@benford1938law]. In contrast, artificially generated data, such as telephone numbers, do in general not obey Benford's law [@hill1995statistical]. Given that Benford's law applies to empirical data but not artificially generated data, a so-called Benford test can be used in fields like accounting and auditing to check for indications for poor data quality [for an overview, see e.g., @durtschi2004effective; @nigrini1997use; @nigrini2012benford]. Data that do not pass the Benford test, should raise audit risk concerns, meaning that it is recommended that they undergo additional follow-up checks [@nigrini2019patterns].

Below we discuss four possible Bayesian adaptations of the Benford test. In a first scenario we simply conduct a Bayesian multinomial test in which we test the point-null hypothesis \(\mathcal{H}_0\) which predicts a Benford distribution. In a second scenario we test \rev{the informed hypothesis} \(\mathcal{H}_{r1}\), which predicts a decreasing trend in the proportions of leading digits. The hypothesis \(\mathcal{H}_{r1}\) exerts considerably more constraint than \(\mathcal{H}_{e}\) and provides a more sensitive test if our primary goal is to test whether data comply with Benford's law or whether the data follow a similar but different trend. In the next two scenarios, our main goal is to identify fabricated data. The third scenario therefore tests the null hypothesis against the hypothesis that all proportions occur equally often. This hypothesis \(\mathcal{H}_{r2}\) could be considered if it is suspected that the data were generated randomly \rev{or could serve as a bookend comparison hypothesis as it maximally constraints the parameter space}. In a fourth scenario we test a hypothesis which predicts a trend that is characteristic for manipulated data. This hypothesis, which we denote as \(\mathcal{H}_{r3}\), could be derived from empirical research on fraud or be based on observed patterns from former fraud cases. For instance, @hill1995statistical instructed students to produce a series of random numbers; in the resulting data the proportion of the leading digit \(1\) occurred most often and the digits \(8\) and \(9\) occurred least often which is consistent with the general pattern of Benford's law. However, the proportion for the remaining leading digits were approximately equal. Note that the predicted distribution derived from @hill1995statistical is not currently used as a test to detect fraud, \rev{however, for the sake of simplicity, we assume that this pattern could be an indication of manipulated auditing data. All hypotheses will be tested against the encompassing hypothesis \(\mathcal{H}_{e}\), which too serves as a bookend comparison hypothesis, and which imposes no constraints on the proportion of leading digits.}

### Data and Hypothesis

The data we use to illustrate the computation of Bayes factors were originally published by the European statistics agency \enquote{Eurostat} and served as basis for reviewing the adherence to the Stability and Growth Pact of EU member states. @rauch2011fact conducted a Benford test on data related to budget deficit criteria, that is, public deficit, public dept and gross national products. The data used for this example features the proportion of first digits from Greek fiscal data in the years between \(1999\) and \(2010\); a total of \(N= 1{,}497\) numerical data were included in the analysis. We choose this data, since the Greek government deficit and debt statistics states has been repeatedly criticized by the European Commission in this time span [@europeanCommision2004; @europeanCommision2010]. In particular, the commission has accused the Greek statistical authorities to have misreported deficit and debt statistics. For further details on the data set see @rauch2011fact. The observed and expected proportions are displayed in Table \ref{Tab:benford}; the expected proportions versus the posterior parameter estimates under the encompassing hypothesis are displayed in Figure \ref{fig:benford-alt}.

\begin{table}[H]
	\centering
	\caption{Observed counts, observed proportions, and expected proportions of first digits in the Greek fiscal data set. The total sample size was $N = 1{,}497$ observations. Note that the observed proportions and counts deviate slightly from those reported in Rauch et al. (2011) (probably due to rounding errors).}
	\begin{tabular}{cccp{4cm}}
		\hline
Leading digit & Observed Counts & Observed Proportions & Expected Proportions: Benford's Law  \\
		\hline
		1 & 509 & 0.340 & 0.301  \\
		2 & 353 & 0.236 & 0.176  \\
		3 & 177 & 0.118 & 0.125  \\
		4 & 114 & 0.076 & 0.097  \\
		5 & 77 & 0.051 & 0.079  \\
		6 & 77 & 0.051 & 0.067  \\
		7 & 53 & 0.035 & 0.058  \\
		8 & 73 & 0.049 & 0.051  \\
		9 & 64 & 0.043 & 0.046  \\
		\hline
	\end{tabular}
    \label{Tab:benford}
\end{table}

In this example, the parameter vector of the multinomial model, \(\theta_1, \cdots, \theta_K\), reflects the probabilities of a leading digit in the Greek fiscal data being a number from \(1\) to \(9\). \rev{Each of the hypotheses above will be tested against the encompassing hypothesis $\mathcal{H}_e$ which imposes no constraints on the parameters.} The hypotheses introduced above can then be formalized as follows:
\begin{align*}
\mathcal{H}_e &: \boldsymbol{\theta} \sim \text{Dirichlet}(\mathbf{1}) \\
\mathcal{H}_0 &: \boldsymbol{\theta}_0 = (0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046), \\
\mathcal{H}_{r1} &: \theta_1 > \theta_2 > \theta_3 > \theta_4 > \theta_5 > \theta_6 > \theta_7 > \theta_8 > \theta_9 \\
\mathcal{H}_{r2} &:  \boldsymbol{\theta}_0 = \left(\frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}\right)\\
\mathcal{H}_{r3} &:  \theta_1 > (\theta_2 = \theta_3 = \theta_4 = \theta_5 = \theta_6 = \theta_7) > (\theta_8, \ \theta_9).
\end{align*}

### Method

Both \(\text{BF}_{0e}\) and \(\text{BF}_{r2e}\) may be readily computed by means of a Bayesian multinomial test which is implemented in the function \texttt{mult\_bf\_equality}. This function requires (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution under $\mathcal{H}_e$, and (3) the vector of expected proportions under $\mathcal{H}_0$ and under $\mathcal{H}_{r2}$. \rev{In this example, we} do not incorporate specific expectations about the distribution of leading digits in the Greek fiscal data and therefore \rev{assign a uniform Dirichlet distribution to the proportion of leading digits. That is, we} set all concentration parameters under $\mathcal{H}_e$ to 1 (i.e., we assign $\boldsymbol{\theta}$ a uniform Dirichlet prior distribution). \rev{This prior supports all possible points equally, meaning that, if the data were completely random, none of the hypotheses under consideration should be favored over the other.}

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Observed counts
x <- c(509, 353, 177, 114,  77,  77,  53,  73,  64)
# Prior specification for Dirichlet prior distribution under H_e
a <-  c(1, 1, 1, 1, 1, 1, 1, 1, 1)
# Expected proportions for H_0 and H_r2
p0  <- log10((1:9 + 1)/1:9)
pr2 <- c(1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9)
# Execute the analysis
results_H0_He   <- mult_bf_equality(x = x, a = a, p = p0)
results_Hr2_He  <- mult_bf_equality(x = x, a = a, p = pr2)
logBFe0  <- results_H0_He$bf$LogBFe0
logBFer2 <- results_Hr2_He$bf$LogBFe0
```

The hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r3}\) contain inequality constraints, and this necessitates the use of the function \texttt{mult\_bf\_informed} to compute the Bayes factors \(\text{BF}_{r1e}\) and \(\text{BF}_{r3e}\). This function requires (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution under $\mathcal{H}_e$, (3) labels for the categories of interest (i.e., leading digits), and (4) the informed hypothesis \(\mathcal{H}_{r1}\) or \(\mathcal{H}_{r3}\) (e.g., as a string). 
In addition to the basic required arguments, we use two additional arguments here. The first argument sets the Bayes factor type, that is, whether the output should print the Bayes factor in favor of the informed hypothesis (i.e., \(\text{BF}_{re}\)) or in favor of the encompassing hypothesis (i.e., \(\text{BF}_{er}\)). It is also possible to compute the log Bayes factor in favor of the hypothesis, which is the setting we choose for this example. The purpose of the second argument \texttt{seed} is to make the results reproducible:

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Observed counts
x <- c(509, 353, 177, 114,  77,  77,  53,  73,  64)
# Prior specification for Dirichlet prior distribution under H_e
a <-  c(1, 1, 1, 1, 1, 1, 1, 1, 1)
# Labels for categories of interest
factor_levels <- 1:9
# Specifying the informed hypotheses as a string
Hr1 <- c('1 > 2 > 3 > 4 > 5 > 6 > 7 > 8 > 9')
Hr3 <- c('1 > 2 = 3 = 4 = 5 = 6 = 7 > 8 , 9')
# Execute the analysis
results_He_Hr1 <- mult_bf_informed(x = x, Hr = Hr1, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
results_He_Hr3 <- mult_bf_informed(x = x, Hr = Hr3, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
logBFer1 <- summary(results_He_Hr1)$bf
logBFer3 <- summary(results_He_Hr3)$bf
```

We also compute the posterior model probabilities for all hypotheses. The results are shown in Table \ref{Tab:benfordResults}.

```{r, echo = FALSE}
bayes_factors <- data.frame(
   BFType = c('LogBF0e', 'LogBFr1e', 'LogBFr2e', 'LogBFr3e'), 
   LogBF  = c(-logBFe0, -logBFer1, -logBFer2, -logBFer3))

denominator <- c(1, exp(bayes_factors$LogBF))
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)', 'p(Hr2 | x)', 'p(Hr3 | x)'), 
   Prob = denominator/sum(denominator))
```

\begin{table}[H]
	\centering
	\caption{Prior model probabilities, posterior model probabilities, and Bayes factors for five rival accounts of first digit frequencies in the Greek fiscal data set.}
	\begin{tabular}{ccll}
		\hline Hypothesis &  $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{log}(\text{BF}_{.e})$ \\
		\hline
		$\mathcal{H}_{0}$  & $0.2$  &
		`r papaja::printnum(post_probs[2, 2], digits = 2, format = 'e')` & 
		`r papaja::printnum(bayes_factors[1, 2], digits = 2)` \\
		$\mathcal{H}_{r1}$ & $0.2$ &
		`r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')` & 
		`r papaja::printnum(bayes_factors[2, 2], digits = 2)`\\
		$\mathcal{H}_{e}$  & $0.2$ &
		`r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` & 
		$0$\\
		$\mathcal{H}_{r3}$ & $0.2$ &
		`r papaja::printnum(post_probs[5, 2], digits = 2, format = 'e')` &
		`r papaja::printnum(bayes_factors[4, 2], digits = 2)`\\
		$\mathcal{H}_{r2}$ & $0.2$ &
		`r papaja::printnum(post_probs[4, 2], digits = 2, format = 'e')` & 
		`r papaja::printnum(bayes_factors[3, 2], digits = 2)`\\
		\hline
	\end{tabular}
    \label{Tab:benfordResults}
\end{table}

The results indicate strong support for $\mathcal{H}_{r1}$ --the model in which the proportions are assumed to decrease monotonically-- over all other models. The log Bayes factor of $\mathcal{H}_{r1}$ against the encompassing hypothesis $\mathcal{H}_e$ is `r papaja::printnum(bayes_factors[2, 2], digits = 2, format = 'f')`, \rev{which equates to `r papaja::printnum(exp(bayes_factors[2, 2]), digits = 0, format = 'f')` on a natural scale}. 

The strong Bayes factor support for $\mathcal{H}_{r1}$ translates to a relatively extreme posterior model probability of `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')`. \rev{By comparison, the posterior model probabilities for hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{r3}$, that is, the bookend null-hypothesis and the hypothesis predicting a data pattern typical of fraud, are only slightly greater than zero. The posterior model probability for $\mathcal{H}_{e}$ is `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')`. Thus,} hypothesis $\mathcal{H}_{r1}$ \rev{can outperform the two bookend hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{e}$}. \rev{That $\mathcal{H}_{r1}$ outperforms the unconstrained model $\mathcal{H}_{e}$}} demonstrates how a parsimonious model that makes precise predictions can be favored over a model that is more complex [e.g., @jefferysberger1992]. 

(ref:benford-alt-caption) Predictions from Benford's law (in pink) show together with the posterior medians (black circles) for the category proportions estimated under the encompassing model $\mathcal{H}_e$. The circle skewers show the 95\% credible intervals. Only three of nine intervals encompass the expected proportions, suggesting that the data do not follow Benford's law. This plot was created using the \texttt{plot}-S3-method for \texttt{summary.bmult} objects in \textbf{multibridge}.

```{r benford-alt, echo = FALSE, message = FALSE, fig.cap = "(ref:benford-alt-caption)"}
first_digits <- 1:9
benford <- log10((first_digits + 1) / first_digits)

plot(
  summary(results_He_Hr1)
  , xlab = "Leading digit"
  # , ylab = "Proportion"
  , main = ""
  , panel.first = {
    lines(x = first_digits, y = benford, lty = "22", col = '#FB9A99')
    points(x = first_digits, y = benford, pch = 16, col = "white", cex = 2)
    points(x = first_digits, y = benford, pch = 16, bg = "white", col = '#FB9A99', cex = 0.8)
  }
)

points(x = first_digits, y = benford, pch = 16, bg = "white", col = '#FB9A99', cex = 0.8)

legend(
  "right"
  , legend = c("Benford", "Posterior")
  , col = c('#FB9A99', "black")
  , pch = c(16, 21)
  , pt.bg = c(NULL, "white")
  , lty = c("22", "solid")
  , lwd = c(1.25, 1)
  , bty = "n"
  , pt.cex = c(0.8, 1.5)
  , title = "Distribution"
  , seg.len = 1.5
)
```

### Sensitivity Analysis

\rev{In a sensitivity analysis we will determine whether our results are robust against different prior choices. In the main analysis we chose a uniform Dirichlet distribution on the category proportions as prior under $\mathcal{H}_{e}$. This prior assigns equal probability to all possible parameter values, but alternative prior distributions are also plausible. Experienced audit researchers may argue for the development of more informative and theory-driven priors that resemble, for instance, one of the hypotheses under consideration. The Dirichlet parameters vectors specified below resemble the four hypotheses, assuming $N = 54$ prior observations:}

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Alternative prior specifications
a0 <-  c(16, 10, 7, 5, 4, 3, 3, 3, 2) # Benford's law
a1 <-  c(10, 9, 8, 7, 6, 5, 4, 3, 2)  # Monotonically decreasing trend
a2 <-  c(6, 6, 6, 6, 6, 6, 6, 6, 6)   # Equal proportions
a3 <-  c(12, 6, 6, 6, 6, 6, 6, 3, 3)  # Fraud pattern
```

\rev{The sensitivity analysis is then carried out for each prior choice and will be compared to the main results. For this analysis, we are particularly interested in the Bayes factors of the hypothesis postulating a decreasing trend $\mathcal{H}_{r1}$ and Benford's law $\mathcal{H}_{0}$ to the encompassing hypothesis $\mathcal{H}_{e}$:}

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Sensitivity analysis for log(BFe_r1)
sensitivity0 <- mult_bf_informed(x = x, Hr = Hr1, a = a0, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity1 <- mult_bf_informed(x = x, Hr = Hr1, a = a1, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity2 <- mult_bf_informed(x = x, Hr = Hr1, a = a2, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity3 <- mult_bf_informed(x = x, Hr = Hr1, a = a3, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)

# Sensitivity analysis for log(BFe_0)
sensitivity4   <- mult_bf_equality(x = x, a = a0, p = p0)
sensitivity5   <- mult_bf_equality(x = x, a = a1, p = p0)
sensitivity6   <- mult_bf_equality(x = x, a = a2, p = p0)
sensitivity7   <- mult_bf_equality(x = x, a = a3, p = p0)
```

```{r, echo = FALSE}
m0 <- summary(sensitivity0)
m1 <- summary(sensitivity1)
m2 <- summary(sensitivity2)
m3 <- summary(sensitivity3)

m4 <- sensitivity4$bf$LogBFe0
m5 <- sensitivity5$bf$LogBFe0
m6 <- sensitivity6$bf$LogBFe0
m7 <- sensitivity7$bf$LogBFe0

bayes_factors1 <- data.frame(
   BFType = c('LogBFr1e_e', 'LogBFr1e_0', 'LogBFr1e_1', 'LogBFr1e_2', 'LogBFr1e_3'), 
   LogBF  = c(-logBFer1, -m0$bf, -m1$bf, -m2$bf, -m3$bf))

bayes_factors2 <- data.frame(
   BFType = c('LogBF0e_e', 'LogBF0e_0', 'LogBF0e_1', 'LogBF0e_2', 'LogBF0e_3'), 
   LogBF  = c(-logBFe0, -m4, -m5, -m6, -m7))
```

\rev{The results of the sensitivity analysis are displayed in Table \ref{Tab:benfordSensitivity}. The general direction of the sensitivity analysis agrees with our conclusions drawn from the main analysis. That is, for the Bayes factors of $\mathcal{H}_{r1}$ compared to $\mathcal{H}_{e}$, the evidence points towards the informed hypothesis. However, the prior exerts an influence on $\text{BF}_{r1e}$; the evidence in favor for the informed hypothesis ranges from weak to extreme evidence. Specifically, when we choose priors that resemble a decreasing trend for the frequency of leading digits, as we did with $\boldsymbol{\alpha_0}$ and $\boldsymbol{\alpha_1}$, the Bayes factor becomes smaller and the evidence weak (i.e., $(\text{BF}_{r1e} \mid \boldsymbol{\alpha_0}) = $ `r papaja::printnum(exp(bayes_factors1[2, 2]), digits = 2, format = 'f')` on the natural scale) and moderate (i.e., $(\text{BF}_{r1e} \mid \boldsymbol{\alpha_1})$ =   `r papaja::printnum(exp(bayes_factors1[3, 2]), digits = 2, format = 'f')` on the natural scale). However, if the data are contrasted to a prior that makes different predictions, the evidence is very strong or extreme. Thus, a prior that closely resembles the predictive trend reduces to some degree the diagnostic value of the data.}

\rev{The Bayes factors $\mathcal{H}_{0}$ compared to $\mathcal{H}_{e}$, on the other hand, are robust against different prior settings. Here too, the prior changes the Bayes factor estimate but in all cases the data suggests overwhelming evidence in favor of the encompassing hypothesis over Benford's law.}

\begin{table}[H]
	\centering
	\caption{Results of a sensitivity analysis for the Greek fiscal data set.}
	\begin{tabular}{cccll}
		\hline Description & Prior & $\text{log}(\text{BF}_{r1e})$ & $\text{log}(\text{BF}_{0e})$ \\
		\hline
		Uniform &
		$\boldsymbol{\alpha_e} = (1, 1, 1, 1, 1, 1, 1, 1, 1)$  & 
		`r papaja::printnum(bayes_factors1[1, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[1, 2], digits = 2)` \\
		Benford's law &
		$\boldsymbol{\alpha_0} = (16, 10, 7, 5, 4, 3, 3, 3, 2)$ & 
		`r papaja::printnum(bayes_factors1[2, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[2, 2], digits = 2)` \\
		Montonically decreasing &
		$\boldsymbol{\alpha_1} = (10, 9, 8, 7, 6, 5, 4, 3, 2)$  & 
		`r papaja::printnum(bayes_factors1[3, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[3, 2], digits = 2)` \\
		Centered on mean &
		$\boldsymbol{\alpha_2} = (6, 6, 6, 6, 6, 6, 6, 6, 6)$ & 
		`r papaja::printnum(bayes_factors1[4, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[4, 2], digits = 2)` \\
		Fraud pattern &
		$\boldsymbol{\alpha_3} = (12, 6, 6, 6, 6, 6, 6, 3, 3)$ & 
		`r papaja::printnum(bayes_factors1[5, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[5, 2], digits = 2)` \\
		\hline
	\end{tabular}
    \label{Tab:benfordSensitivity}
\end{table}

To summarize, the data offer overwhelming support for hypothesis \(\mathcal{H}_{r1}\), which postulates a decreasing trend in the digit proportions. This model outperformed both simpler models (e.g., the Benford model \rev{and bookend null-hypothesis}) and a more complex model in which the proportions were free to vary. \rev{The results are sensitive to our prior choices as a sensitivity analysis showed: for moderately informative priors which resemble the predicted decreasing trend, the \(\mathcal{H}_{r1}\) cannot outperform the encompassing model. On the other hand, the conclusion that Benford's law does not offer a good description of the data was robust to different prior settings.} Detailed follow-up analyses are needed to discover why the \rev{Greek fiscal data fail to adhere to Benford's law} [@nigrini2019patterns].

## Example 2: Prevalence of Statistical Reporting Errors

This section illustrates how \textbf{multibridge} may be used to evaluate models for independent binomial data rather than multinomial data. Our example concerns the prevalence of statistical reporting errors across eight different psychology journals. In any article that uses null hypothesis significance testing, there is a chance that the reported test statistic and degrees of freedom do not match the reported \(p\)-value, possibly because of copy-paste errors. To flag these errors, @epskamp2014statcheck developed the \texttt{R} package \texttt{statcheck}, which scans the PDF of a given scientific article and automatically detects statistical inconsistencies. This package allowed @nuijten2016prevalence to estimate the prevalence of statistical reporting errors in the field of psychology. In total, the authors investigated a sample of \(30{,}717\) articles (which translates to over a quarter of a million \(p\)-values) published in eight major psychology journals between 1985 to 2013: \emph{Developmental Psychology} (DP), the \emph{Frontiers in Psychology} (FP), the \emph{Journal of Applied Psychology} (JAP), the \emph{Journal of Consulting and Clinical Psychology} (JCCP), \emph{Journal of Experimental Psychology: General} (JEPG), the \emph{Journal of Personality and Social Psychology} (JPSP), the \emph{Public Library of Science} (PLoS), \emph{Psychological Science} (PS).

Based on several background assumptions, @nuijten2016prevalence predicted that the proportion of statistical reporting errors is higher for articles published in the \emph{Journal of Personality and Social Psychology} (JPSP) than for articles published in the seven other journals.

### Data and Hypothesis

Here we reuse the original data published by @nuijten2016prevalence, which we also distribute with the package \textbf{multibridge} under the name \texttt{journals}.

```{r, echo = TRUE}
data(journals)
```

The @nuijten2016prevalence hypothesis of interest, \(\mathcal{H}_r\), states that the prevalence for statistical reporting errors is higher for JPSP than for the other journals.^[@nuijten2016prevalence did not report inferential tests because they had sampled the entire population. We do report inferential tests here because we wish to learn about the latent data-generating process.] We will consider two specific versions of the @nuijten2016prevalence \(\mathcal{H}_r\) hypothesis. The first hypothesis, \(\mathcal{H}_{r1}\), stipulates that JPSP has the highest prevalence of reporting inconsistencies, whereas the other seven journals share a prevalence that is lower. The second hypothesis, \(\mathcal{H}_{r2}\), also stipulates that JPSP has the highest prevalence of reporting inconsistencies, but does not commit to any particular structure on the prevalence for the other seven journals.

The \textbf{multibridge} package can be used to test \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) against the null hypothesis \(\mathcal{H}_0\) that all eight journals have the same prevalence of statistical reporting errors. In addition, we will compare \(\mathcal{H}_{r1}\), \(\mathcal{H}_{r2}\), and \(\mathcal{H}_0\) against the encompassing hypothesis \(\mathcal{H}_e\) that makes no commitment about the prevalence of reporting inconsistencies across the eight journals. In this example, the parameter vector of the binomial success probabilities, \(\boldsymbol{\theta}\), reflects the probabilities that articles contain at least one statistical reporting inconsistency across journals. Thus, the above hypotheses can be formalized as follows:

\begin{align*}
   \mathcal{H}_e &:  \theta_{\text{JAP}} \cdots \theta_{\text{JPSP}} \sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k) \\
    \mathcal{H}_0 &:  \theta_{\text{JAP}} = \theta_{\text{PS}} = \theta_{\text{JCCP}} = \theta_{\text{PLOS}} = \theta_{\text{DP}} = \theta_{\text{FP}}= \theta_{\text{JEPG}} = \theta_{\text{JPSP}}\\
    \mathcal{H}_{r1} &:  (\theta_{\text{JAP}} = \theta_{\text{PS}} = \theta_{\text{JCCP}} = \theta_{\text{PLOS}} = \theta_{\text{DP}} = \theta_{\text{FP}}= \theta_{\text{JEPG}}) < \theta_{\text{JPSP}} \\
    \mathcal{H}_{r2} &: (\theta_{\text{JAP}} , \theta_{\text{PS}} , \theta_{\text{JCCP}} , \theta_{\text{PLOS}} , \theta_{\text{DP}} , \theta_{\text{FP}} , \theta_{\text{JEPG}}) < \theta_{\text{JPSP}}.
\end{align*}

### Method

To compute the Bayes factor \(\text{BF}_{0r}\) we need to specify (1) a vector with observed successes (i.e., the number of articles that contain a statistical inconsistency), (2) a vector containing the total number of observations (i.e., the number of articles), (3) a vector with prior parameter \(\alpha_k\) for each binomial proportion of the beta prior distribution under \(\mathcal{H}_e\), (4) a vector with prior parameter \(\beta_k\) for each binomial proportion of the beta prior distribution under \(\mathcal{H}_e\), (5) the category labels (i.e., journal names), and (6) the informed hypothesis \(\mathcal{H}_{r1}\) or \(\mathcal{H}_{r2}\) (e.g., as a string). We also change the Bayes factor type to \texttt{LogBFr0} so that the function returns the log Bayes factor in favor for the informed hypothesis compared to the null hypothesis. Since we have no specific expectations about the distribution of statistical reporting errors in any given journal, we set all parameters \(\alpha_k\) and \(\beta_k\) to one which corresponds to uniform beta distributions. With this information, we can now conduct the analysis with the function \texttt{binom\_bf\_informed}.

```{r, echo=TRUE, message=FALSE, results='hide'}
# Since percentages are rounded to two decimal values, we round the
# articles with an error to obtain integer values
x <- round(journals$articles_with_NHST  * 
             (journals$perc_articles_with_errors/100))
# Total number of articles
n <- journals$articles_with_NHST
# Prior specification for beta prior distributions under H_e
a <- c(1, 1, 1, 1, 1, 1, 1, 1)
b <- c(1, 1, 1, 1, 1, 1, 1, 1)
# Labels for categories of interest
journal_names <- journals$journal

# Specifying the informed Hypothesis
Hr1 <- c('JAP = PS = JCCP = PLOS = DP = FP = JEPG < JPSP')
Hr2 <- c('JAP , PS , JCCP , PLOS , DP , FP , JEPG < JPSP')

# Execute the analysis for Hr1
results_H0_Hr1 <- binom_bf_informed(x = x, n = n, Hr = Hr1, a = a, b = b,
                                factor_levels = journal_names,
                                bf_type = 'LogBFr0', seed = 2020)
# Execute the analysis for Hr2
results_H0_Hr2 <- binom_bf_informed(x = x, n = n, Hr = Hr2, a = a, b = b,
                                factor_levels = journal_names,
                                bf_type = 'LogBFr0', seed = 2020)
```

```{r, echo = TRUE}
LogBFe0  <- results_H0_Hr1$bf_list$bf0_table[['LogBFe0']]
LogBFr10 <- summary(results_H0_Hr1)$bf
LogBFr20 <- summary(results_H0_Hr2)$bf
```

```{r compute_post_probs, echo = FALSE}
LogBFr1e <- -results_H0_Hr1$bf_list$bfr_table[['LogBFer']]
LogBFr2e <- -results_H0_Hr2$bf_list$bfr_table[['LogBFer']]

bayes_factors <- data.frame(
   BFType = c('LogBFe0', 'LogBFr10', 'LogBFr20'), 
   BF = c(LogBFe0, LogBFr10, LogBFr20))

denominator <- sum(1, exp(-LogBFe0), exp(LogBFr1e), exp(LogBFr2e))
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)' , 'p(Hr2 | x)'), 
   Prob = c(1, exp(-LogBFe0), exp(LogBFr1e), exp(LogBFr2e))/denominator)
```


\begin{table}[H]
    \centering
    \caption{Prior model probabilities, posterior model probabilities, and Bayes factors for four hypotheses concerning the prevalence of statistical reporting errors across psychology journals.}
    \begin{tabular}{ccll}
        \hline Hypothesis & $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{log}(\text{BF}_{.0})$ \\
        \hline
        $\mathcal{H}_{0}$  & $0.25$ & 
        `r papaja::printnum(post_probs[2, 2], digits = 4, format = 'e')` & 
        $0$ \\
        $\mathcal{H}_{r2}$ & $0.25$ & 
        `r papaja::printnum(post_probs[4, 2], digits = 4, format = 'f')` &  
        `r papaja::printnum(bayes_factors[3, 2], digits = 2, format = 'f')`\\
        $\mathcal{H}_{e}$  &  $0.25$  & 
        `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` & 
        `r papaja::printnum(bayes_factors[1, 2], digits = 2, format = 'f')` \\
        $\mathcal{H}_{r1}$  &  $0.25$  & 
        `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'e')` & 
        `r papaja::printnum(bayes_factors[2, 2], digits = 2, format = 'f')` \\
        \hline
    \end{tabular}
    \label{Tab:journalsResults}
\end{table}

As the evidence is extreme in all four cases, we again report all Bayes factors on the log scale. The Bayes factor \(\text{log}(\text{BF}_{r20})\) indicates overwhelming evidence for the informed hypothesis that JPSP has the highest prevalence for statistical reporting inconsistencies compared to the null hypothesis that the statistical reporting errors are equal across all eight journals; \( \text{log}(\text{BF}_{r20}) = \) `r papaja::printnum(bayes_factors[3, 2], digits = 2, format = 'f')`.

For a clearer picture about the ordering of the journals we can investigate the posterior distributions for the prevalence rates obtained under the encompassing model. 

(ref:journals-caption) Posterior medians for the prevalence of statistical reporting inconsistencies across eight psychology journals, as obtained using the encompassing model. The circle skewers show the 95\% credible intervals. Analysis based on data from @nuijten2016prevalence. This plot was created using the \texttt{plot}-S3-method for \texttt{summary.bmult} objects.

```{r journals, echo = TRUE, message = FALSE, fig.cap = "(ref:journals-caption)"}
plot(summary(results_H0_Hr2), xlab = "Journal")
```

The posterior medians and 95\% credible intervals are returned by the \texttt{summary}-method and are shown in Figure \ref{fig:journals}. The figure strongly suggests that the prevalence of reporting inconsistencies is not equal across all eight journals. This impression may be quantified by comparing the null hypothesis \(\mathcal{H}_0\) to the encompassing hypothesis \(\mathcal{H}_e\). The corresponding Bayes factor equals \( \text{log}(\text{BF}_{e0}) = \) `r papaja::printnum(bayes_factors[1, 2], digits = 2, format = 'f')`, which confirms that the data dramatically undercut the null hypothesis that the prevalence of statistical reporting inconsistencies is equal across journals.

The data offer most support for the Nuijten hypothesis \(\mathcal{H}_{r2}\), which posits that JPSP has the highest prevalence but does not commit to any restriction on the prevalences for the remaining seven journals. This hypothesis may be compared to the encompassing hypothesis \(\mathcal{H}_e\), which yields \( \text{log}(\text{BF}_{r2e}) = \) `r papaja::printnum(LogBFr2e, digits=2, format = 'f')`. This means that the observed data are \(\exp(2.01) \approx 7.45\) times more likely under \(\mathcal{H}_{r2}\) than under \(\mathcal{H}_e\); this is moderate evidence for the restriction suggested by @nuijten2016prevalence. Under equal prior probability for the models, this Bayes factor translates to a posterior probability on \(\mathcal{H}_e\) of `r papaja::printnum(post_probs[1, 2], digits = 3, format = 'f')`, an amount that researchers may deem too large to discard in an all-or-none fashion.

To summarize, the data provide moderate evidence for the hypothesis stated by Nuijten et al. (2016) that the prevalence of statistical reporting inconsistencies in JPSP is higher than that in seven other psychology journals.

## Example 3: Effects of Gender and Education on the Violation of Stochastic Dominance

\rev{This section illustrates concerns the comparison of four nested hypotheses concerning independent binomial probabilities. In his study,} @birnbaum1999testing \rev{presented new possibilities of online testing for psychological science (in the late 1990s online testing was still novel and rarely used). To compare data collected from an online research to traditional lab research,} @birnbaum1999testing \rev{collected experimental data from 1224 participants online and 124 participants in the lab. In his experiment participants played 20 rounds of a gambling game. In each round, they were presented with two money gambles with different probabilities and monetary values and were asked to indicate which gamble they would rather play. The gamble chosen by the participants was then played once.} @birnbaum1999testing \rev{then examined the characteristics of the two samples, for instance, in terms of their risk aversion and their consistency with decision making axioms, such as stochastic violations, and correlated them with different demographics.} 

\rev{The author analyzed the proportion of stochastic violations for different demographic variables, noting a seemingly ordinal pattern for the probabilities to violate of stochastic dominance for the factors gender (m=male, f=female) and education (1=doctorate, 2=postgraduate degree, 3=bachelor's degree, 4=less than bachelor's degree). In a later study,} @myung2005bayesian \rev{presented a Bayesian inference framework to test decision making axioms (using the ``Bayesian $p$-value'') and used Birnbaum's data as an example on how to assess violances of stochastic dominance and their relationship with covariates. Concretely,} @myung2005bayesian \rev{reanalyzed the data from} @birnbaum1999testing \rev{and tested the informed hypothesis that stochastic dominance is violated more frequently in women compared to men and more frequently in lower education levels than higher education levels.}

### Data and Hypothesis

\rev{We will use data from} @birnbaum1999testing \rev{as presented in} @myung2005bayesian\rev{. The data show the stochastic violations of the online sample for one of the gambling rounds featuring 1212 valid responses (see Table \ref{Tab:decisionData}).}

```{r, echo = TRUE}
dat <- data.frame(gender = rep(c('male', 'female'), each = 4),
                  education = rep(c('1', '2', '3', '4'), 2),
                  levels = paste0(rep(c('m', 'f'), each = 4), 1:4),
                  violation = c(0.487, 0.477, 0.523, 0.601,
                                0.407, 0.555, 0.650, 0.622),
                  n = c(80, 88, 195, 163,
                        54, 108, 206, 318),
                  x = c(39, 42, 102, 98, 
                        22, 60, 134, 198))

```


```{r, echo = FALSE}
descriptives_tab <- data.frame(
  education =  rep(c('Doctorate Degree', 'Postgraduate Degree', 'Bachelor\'s Degree', 'Less than Bachelor\'s degree'), 2),
  counts = paste0(dat$x, '/' , dat$n),
  props = dat$violation
)

colnames(descriptives_tab) <- c('Education', 'Observed Counts', 'Observed Proportions')
papaja::apa_table(
  descriptives_tab,
  label = 'Tab:decisionData',
  stub_indents = list(`Male` = 1:4, `Female` = 5:8),
  caption = "Observed counts and observed proportions of stochastic dominance violations for the N = 1,212 participants in Birnbaum (1999). The data are split by gender and education level of the participants.",
  escape = TRUE
)
```

\rev{The parameter vector of the binomial success probabilities, $\theta_1, \cdots, \theta_K$, contains the probabilities of observing a value in a particular category; here, it reflects the probabilities of violating stochastic dominance for a particular subgroup (e.g., females with a doctorate). We will compare three inequality-constrained hypotheses $\mathcal{H}_{r1}$, $\mathcal{H}_{r2}$, $\mathcal{H}_{r3}$ formulated by} @myung2005bayesian\rev{. The first hypothesis $\mathcal{H}_{r1}$ encodes the main effect for gender and states that the probability to violate stochastic dominance is lower for males than for females. The second hypothesis $\mathcal{H}_{r2}$ encodes the main effect of education and states that the probability to violate stochastic dominance is lower for persons with higher education levels. The third hypothesis $\mathcal{H}_{r3}$ combines hypotheses $\mathcal{H}_{r1}$ and $\mathcal{H}_{r2}$. We will test this hypothesis against the encompassing hypothesis $\mathcal{H}_e$ without any constraints. In addition, we will include a bookend null-hypothesis $\mathcal{H}_{0}$ predicting that all probabilities are equal.}

\begin{align*}
\mathcal{H}_e &: (\theta_{\text{m1}}, \theta_{\text{m2}}, \theta_{\text{m3}}, \theta_{\text{m4}}, \theta_{\text{f1}}, \theta_{\text{f2}}, \theta_{\text{f3}}, \theta_{\text{f4}}) \\
\mathcal{H}_0 &:   \boldsymbol{\theta}_0 = \left(\frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}\right), \\
    \mathcal{H}_{r1} &: (\theta_{\text{m1}}, \theta_{\text{m2}}, \theta_{\text{m3}}, \theta_{\text{m4}}) < (\theta_{\text{f1}}, \theta_{\text{f2}}, \theta_{\text{f3}}, \theta_{\text{f4}})\\
    \mathcal{H}_{r2} &: (\theta_{\text{m1}}, \theta_{\text{f1}}) < (\theta_{\text{m2}}, \theta_{\text{f2}}) < (\theta_{\text{m3}}, \theta_{\text{f3}}) < (\theta_{\text{m4}}, \theta_{\text{f4}}) \\
    \mathcal{H}_{r3} &: \theta_{\text{m1}} < \theta_{\text{f1}} < \theta_{\text{m2}} < \theta_{\text{f2}} < \theta_{\text{m3}} < \theta_{\text{f3}} < \theta_{\text{m4}} < \theta_{\text{f4}}.
\end{align*}

### Method

\rev{To evaluate the inequality-constrained hypothesis, we need to specify (1) a vector with observed successes, and (2) a vector containing the total number of observations, (3) the informed hypothesis, (4) a vector with prior parameters alpha for each binomial proportion, (5) a vector with prior parameters beta for each binomial proportion, and (6) the labels of the categories of interest (i.e., gender and education level). As with the previous two example, we assign a uniform Beta prior to the binomial probabilites:}

```{r, echo = TRUE}
# number of violations
x <- dat$x
# total number people in the category
n <- dat$n

# Specifying the informed hypotheses (step 3)

# null hypothesis
p0  <- c(1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8)

# informed hypotheses
Hr1 <- c('m1, m2, m3, m4 < f1, f2, f3, f4')
Hr2 <- c('m1, f1 < m2, f2 < m3, f3 < m4, f4')
Hr3 <- c('m1 < f1 < m2 < f2 < m3 < f3 < m4 < f4')

# Prior specification (step 4 and 5)
# We assign a uniform beta distribution to each binomial propotion
a <- c(1, 1, 1, 1, 1, 1, 1, 1)
b <- c(1, 1, 1, 1, 1, 1, 1, 1)

# categories of interest (step 6)
gender_edu <- dat$levels
```

\rev{With this information, we can now conduct the analysis with the function \texttt{binom\_bf\_informed()}. Since we are interested in quantifying evidence in favor of the informed hypotheses compared to the encompassing hypothesis, we set the Bayes factor type to \texttt{BFre}. For reproducibility, we are also setting a seed:}

```{r, message=FALSE, echo = TRUE}
results_H0_He <- multibridge::mult_bf_equality(x = x, a = a, p = p0)

results_Hr1_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr1, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)


results_Hr2_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr2, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)


results_Hr3_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr3, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)
```

```{r, message=FALSE, echo = FALSE}
BF0e <- results_H0_He$bf$BF0e
BFr1e <- summary(results_Hr1_He)$bf
BFr2e <- summary(results_Hr2_He)$bf
BFr3e <- summary(results_Hr3_He)$bf
```

\rev{The results for the analysis are summarized in Table \ref{Tab:decisionResults}. We first inspect the Bayes factors for the three informed hypotheses compared to the encompassing hypothesis. For hypotheses $\mathcal{H}_{r1}$, the data suggest moderate evidence for the encompassing hypothesis than compared to the informed hypothesis, with a Bayes factor of `r papaja::printnum(1/BFr1e, digits = 2, format = 'f')`. This hypothesis predicted a main effect of gender, that is, males should have a lower probability of violating stochastic dominance than females regardless of their education level. For hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{r3}$, the data suggest strong evidence for the the informed hypothesis compared to the encompassing hypothesis, with Bayes factors of `r papaja::printnum(BFr2e, digits = 2, format = 'f')` and `r papaja::printnum(BFr3e, digits = 2, format = 'f')`, respectively. Thus, based on these data, people with lower education levels are more likely to violate stochastic dominance ($\mathcal{H}_{r2}$), and that the factors gender and education level interact with each other ($\mathcal{H}_{r3}$). The data provide strong evidence for both hypotheses. The ordinal constraint predicted by $\mathcal{H}_{r3}$ also becomes apparent, when we plot the posterior estimates.}

```{r, echo = TRUE}
plot(summary(results_Hr3_He))
```

\rev{To compare all four hypotheses directly with each other, we computed the posterior model probabilities. The model which predicts only a gender effect performs worse than the baseline model without any restrictions. Hypothesis $\mathcal{H}_{r3}$ outperforms all other models, including the bookend hypotheses, with a posterior model probability of `r signif(post_probs[4, 'Prob'], 2) * 100` \%. These results are in line with the conclusions drawn by} @myung2005bayesian and @birnbaum1999testing\rev{, that is, that taken into account the complexity of the model, hypothesis $\mathcal{H}_{r3}$ performs the best. That is, there is a combined effect of gender and education with respect to the probability to violate stochastic dominance. With regard to hypothesis $\mathcal{H}_{r1}$, we can conclude that the gender effect only becomes apparent when taking into account the level of education.}

```{r, echo = TRUE}
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)', 'p(Hr2 | x)', 'p(Hr3 | x)'),
   Prob = c(1, BF0e, BFr1e, BFr2e, BFr3e)/sum(c(1, BF0e, BFr1e, BFr2e, BFr3e)))
```


\begin{table}[H]
    \centering
    \caption{Prior model probabilities, posterior model probabilities, and Bayes factors for four hypotheses concerning the relationship between gender and education level on the probability to violate stochastic domaniance.}
    \begin{tabular}{ccll}
        \hline Hypothesis & $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{BF}_{.e}$ \\
        \hline
        $\mathcal{H}_{e}$  & $0.25$ &
        `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` &
        $1$ \\
        $\mathcal{H}_{0}$ & $0.25$ &
        `r papaja::printnum(post_probs[2, 2], digits = 2, format = 'e')` &
        `r papaja::printnum(BF0e, digits = 2, format = 'e')`\\
        $\mathcal{H}_{r1}$ & $0.25$ &
        `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr1e, digits = 2, format = 'f')`\\
        $\mathcal{H}_{r2}$  &  $0.25$  &
        `r papaja::printnum(post_probs[4, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr2e, digits = 2, format = 'f')` \\
        $\mathcal{H}_{r3}$  &  $0.25$  &
        `r papaja::printnum(post_probs[5, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr3e, digits = 2, format = 'f')` \\
        \hline
    \end{tabular}
    \label{Tab:decisionResults}
\end{table}

# Discussion

The \texttt{R} package \textbf{multibridge} facilitates the estimation of Bayes factors for informed hypotheses in both multinomial and independent binomial models. The efficiency gains of \textbf{multibridge} are particularly pronounced when the parameter restrictions are highly informative or when the number of categories is large.

\textbf{multibridge} supports the evaluation of informed hypotheses that feature equality constraints, inequality constraints, and free parameters, as well as \rev{combinations} between them. Moreover, users can choose to test the informative hypothesis against an encompassing hypothesis that lets all parameters vary freely or against the null hypothesis that states that category proportions are exactly equal.
Beyond the core functions currently implemented in \textbf{multibridge}, there are several natural extensions we aim to include in future versions of this package. For instance, to compare several models with each other we plan to implement functions that compute the posterior model probabilities. Another extension is to facilitate the specification of hierarchical binomial and multinomial models which would allow users to analyze data where responses are nested within a higher-order structure such as participants, schools, or countries. Hierarchical multinomial models can be found, for instance, in source memory research where people need to select a previously studied item from a list} [e.g., @arnold2019testing]\rev{; a hierarchical binomial model was applied, for instance, in} @hoogeveen2020laypeople\rev{, to evaluate laypeople's accuracy in predicting replication outcomes for social science studies.}

\rev{Furthermore, to make the method accessible to a larger audience of users and students, \textbf{multibridge} will be made available in future versions of the software package JASP} [@jasp]\rev{. JASP offers an intuitive graphical user interface and does not require extensive knowledge in programming. A first prototype of the \textbf{multibridge} module can be seen in Figure \ref{fig:jasp}:}

(ref:jasp-caption) A first prototype of the implementation of the \textbf{multibridge} module in JASP.
```{r jasp, fig.cap='(ref:jasp-caption)', out.width = "80%", message=FALSE, fig.align='center'}
knitr::include_graphics("jasp_implementation.png", auto_pdf = TRUE)
``` 

\rev{In addition, we plan to expand the types of hypotheses that can be evaluated in future versions of this package, Currently, \textbf{multibridge} only supports informed hypotheses which are "stick-hypotheses", that is, hypotheses in which all parameters shared common upper and lower bounds. While the quantity shown in Equation \ref{Eq:klugkistIdentity} admits in principle any constraint imposed on a vector of category proportions, this requirement is necessary for the bridge sampling routine, in order to transform samples from the real line to the probability space. To be able to evaluate more general ordinal constrains including "branch-hypotheses" with bridge sampling in the future, the stick-breaking transformation needs to be further refined. Arguably, this refinement can be realized more easily for transformations of multiple binomials than for multinomials, since independent binomials live in probability space but are not constrained by the sum-to-one condition.}

\rev{In addition, we aim to enable the specification of more general informed hypotheses, including hypotheses on the size ratios of the parameters (e.g., $\theta_1 < 2 \times \theta_2$) or on their odds ratios (e.g., $\frac{\theta_1}{(\theta_1 + \theta_2)} < \frac{\theta_3}{(\theta_3 + \theta_4)}$). A framework to evaluate these constraints using the unconditional encompassing approach has already been proposed} [@klugkist2010bayesian]\rev{. We believe that the bridge sampling method could also be extended to test these hypotheses as in principle, all the building blocks are already in place. Specifically, \textbf{multibridge} takes size ratios into account when it evaluates hypotheses featuring combinations of equality and inequality constraints. For these hypotheses, \textbf{multibridge} first evaluates the equality constraints separately and then evaluates the inequality constraints given the equality constraints hold. To do so, the algorithm combines equality-constrained categories but tracks their initial number to effectively sample from the constrained parameter space and when transforming the parameters. For odds ratios, on the other hand, a suitable sampling method and transformation has not yet been developed. To facilitate the evaluation of these hypotheses, alternative methods to sample and transform the parameters are required.}

<!-- \rev{To transform parameters from the real line to an ordered probability vector using the stick-breaking transformation, it is necessary to determine the upper and lower bound for each considered parameter. Using these bounds, a "stick" of length one is sequentially divided into larger and larger pieces, so that at the end a probability vector is created which corresponds to the informed hypothesis. These bounds are derived through the number of parameters that are smaller and larger than the current one. For this step it is essential that all parameters are considered when determining the bounds. Otherwise, one risks to create a probability vector which does not satisfy the sum-to-one constraint, since either too little or too much has been broken off the "stick".} -->
<!-- \rev{That is, only throught the requirement of a "stick-hypothesis" it can be guaranteed that the stick is broken in such a way that the transformation produces a probability vector in accordance with the constraints.  -->

<!-- \rev{Lastly, ordinal constrained parameters can only be part of one constrained. Consider, for example, a transitivity axiom of choice preference which states that if a person prefers A over B and B over C, if should follow that they also prefer A over C. In multibridge it is not possible to define a hypothesis of the form 'A<B \& B<C \& A<C'. It is possible to either test the full axiom, that is 'A<B<C' or to define each constraint as separate model and compute a Bayes factor for each restriction separately, so  -->
<!-- binom_informed_bf('A<B') -->
<!-- binom_informed_bf('B<C') -->
<!-- binom_informed_bf('A<C'). -->
<!-- For these instances it is then possible to extract the marginal likelihood for each model and assess for each restriction whether the data support the hypothesis. -->
<!-- } -->

# Declarations

## Availability of data and code
The source code of the \texttt{R} package is available at: \url{https://github.com/ASarafoglou/multibridge/}. In addition, readers can access the code for reproducing all analyses and plots via our project folder on the Open Science Framework: \url{https://osf.io/2wf5y/}.

## Funding
This research was supported by a Netherlands Organisation for Scientific Research (NWO) grant
to AS (406-17-568), a Veni grant from the NWO to MM (451-17-017), a Vici grant from the NWO to EJW (016.Vici.170.083), as well as a a European Research Council (ERC) grant to EJW (283876). This paper was written in Rmarkdown, using the \texttt{R} package \textbf{papaja} [@papaja].

## Author contributions
The authors made the following contributions. Alexandra Sarafoglou: Conceptualization, Data Curation, Formal Analysis, Funding Acquisition, Methodology, Project Administration, Software, Validation, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; Frederik Aust: Conceptualization, Software, Supervision, Validation, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; Maarten Marsman: Funding Acquisition, Conceptualization, Methodology, Supervision, Validation, Writing - Review & Editing; Frantisek Bartos: Software; Eric-Jan Wagenmakers: Funding Acquisition, Methodology, Supervision, Validation, Writing - Review & Editing; Julia M. Haaf: Conceptualization, Formal Analysis, Methodology, Software, Supervision, Validation, Writing - Original Draft Preparation, Writing - Review & Editing.

## Conflicts of interest
The authors declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## Ethical Approval
This is a methodological contribution which requires no ethical approval.

\clearpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\clearpage

```{r echo = FALSE, results = 'asis', cache = FALSE, child = "Rpackage_appendix.Rmd"}
# papaja::render_appendix('Rpackage_appendix.Rmd')
```

