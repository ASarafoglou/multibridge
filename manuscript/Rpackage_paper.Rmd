---
title             : "multibridge: An R Package To Evaluate Informed Hypotheses in Binomial and Multinomial Models" 
shorttitle        : "multibridge"

author:
  - name: Alexandra Sarafoglou
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Data Curation
    #   - Formal Analysis
    #   - Funding Acquisition
    #   - Methodology
    #   - Project Administration
    #   - Software
    #   - Validation
    #   - Visualization
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing
    corresponding: yes
  - name: Frederik Aust
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Visualization
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing
  - name: Maarten Marsman
    affiliation: ' '
    # role:
    #   - Funding Acquisition
    #   - Conceptualization
    #   - Methodology
    #   - Supervision
    #   - Validation
    #   - Writing - Review & Editing
  - name: Frantisek Bartos
    affiliation: ' '
    # role:
    #   - Software
  - name: Eric-Jan Wagenmakers
    affiliation: ' '
    # role:
    #   - Funding Acquisition
    #   - Methodology
    #   - Supervision
    #   - Validation
    #   - Writing - Review & Editing
  - name: Julia M. Haaf
    affiliation: ' '
    # role:
    #   - Conceptualization
    #   - Formal Analysis
    #   - Methodology
    #   - Software
    #   - Supervision
    #   - Validation
    #   - Writing - Original Draft Preparation
    #   - Writing - Review & Editing

affiliation:
  - id: ' '
    institution: University of Amsterdam
    
note: | 
  Correspondence concerning this article should be addressed to: Alexandra Sarafoglou, Department of Psychology, PO Box 15906, 1001 NK Amsterdam, The Netherlands, E-mail: alexandra.sarafoglou@gmail.com

abstract: |
  The \textbf{multibridge} \texttt{R} package allows a Bayesian evaluation of informed hypotheses \(\mathcal{H}_r\) applied to frequency data from an independent binomial or multinomial distribution. \textbf{multibridge} uses bridge sampling to efficiently compute Bayes factors for the following hypotheses concerning the latent category proportions \(\boldsymbol{\theta}\): (a) hypotheses that postulate equality constraints (e.g., \(\theta_1 = \theta_2 = \theta_3\)); (b) hypotheses that postulate inequality constraints (e.g., \(\theta_1 < \theta_2 < \theta_3\) or \(\theta_1 > \theta_2 > \theta_3\)); (c) hypotheses that postulate combinations of inequality constraints and equality constraints (e.g., \(\theta_1 < \theta_2 = \theta_3\)); and (d) hypotheses that postulate combinations of (a)--(c) (e.g., \(\theta_1 < (\theta_2 = \theta_3) , \theta_4\)). Any informed hypothesis \(\mathcal{H}_r\) may be compared against the encompassing hypothesis \(\mathcal{H}_e\) that all category proportions vary freely, or against the null hypothesis \(\mathcal{H}_0\) that all category proportions are equal. \textbf{multibridge} facilitates the fast and accurate comparison of large models with many constraints and models for which relatively little posterior mass falls in the restricted parameter space. This paper describes the underlying methodology and illustrates the use of \textbf{multibridge} through fully reproducible examples.

bibliography      : "../inst/REFERENCES.bib"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
numbersections    : false

documentclass     : "apa6"
classoption       : "man"
biblio-style      : "apa"
output            : papaja::apa6_pdf
header-includes:
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{nicefrac}
   - \usepackage{caption}
   - \usepackage{xcolor}
   - \definecolor{mypink}{RGB}{255, 230, 255}
   - \definecolor{myWheat}{RGB}{245, 222, 179}
   - \definecolor{myGreen}{RGB}{27, 158, 119}
   - \usepackage{todonotes}
   - \usepackage[toc]{appendix}
   - \newcommand{\Julia}[1]{\todo[inline, color=mypink]{#1}}
   - \newcommand{\Frederik}[1]{\todo[inline, color=myWheat]{#1}}
   - \newcommand{\Alex}[1]{\todo[inline, color=myGreen]{#1}}
   - \newcommand{\rev}[1]{\textcolor{blue}{#1}}
---

```{r, echo = FALSE, warning=FALSE}
library(plyr)
library(knitr)
library(stats)
library(ggplot2)
library(RColorBrewer)
library(multibridge)

.correlation_plot <- function(x=x, 
                              y=y, 
                              yrange=c(-1, 1),
                              xrange=c(-1, 1), 
                              binwidth = 0.05,
                              xlab=expression(theta[1]), ylab= expression(theta[2]),
                              title= 'Plot', 
                              alpha=1/2,
                              col = '#03A89E'){
  
  data <- data.frame(x=x,  y=y)

  blankPlot <- ggplot() +
    geom_blank(aes(1,1)) +
    theme(line = element_blank(),
          text  = element_blank(),
          title = element_blank(),
          plot.background = element_blank(),
          panel.border = element_blank(),
          panel.background = element_blank())
  
  scatterP <- ggplot(data, aes(x=x, y=y)) +
    geom_point(color="black", fill=col, shape=21, size=2, alpha=alpha) +
    theme_classic() +
    scale_x_continuous(name = xlab, limits= xrange) +
    scale_y_continuous(name = ylab, limits= yrange) +
    geom_segment(aes(y=-Inf,yend=-Inf,x=xrange[1],xend=xrange[2])) +
    geom_segment(aes(y=yrange[1],yend=yrange[2],x=-Inf,xend=-Inf)) +
    theme(axis.line=element_blank()) +
    theme(legend.position="none", plot.margin = unit(c(0,0,2,2), "lines"), 
          axis.text.x=element_text(size=14),  
          axis.text.y=element_text(size=14), 
          axis.title.x=element_text(size=14, vjust=-1.6), 
          axis.title.y=element_text(size=14, vjust=2.6)) 
  
  yHist <- ggplot(data, aes(x=y)) +
    geom_histogram(color="black", fill=col, binwidth = binwidth) +
    xlim(xrange[1],xrange[2]) +
    labs(title="", x="", y="")+
    theme_void() +
    coord_flip() + 
    theme(legend.position = "none", 
          plot.margin = unit(c(0,0,5,0), "lines")) 
  
  xHist <- ggplot(data, aes(x=x)) +
    geom_histogram(color="black", fill=col, binwidth = binwidth) +
    xlim(xrange[1],xrange[2]) +
    labs(title=title, x="", y="")+
    theme_void() +
    theme(legend.position = "none", 
          plot.margin = unit(c(0,0,0,5), "lines")) 
  
  p <- cowplot::plot_grid(xHist, blankPlot, scatterP,
                          yHist, nrow = 2, 
                          labels=c(' '), label_size = 18,
                          rel_heights = c(1, 2), rel_widths = c(2, 1))
  return(p)
}
```

# Introduction

The most common way to analyze categorical variables is to conduct either binomial tests, multinomial tests, or chi-square goodness of fit tests. These tests compare the encompassing hypothesis to a null hypothesis that all underlying category proportions are either exactly equal, or follow a specific distribution. Accordingly, these tests are suitable when theories predict either the invariance of all category proportions or specific values. For instance, chi-square goodness of fit tests are commonly used to test Benford's law, which predicts the distribution of leading digits in empirical datasets [@benford1938law; @newcomb1881note]. Often, however, the predictions that researchers are interested in are of a different kind. Consider for instance the weak-order mixture model of decision-making  [@regenwetter2012behavioral]. The theory predicts that individuals' choice preferences are weakly ordered at all times, that is, if they prefer choice \(A\) over \(B\) and \(B\) over \(C\) then they will also prefer \(A\) over \(C\) [@regenwetter2011transitivity]---a well-constrained prediction of behavior. The theory is, however, silent about the exact values of each choice preference. Hence, the standard tests that compare \(\mathcal{H}_e\) to \(\mathcal{H}_0\) are unsuited to test the derived predictions. Instead, the predictions need to be translated into an informed hypothesis \(\mathcal{H}_r\) that reflects the predicted ordinal relations among the parameters. Only then is it possible to adequately test whether the theory of weakly-ordered preference describes participants' choice behavior. Of course, researchers may be interested in more complex hypotheses, including ones that feature combinations of equality constraints, inequality constraints, and unconstrained category proportions. For instance, @nuijten2016prevalence hypothesized that articles published in social psychology journals would have higher error rates than articles published in other psychology journals. As in the previous example, the authors had no expectations about the exact error rate distribution across journals. Here, again, the standard tests are inadequate. Generally, by specifying informed hypotheses researchers and practitioners are able to ``add theoretical expectations to the traditional alternative hypothesis'' [@hoijtink2008bayesian, p. 2] and thus test hypotheses that relate more closely to their theories [@haaf2019capturngPreprint; @rijkeboer2008psychologists].

In the Bayesian framework, researchers may test hypotheses of interest by means of Bayes factors [@jeffreys1935some; @kass1995bayes]. Bayes factors quantify the extent to which the data change the prior model odds to the posterior model odds, that is, the extent to which one hypothesis outpredicts the other. Specifically, Bayes factors are the ratio of marginal likelihoods of the respective hypotheses. For instance, the Bayes factor for the informed hypothesis versus the encompassing hypothesis is defined as:
\begin{align*}
\text{BF}_{re} = \cfrac{\overbrace{p(\mathbf{x}\mid \mathcal{H}_r)}^{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_r$}}}}{\underbrace{p(\mathbf{x}\mid \mathcal{H}_e)}_{\substack{\text{Marginal likelihood}\\\text{under $\mathcal{H}_e$}}}},
\end{align*}
where the subscript \(r\) denotes the informed hypothesis and \(e\) denotes the encompassing hypothesis. Several available \texttt{R} packages compute Bayes factors for informed hypotheses. For instance, the package \textbf{multinomineq} [@heck2019multinomial] evaluates informed hypotheses for multinomial models as well as models that feature independent binomials. The package \textbf{BFpack} [@mulderBfpackInPress] evaluates informed hypotheses for statistical models such as univariate and multivariate normal linear models, generalized linear models, special cases of linear mixed models, survival models, and relational event models. The package \rev{\textbf{bain}} [@gu2019bain] evaluates informed hypotheses for structural equation models. Outside of \texttt{R}, the Fortran 90 program \textbf{BIEMS} [@mulder2012biems] evaluates informed hypotheses for multivariate linear models such as MANOVA, repeated measures, and multivariate regression. All these packages rely on one of two implementations of the encompassing prior approach [@klugkist2005bayesian; @sedransk1985bayesian] to approximate order constrained Bayes factors: the unconditional encompassing method [@klugkist2005bayesian ; @hoijtink2008bayesian; @hoijtink2011informative] and the conditional encompassing method [@gu2014bayesian; @laudy2006bayesian; @mulder2009bayesian; @mulder2014prior; @mulder2016bayes]. Even though the encompassing prior approach is currently the most common method to evaluate informed hypotheses, it becomes increasingly unreliable and inefficient as the number of restrictions increases or the parameter space of the restricted model decreases [@sarafoglou2020evaluatingPreprint]. For instance, simulation studies conducted by @sarafoglou2020evaluatingPreprint have illustrated that the unconditional encompassing approach is not able to produce Bayes factors when hypotheses with a large number of constrained parameters are considered (i.e., they considered 18 categories). For hypotheses with fewer categories (i.e., 5 or 6), the method worked well when the data provided either weak or moderate evidence in favor of or against the informed hypothesis. However, when the data provided extreme evidence against the predicted constraints, the method again failed to compute Bayes factors.

As alternative to the encompassing prior approach, @sarafoglou2020evaluatingPreprint recently proposed a bridge sampling routine [@bennett1976efficient; @meng1996simulating] that computes Bayes factors for informed hypotheses more reliably and efficiently. This routine is implemented in \textbf{multibridge} (\url{https://CRAN.R-project.org/package=multibridge}) and is suitable to evaluate inequality constraints for multinomial and binomial models as well as combinations between equality and inequality constraints. 

Here we showcase how the proposed bridge sampling routine by @sarafoglou2020evaluatingPreprint can be performed with \textbf{multibridge}. In the remainder of this article, we will introduce the package and its functionalities and describe the methods used to compute the informed hypotheses in binomial and multinomial models. We will illustrate its core functions using three examples and end with a brief discussion and future directions.

# Multibridge

The general workflow of \textbf{multibridge} is illustrated in Figure \ref{fig:scheme-multibridge}. The core functions of \textbf{multibridge}, that is \(\texttt{mult\_bf\_informed}\) and \(\texttt{binom\_bf\_informed}\), return the Bayes factor estimate in favor of or against the informed hypothesis. To compute a Bayes factor, the core functions require the observed counts, the informed hypothesis, the parameters of the prior distribution under \(\mathcal{H}_e\), and the category labels. An overview of the basic required arguments of the two core functions are provided in Table \ref{table:arguments}.

When calling \texttt{mult\_bf\_informed} or \texttt{binom\_bf\_informed}, the user specifies the data values (\texttt{x} and \texttt{n} for binomial models and \texttt{x} for multinomial models, respectively), the informed hypothesis (\texttt{Hr}), the \(\alpha\) and \(\beta\) parameters of the binomial prior distributions (\texttt{a} and \texttt{b}) or the concentration parameters for the Dirichlet prior distribution (\texttt{a}), respectively, and the category labels of the factor levels (\texttt{factor\_levels}). The functions then return the estimated Bayes factor for the informed hypothesis relative to the encompassing hypothesis that imposes no constraints on the category proportions or the null hypothesis which states that all category proportions are equal. Based on these results different S3 methods can be used to get more detailed information on the individual components. For instance, users can extract the Bayes factor with the \texttt{bayes\_factor}-method, visualize the posterior parameter estimates under the encompassing hypothesis using the \texttt{plot}-method, or get more detailed information on how the Bayes factor is composed using the \texttt{summary}-method. Table \ref{table:s3_methods} summarizes all S3 methods currently available in \textbf{multibridge}.

(ref:scheme-multibridge-caption) The \textbf{multibridge} workflow. The functions \texttt{mult\_bf\_informed} or \texttt{binom\_bf\_informed} return the estimated Bayes factor for the informed hypothesis relative to the encompassing or the null hypothesis. Based on these results different S3 methods can be used to get more detailed information on the individual components of the analysis (e.g., \texttt{summary}, \texttt{bayes\_factor}), and parameter estimates of the encompassing distribution (\texttt{plot}).
```{r scheme-multibridge, fig.cap='(ref:scheme-multibridge-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/scheme-multibridge.pdf")
``` 

## Supported Hypotheses

The following hypotheses are supported in \textbf{multibridge}. Users can test hypotheses on equality and inequality constraints among parameters (left column in Figure \ref{fig:hypotheses})\rev{. We consider inequality constraints, for instance, in Example 3 of this manuscript, when we test whether the probability to violate stochastic dominance decreases for persons with higher education levels} [@myung2005bayesian].

Additionally, \textbf{multibridge} supports the evaluation of combinations of equality constraints, inequality constraints, and free parameters (middle column). As an example, the hypothesis in the top middle panel identifies a largest parameter ($\theta_1$) and a smallest parameter ($\theta_5$), and equates the remaining parameters ($\theta_2 = \theta_3 = \theta_4$). \rev{Combinations of constraints are considered, for instance, in Example 2 of this manuscript. Based on} @nuijten2016prevalence \rev{we test whether the proportion of statistical reporting errors is higher for articles published in the \emph{Journal of Personality and Social Psychology} (JPSP) than for articles published in seven other high-profile psychology journals.}

The package also supports the computation of Bayes factors for multiple independent constraints\rev{, representing, for instance, two main effects} (right column). For instance, the hypothesis in the bottom right panel describes an inequality constraint on the first three category proportions ($\theta_1 > \theta_2 > \theta_3$) and an equality constraint on the fourth and fifth category proportion ($\theta_4 = \theta_5$).

(ref:hypotheses-caption) \textbf{multibridge} supports informed hypotheses including inequality and equality constraints (left column), combinations of inequality and equality constraints and free parameters (middle column), and multiple independent constraints (right column). Parameters with larger values appear higher in the drawing. A prerequisite of \textbf{multibridge} is that all elements within a constraint can be arranged as a linearly ordered set.
```{r hypotheses, fig.cap='(ref:hypotheses-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/supported-hyps.pdf")
``` 

An important requirement for the hypotheses supported in \textbf{multibridge} is that within each independent constraint, all elements are arranged as a linearly ordered set. Elements can refer to individual parameters as shown in the top left panel of Figure \ref{fig:hypotheses}. In this example, for each pair of elements one precedes the other in the sequence (i.e., $\theta_1$ precedes $\theta_2$ and $\theta_2$ precedes $\theta_3$). Elements can also refer to a group of equality constrained parameters or a group of free parameters as shown in the middle panel of Figure \ref{fig:hypotheses}. In the top middle panel, too, for each pair of elements one precedes the other in the sequence (e.g., $\theta_1$ precedes $(\theta_2 = \theta_3 = \theta_4)$ and $(\theta_2 = \theta_3 = \theta_4)$ precedes $\theta_5$). That is, if the constraint was to be drawn as a Hasse diagram or specified as a character vector, the constrained elements should string together like a chain, ranging from the smallest element to the largest. We refer to these hypotheses as "stick hypotheses".

Conversely, ``branched hypotheses'', are hypotheses in which elements are not arranged as a linearly ordered set but as a partial order, meaning that some but not all pairs of elements precede one another. These hypotheses are \emph{not} supported in \textbf{multibridge}. Examples for branched hypotheses are shown in Figure \ref{fig:branch}. For instance, the hypothesis illustrated in the left panel states that $\theta_1$ precedes all other parameters. In addition, the hypothesis orders the branches ($\theta_2$, $\theta_3$, $\theta_4$) and ($\theta_5$, $\theta_6$, $\theta_7$). However, it remains unclear whether, for instance, $\theta_3$ or $\theta_5$ precede the other in the sequence. Thus, not all pairs of elements are comparable. Similarly, in all three examples of branched hypotheses it is unclear whether $\theta_3$ precedes or follows $\theta_6$. Researchers whose theories give rise to branched hypotheses and wish to test them can do so using one of the alternative \texttt{R} packages, for instance, \textbf{multinomineq} by @heck2019multinomial.

(ref:branch-caption) Examples of three hypotheses in which elements in a constraint are arranged as a partial order. In each panel there exist elements that are not comparable with each other, that is, for which neither element precedes the other in the sequence. The partial order shows itself in the branching of the Hasse diagram. These branched hypotheses are currently not supported in \textbf{multibridge}.
```{r branch, fig.cap='(ref:branch-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/branched-hyps.pdf")
``` 

When an informed hypothesis includes combinations of equality and inequality constraints, the core functions in \textbf{multibridge} split the hypothesis to compute Bayes factors separately for imposed equality constraints (for which the Bayes factor has an analytic solution) and inequality constraints (for which the Bayes factor is estimated using bridge sampling). Hence, for hypotheses that include combinations of equality and inequality constraints the \texttt{bayes\_factor} method separately returns the Bayes factor for the equality constraints and the conditional Bayes factor for the inequality constraints given the equality constraints.

The informed hypothesis \texttt{Hr} can be conveniently specified as a string or a character vector describing the relations among the category proportions. A simple ordering of three category proportions, \(\theta_1 > \theta_2 > \theta_3 \), can be specified either as \texttt{c("t1", "$>$", "t2", "$>$", "t3")}, or as \texttt{"t1 $>$ t2 $>$ t3"}. To assign labels to the parameters, they must be passed to the argument \texttt{factor\_levels}. \textbf{multibridge} then assumes that the order within the category labels correspond to the order of the data vector. Alternatively, the informed hypotheses can be specified using indices (e.g., \texttt{"1 $>$ 2 $>$ 3"}). To avoid circularity, an index or category label can be used only once within an informed hypothesis.
 
\begin{table}[H]
\caption{To estimate the Bayes factor in favor for or against the specified informed hypothesis, the user provides the core functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} with the basic required arguments listed below.}
\label{table:arguments}
\begin{center}
\begin{tabular}{p{4cm}p{12cm}}
        \toprule
Argument & Description \\\midrule
\texttt{x} & \texttt{numeric}. Vector with data (for multinomial models) or a vector of counts of successes, or a two-dimensional table (or matrix) with 2 columns, giving the counts of successes and failures, respectively (for binomial models).  \\
\texttt{n} &  \texttt{numeric}. Vector with counts of trials. Must be the same length as \texttt{x}. Ignored if \texttt{x} is a matrix or a table. Included only in \texttt{binom\_bf\_informed}. \\
\texttt{Hr} & \texttt{string} or \texttt{character}. \rev{String or a character vector} with the user specified informed hypothesis. Parameters may be referenced by the specified \texttt{factor\_levels} or by numerical indices.\\
\texttt{a} & \texttt{numeric}. Vector with concentration parameters of Dirichlet distribution (for multinomial models) or $\alpha$ parameters for independent beta distributions (for binomial models). Must be the same length as \texttt{x}. Default sets all parameters to 1. \\
\texttt{b} & \texttt{numeric}. Vector with $\beta$ parameters. Must be the same length as \texttt{x}. Default sets all $\beta$ parameters to 1. Included only in \texttt{binom\_bf\_informed}.\\
\texttt{factor\_levels} &  \texttt{character}. Vector with category labels. Must be the same length as \texttt{x}.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}
 
Signs permitted to specify informed hypotheses are the ``$<$''-sign and ``$>$''-sign for inequality constraints, the ``$=$''-sign for equality constraints, the ``$,$''-sign for parameters that vary freely within a constraint, and the ``\&''-sign to connect multiple independent constraints. For instance, the informed hypothesis in the top right panel in Figure \ref{fig:hypotheses}, that is, \texttt{"t1 $>$ t2 $>$ t3 $\&$ t4 , t5 $>$ t6"}, states that \texttt{t1} is bigger than \texttt{t2}, and \rev{that \texttt{t2}, is bigger than \texttt{t3}}. In addition, the hypothesis states that \texttt{t4} and \texttt{t5} are bigger than \texttt{t6}, with no further constraints imposed among \texttt{t4} and \texttt{t5}.

```{r, eval = FALSE}
x <- c(2, 1, 5, 4, 1, 8)
n <- c(10, 7, 13, 7, 9, 14)
a <- b <- c(1, 1, 1, 1, 1, 1)

# Testing ordinal relations and equality constraints
mult_bf_informed(x=x, a=a, Hr='1 > 2 > 3')
mult_bf_informed(x=x, a=a, Hr='1 = 2 = 3')

binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 > 2 > 3')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 = 2 = 3')

# Testing combinations of ordinal constraints, 
# equality constraints, and free parameters
mult_bf_informed(x=x, a=a, Hr='1 = 2 = 3 > 4 > 5 = 6')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 < 2 , 3 , 4 < 5 = 6')

# Testing combinations of independent constraints
mult_bf_informed(x=x, a=a, Hr='1 > 2 > 3 & 4 , 5 = 6')
binom_bf_informed(x=x, n=n, a=a, b=b, Hr='1 > 2 > 3 & 5 = 6')
```

When testing equality constrained hypotheses, users should be aware that there is a difference between assuming equality of category proportions and adding categories together, that is, the hypothesis $\mathcal{H}_r: \theta_1 = \theta_2 > \theta_3 = \theta_4$ differs from the hypothesis $\mathcal{H}_r: \theta_1 + \theta_2 > \theta_3 + \theta_4$. The first hypothesis concerns four category proportions of which two pairs are expected to be equal; as a result, we assign a $K = 4$ Dirichlet prior to this distribution. The second hypothesis concerns only two categories since we assume that $\theta_1$ and $\theta_2$ belong to one group and $\theta_3$ and $\theta_4$ belong to the other. Consequently, one assigns a $K = 2$ Dirichlet prior to this distribution. Therefore, to test the second hypothesis, the respective counts of the categories should first be combined and the analysis should be performed on the basis of these new data.

```{r, eval = FALSE}
# Merging categories or setting them equal do not yield the same results
# Hr: t1 = t2 < t3 = t4
x <- c(20, 7, 5, 9)
a <- c(1, 1, 1, 1)
summary(mult_bf_informed(x=x, a=a, Hr='1 = 2 > 3 = 4'))$bf

# Hr: t1 + t2 < t3 + t4
x <- c(20 + 7, 5 + 9)
a <- c(1, 1)
summary(mult_bf_informed(x=x, a=a, Hr='1 > 2'))$bf
```

\begin{table}[H]
\caption {S3 methods available in $\textbf{multibridge}$.}
\label{table:s3_methods}
\begin{center}
\begin{tabular}{p{4cm}p{3.5cm}p{9cm}}
        \toprule
Function Name(s) & S3 Method & Description \\\midrule
$\texttt{mult\_bf\_informed}$, $\texttt{binom\_bf\_informed}$ & $\texttt{print}$ & Prints model specifications and descriptives. \\
 & $\texttt{summary}$ &  Prints and returns the Bayes factor and associated hypotheses for the full model, and all equality and inequality constraints.\\
  & $\texttt{plot}$ & Plots the posterior median and credible interval of the parameter estimates of the encompassing model. Default sets credible interval to 95\%.\\
 & $\texttt{bayes\_factor}$ & Contains all Bayes factors and log marginal likelihood estimates for inequality constraints.\\
 & $\texttt{samples}$ & Extracts prior and posterior samples from constrained densities (if bridge sampling was applied). \\
& $\texttt{bridge\_output}$  &  Extracts bridge sampling output and associated error measures.\\
& $\texttt{restriction\_list}$ & Extracts restriction list and associated informed hypothesis. \\
$\texttt{mult\_bf\_inequality}$, $\texttt{binom\_bf\_inequality}$  & $\texttt{print}$ & Prints the bridge sampling estimate for the log marginal likelihood and the corresponding percentage error. \\
& $\texttt{summary}$ & Prints and returns the bridge sampling estimate for the log marginal likelihood and associated error terms.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

In \textbf{multibridge}, the functions \texttt{mult\_bf\_informed} and \texttt{binom\_bf\_informed} perform all necessary analysis steps. Other available functions compute Bayes factors for hypotheses that postulate only equality or only inequality constraints, and draw from constrained multinomial distributions and distributions of multiple independent binomials. A list of all currently available functions and data sets is given in Table \ref{table:core_functions}.

\begin{table}[H]
\caption {Core functions available in $\textbf{multibridge}$.}
\label{table:core_functions}
\begin{center}
\begin{tabular}{p{5.5cm}p{10.5cm}}
        \toprule
Function Name(s) & Description \\\midrule
$\texttt{mult\_bf\_informed}$ & Evaluates informed hypotheses on multinomial parameters.  \\
$\texttt{mult\_bf\_inequality}$ & Estimates the marginal likelihood of a constrained prior or posterior Dirichlet distribution.  \\
$\texttt{mult\_bf\_equality}$ & Computes Bayes factor for equality constrained multinomial parameters using the standard Bayesian multinomial test.  \\
$\texttt{mult\_tsampling}$ & Samples from constrained prior or posterior Dirichlet density.\\
$ \texttt{lifestresses}, \texttt{peas}$ & Data sets associated with informed hypotheses in multinomial models.\\\midrule
$\texttt{binom\_bf\_informed}$ & Evaluates informed hypotheses on binomial parameters.  \\
$\texttt{binom\_bf\_inequality}$ & Estimates the marginal likelihood of constrained prior or posterior beta distributions.\\
$\texttt{binom\_bf\_equality}$ & Computes Bayes factor for equality constrained binomial parameters. \\
$\texttt{binom\_tsampling}$ & Samples from constrained prior or posterior beta densities.\\
$ \texttt{journals}$ & Data set associated with informed hypotheses in binomial models.\\\midrule
$ \texttt{generate\_restriction\_list}$ & Encodes the informed hypothesis.\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

# Methodological Background

In this section we provide background information on the methods implemented in \textbf{multibridge}. Specifically, we formalize multinomial models and models that feature independent binomial probabilities, and define Bayes factors for the Bayesian multinomial test and testing equality of multiple independent binomial probabilities. Furthermore, the section discusses the influence of priors on the Bayes factors, illustrates how to compute posterior model probabilites and how to compare two informed hypotheses with each other, and provides a non-technical introduction into the bridge sampling routine implemented in \textbf{multibridge}. Mathematical details of the methods and principles discussed here can be found in @sarafoglou2020evaluatingPreprint and @gronau2017tutorial.

In the binomial model, we assume that the elements in the vector of successes \textbf{x} and the elements in the vector of total number of observations \textbf{n} in the \(K\) categories follow independent binomial distributions $\textbf{x}  \sim \prod_{k = 1}^K \text{Binomial}(\theta_k, n_k)$, where \(\theta_k\) is the \(k\)th category proportion. From this distribution we can derive the likelihood of the data given the parameters:
\[
p(\mathbf{x} \mid \boldsymbol{\theta}) = \prod_{k=1}^K {{n_k}\choose{x_k}}\theta_k^{x_k}(1-\theta_k)^{n_k-x_k}.
\]

The parameter vector of the binomial success probabilities \(\boldsymbol{\theta}\) contains the underlying category proportions and assume that categories are independent. Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is a vector of independent beta distributions with parameters \(\boldsymbol{\alpha}\) and \(\boldsymbol{\beta}\), thus $\boldsymbol{\theta} \sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k)$. The prior density is given by:
\[
p(\boldsymbol{\theta}) = \prod_{k=1}^K \frac{ \theta_k^{\alpha_k - 1}(1-\theta_k)^{\beta_k - 1}}{\text{B}(\alpha_k\text{, }\beta_k)},
\]
where B$(\alpha_k\text{, }\beta_k)$ is the beta function:
\[
\text{B}(\alpha_k\text{, }\beta_k) = \frac{\Gamma(\alpha_k)\Gamma(\beta_k)}{\Gamma(\alpha_k + \beta_k)}.
\]

The multinomial model generalizes the binomial model for cases where \(K > 2\). In this model, we assume that the vector of observations \textbf{x} in the \(K\) categories follows a multinomial distribution in which the parameters of interest, \(\boldsymbol{\theta}\), represent the underlying category proportions, thus $\textbf{x} \sim \text{Multinomial}(x_+, \boldsymbol{\theta})$, where $x_+ = \sum_{k=1}^K x_k$.

Since the \(K\) categories are dependent, the vector of probability parameters is constrained to sum to one, such that \(\sum_{k = 1}^K (\theta_1, \cdots, \theta_K) = 1\). Therefore, a suitable choice for a prior distribution for \(\boldsymbol{\theta}\) is the Dirichlet distribution with concentration parameter vector \(\boldsymbol{\alpha}\), $\boldsymbol{\theta} \sim \text{Dirichlet}(\boldsymbol{\alpha}):$
\[
p(\boldsymbol{\theta}) = \frac{1}{\text{B}(\boldsymbol{\alpha})}\, \prod_{k=1}^K\, \theta_k^{\alpha_k-1},
\]
where B$(\boldsymbol{\alpha})$ is the multivariate beta function:
\[
\text{B}(\boldsymbol{\alpha}) = \cfrac{\prod_{k = 1}^K \Gamma(\alpha_k)}{\Gamma \left( \sum_{k = 1}^K \alpha_k \right)}. 
\]

In \texttt{multibridge}, we have deliberately chosen to leave the priors at the original scale (i.e., in the probability space), because it makes it easier to express ones expectations about data patterns. Alternative approaches transform the model parameters into the probit space, which has the advantage that correlations can be specified for hierarchical models [e.g., as in the latent-trait model for multinomial processing tree models, @klauer2010hierarchical; @matzke2015bayesian]. However, these transformations make the development of priors more difficult and can lead to unintended consequences; for instance, a uniform prior on the probit scale does not translate to a uniform prior on the probability scale [as discussed in @heck2016adjusted].

## Developing Suitable Prior Distributions

In the binomial and multinomial model, the concentration parameters have an intuitive interpretation. In the binomial model, the parameters $\alpha_k$ can be interpreted as vector of \emph{a priori} successes that observations fall within the various categories and $\beta_k$ can be interpreted as vector of \emph{a priori} failures. Likewise, in the multinomial model, \(\alpha_k\) can be interpreted as vector of \emph{a priori} category counts. It follows, that the higher the number of concentration parameters is, the information the prior contains and the more influence it has on parameter estimation and hypothesis testing.

Developing suitable prior distributions for Bayesian inference is a much discussed topic involving various theoretical and computational considerations (see @consonni2018prior for a review paper on prior distributions for objective Bayesian analysis). Therefore, recommending approaches for developing appropriate prior distributions is, in our view, a difficult undertaking. In this section, we therefore present a selected subset of approaches that we consider particularly suitable for assigning adequate priors for the multiple binomials model and the multinomial model.

If researchers possess no knowledge or expectations about the plausible parameter values, a uniform distribution can be assigned across the parameter space. This prior assumes that before seeing the data, each category contains one observation, that is, all concentration parameters are set to one. A uniform prior distribution, puts equal probability mass on all permitted parameter values, similar to the adjusted priors for reparametrized models proposed by @heck2016adjusted (see Figure \ref{fig:prior}). However, \textbf{multibridge} allows priors to be set on the original scale.

(ref:prior-caption) The development of a prior distribution should be accompanied by a visual inspection of the prior predictive. Here we display three prior distributions on two binomial probabilities that are constrained to be \(\theta_1 < \theta_2\). The uniform distribution (panel a) assigns equal mass to all permissible values of the constrained space. A symmetric prior (panel b) concentrates the mass in the center of the distribution. A prior describing a constraint in the opposite direction (panel c), puts most of the density along the diagonal.
```{r prior, echo = FALSE, message = FALSE, fig.cap = "(ref:prior-caption)", out.width = "75%", warning=FALSE, fig.align = 'center'}
knitr::include_graphics("prior.pdf")
# set.seed(4491)
# x <- c(10, 10)
# n <- c(20, 20)
# a1 <- c(1, 1)
# b1 <- c(1, 1)
# a2 <- c(10, 10)
# b2 <- c(10, 10)
# a3 <- c(15, 5)
# b3 <- c(5, 15)
# factor_levels <- c('t1', 't2')
# Hr <- c('t1 < t2')
# He <- c('t1 , t2')
# N <- 1.5e3
# 
# # generate restriction list
# inequalities1 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a1, b=b1,
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr1 <- binom_tsampling(inequalities1, niter = N, prior=TRUE)
# 
# inequalities2 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a2, b=b2,
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr2 <- binom_tsampling(inequalities2, niter = N, prior=TRUE)
# 
# inequalities3 <- generate_restriction_list(x=x, n=n, Hr=Hr, a=a3, b=b3,
# factor_levels=factor_levels)$inequality_constraints
# prior_samples_Hr3 <- binom_tsampling(inequalities3, niter = N, prior=TRUE)
# 
# xrange <- yrange <- c(0, 1)
# 
# 
# p1 <- .correlation_plot(x = prior_samples_Hr1[, 1],
#                         y = prior_samples_Hr1[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title = expression(paste(theta[1], " ~ Beta(1, 1)   ", theta[2], " ~ Beta(1, 1)")),
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# p2 <- .correlation_plot(x = prior_samples_Hr2[, 1],
#                         y = prior_samples_Hr2[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title=expression(paste(theta[1], " ~ Beta(10, 10)   ", theta[2], " ~ Beta(10, 10)")),
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# p3 <- .correlation_plot(x = prior_samples_Hr3[, 1],
#                         y = prior_samples_Hr3[, 2],
#                         yrange = yrange,
#                         xrange = xrange,
#                         title=expression(paste(theta[1], " ~ Beta(5, 15)   ", theta[2], " ~ Beta(15, 5)")),
#                         binwidth=0.15,
#                         col = c('#FB9A99'))
# 
# top_row <- cowplot::plot_grid(p1, p2, nrow = 1,
#                         labels=c('a)', 'b)'), label_size = 12, 
#                         rel_widths = c(1, 1))
# bottom_row <- cowplot::plot_grid(NULL, p3, NULL, ncol = 3,
#                         labels=c('', 'c)', ''), label_size = 12, 
#                         rel_widths = c(0.5, 1, 0.5))
# 
# p <- cowplot::plot_grid(top_row, bottom_row, nrow = 2,
#                         labels=NULL, label_size = 12)
``` 


We recommend incorporating prior knowledge into the models whenever possible. Based on theories, expert knowledge, or informed guesses, researchers often have expectations about plausible and implausible parameter values. In these cases, the prior should match these expectations [@lee2018determining]. For instance, in the case of informed hypotheses, prior counts can be chosen to match a particular expected ordinal trend. To determine whether the chosen priors are consistent with the theory, researchers can visualize and assess prior predictive distributions, that is, the distribution of the model parameters and data patterns predicted by the priors [@gabry2019visualization; @schad2021toward; @wagenmakers2021seven]. The developed priors should reflect expectations about the parameters and make sensible predictions. This is particularly important for Bayes factor hypothesis testing; when the purpose is parameter estimation, however, it may be more informative to assign prior parameter distributions that are relatively wide [e.g., @van2021jasp].

Furthermore, one can choose the observed category counts of previous studies as priors for the current one, as is often suggested for replication studies and referred to as ``Bayesian learning'' [e.g., @verhagen2014bayesian]. This approach constructs highly informative priors; instead of describing the new data as precisely as possible, the goal with this approach is quantify the additional knowledge gained by the new data. Finally, priors can be constructed using a fraction of the likelihood of the data while centering it on the the mean of the parameter range [@mulder2014prior; @gu2018approximated].

## Bayes factor


\textbf{multibridge} features two different methods to compute Bayes factors: one method computes Bayes factors for equality constrained parameters (which can be computed analytically) and one method computes Bayes factors for inequality constrained parameters (which needs to be approximated). In cases where informed hypotheses feature combinations between inequality and equality constraints, \textbf{multibridge} computes the overall Bayes factor \(\text{BF}_{re}\) by multiplying the individual Bayes factors for both constraint types. This is motivated by the fact that the Bayes factor for combinations \rev{($\text{BF}_{re}$)} will factor into a Bayes factor for the equality constraints \rev{($\text{BF}_{1e}$)} and a conditional Bayes factor for the inequality constraints given the equality constraints \rev{($\text{BF}_{2e \mid 1e}$). For instance, to evaluate the hypothesis $\mathcal{H}_r: \theta_1 > \theta_2 = \theta_3$, \textbf{multibridge} factors the Bayes factor as follows:}

\[
\text{BF}_{re} 
= \underbrace{\frac{p(\theta_1 > \theta_{23} \mid \theta_2 = \theta_3 \text{, }\mathbf{x}\text{, } \mathcal{H}_e)}{p(\theta_1 > \theta_{23} \mid \theta_2 = \theta_3\text{, } \mathcal{H}_e)}}_{\text{BF}_{2e \mid 1e}} \times \underbrace{\frac{p(\theta_2 = \theta_3 \mid \mathbf{x}\text{, } \mathcal{H}_e)}{p(\theta_2 = \theta_3 \mid \mathcal{H}_e)}}_{\text{BF}_{1e}},
\]

\rev{where the subscript $1$ denotes the hypothesis that only features
equality constraints, the subscript $2$ denotes the hypothesis that
only features inequality constraints, and $p(\theta_1 > \theta_{23} \mid \theta_2 = \theta_3)$ refers to a Dirichlet integral, where the category proportions $\theta_2$ and $\theta_3$ are collapsed. See} @sarafoglou2020evaluatingPreprint \rev{for the proof and a detailed account of this method.}

### Testing Equality Constraints

For equality constrained binomial models \textbf{multibridge} supports two null hypotheses, one stating that all parameters are equal and one stating that all parameters are equal to a specific value. Both null hypotheses are tested against an encompassing hypothesis. Under the encompassing hypothesis, we specify a Beta$(\alpha_k\text{, }\beta_k)$ prior on each of the $\theta_k$ that yields the following marginal likelihood:
\[
p(\mathbf{x} \mid \mathcal{H}_e) = \frac{\prod_{k=1}^K {{n_k}\choose{x_k}} \times \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}.
\]

Under the first null hypothesis which states that all binomial probabilities are set equal without a constraint on a specific value, we collapse all individual Beta$(\alpha_k\text{, }\beta_k)$ priors and correct for the change in categories; if $K$ categories are collapsed, $K-1$ is subtracted from the concentration parameters. The resulting prior is a Beta$(\alpha_+ - (K - 1)\text{, }\beta_+ - (K - 1))$ distribution on $\theta$, where $\alpha_+ = \sum_{k=1}^K \alpha_k$ and $\beta_+ = \sum_{k=1}^K \beta_k$. Hence, a Beta$(1\text{, }1)$ prior on each individual category proportion yields again a Beta$(1\text{, }1)$ prior on the categories that are collapsed. When the prior is more informative, say a Beta$(2\text{, }2)$ prior on three individual category proportions, it would result in a Beta$(4\text{, }4)$ prior on $\theta$ as the information available is added together. The corresponding marginal likelihood takes the following form:
\[
p(\mathbf{x} \mid \mathcal{H}_{01}) = \frac{ \prod_{k=1}^K{{n_k}\choose{x_k}} \times \text{B}(x_+ + \alpha_+ - (K - 1)\text{, }n_+-x_+ +\beta_+ - (K - 1))}{\text{B}(\alpha_+ - (K - 1)\text{, }\beta_+ - (K - 1))}.
\]

We can now compute the Bayes factor $\text{BF}_{01e}$ as follows:
\begin{align*}
\text{BF}_{0e} &= \frac{p(\mathbf{x} \mid \mathcal{H}_0)}{p(\mathbf{x} \mid \mathcal{H}_e)} \\
&= \frac{\frac{ \prod_{k=1}^K{{n_k}\choose{x_k}} \times \text{B}(x_+ + \alpha_+ -(K - 1)\text{, }n_+-x_+ +\beta_+ - (K - 1))}{\text{B}(\alpha_+ - (K - 1)\text{, }\beta_+ - (K - 1))}
}{\frac{\prod_{k=1}^K {{n_k}\choose{x_k}} \times \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}} \\
&=\frac{ \prod_{k=1}^K \text{B}(x_+ + \alpha_+ - (K - 1)\text{, }n_+-x_+ +\beta_+ -(K - 1))
}{\prod_{k=1}^K \text{B}(x_k + \alpha_k\text{, }n_k - x_k + \beta_k)} \times \frac{\prod_{k=1}^K \text{B}(\alpha_k\text{, }\beta_k)}{\text{B}(\alpha_+ - (K - 1)\text{, }\beta_+ - (K - 1))}
\end{align*}

The second null hypothesis states that all binomial probabilities in a model are assumed to be exactly equal \textit{and} equal to a predicted value $\theta_0$. Under this hypothesis, the prior reduces to a single point and the marginal likelihood simplifies to the likelihood function:

\[
p(\mathbf{x} \mid \mathcal{H}_{02}) = \theta_0^{x_+}(1 - \theta_0)^{n_+ - x_+} \times \prod_{k=1}^K{{n_k}\choose{x_k}}.
\]

The Bayes factor for the second null hypothesis is then defined as:
\[
\text{BF}_{02e}
= \frac{\prod_{k=1}^K \text{B}(\alpha_k \text{, } \beta_k)}{\prod_{k=1}^K \text{B}(\alpha_k + x_k\text{, } \beta_k + n_k - x_k)} \times \theta_{0}^{x_+} (1 - \theta_{0})^{n_+ - x_+}.
\]
Note that \textbf{multibridge} only supports the specification of one predicted value for all binomial probabilities. 

```{r, eval=FALSE, echo=TRUE}
x <- c(3, 4, 10, 11)
n <- c(15, 12, 12, 12)
a <- c(1, 1, 1, 1)
b <- c(1, 1, 1, 1)
# assuming all binomial proportions are equal
binom_bf_equality(x=x, n=n, a=a, b=b)
# assuming all binomial proportions are equal 
# and equal to a predicted value
binom_bf_equality(x=x, n=n, a=a, b=b, p = 0.5)
```

The Bayes factor \(\text{BF}_{0e}\) for the multinomial test is defined as:
\[
\text{BF}_{0e} =  
% \frac{ \text{B}\left(\alpha_{1}\text{, }\dots\text{, }\alpha_K\right)}{\text{B}\left(\alpha_1+x_1\text{, }\dots\text{, }\alpha_K+x_K\right)} \, \times 
\frac{\text{B}(\boldsymbol{\alpha})}{\text{B}(\boldsymbol{\alpha}+\mathbf{x})} \, \times  \prod_{k=1}^K \theta_{0k}^{x_k},
\]
where \(\theta_{0k}\) represent the predicted category proportions [see @sarafoglou2020evaluatingPreprint for the derivation]. For multinomial models, under the null hypothesis, category probabilities can either all be set equal (i.e., all category probabilities are \(\tfrac{1}{K}\)) or can replaced with the user-specified predicted values.

```{r, eval=FALSE, echo=TRUE}
x <- c(3, 4, 10, 11)
a <- c(1, 1, 1, 1)
# assuming all category proportions are exactly equal
mult_bf_equality(x=x, a=a)
# specifying predicted values
mult_bf_equality(x=x, a=a, p = c(0.1, 0.1, 0.3, 0.5))
```

### Testing Inequality Constraints

For inequality constrained binomial and multinomial models, users can specify informed hypotheses that are either tested against a null hypothesis postulating that all parameters are equal or against the encompassing hypothesis which lets all parameters free to vary. Generally, to obtain the marginal likelihood of the informed hypothesis, it is necessary to integrate over the restricted parameter space, which is difficult to compute. As a solution to the problem of computing marginal likelihood of the informed hypothesis, @klugkist2005bayesian derived an identity that defines the Bayes factor \(\text{BF}_{re}\) as the ratio of proportions of posterior and prior parameter space consistent with the restriction. This identity forms the basis of the encompassing prior approach. Recently, @sarafoglou2020evaluatingPreprint highlighted that these proportions can be reinterpreted as the marginal likelihoods (i.e., the normalizing constants) of the constrained posterior and constrained prior distribution. The prior distribution consistent with the restriction takes the following form:

\[
p(\boldsymbol{\theta} \mid \mathcal{H}_r)
= \frac{p(\boldsymbol{\theta} \mid \mathcal{H}_e)\, \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)}{\int_{\mathcal{R}_e}\, p(\boldsymbol{\theta}\mid\mathcal{H}_r)\,\text{d}\boldsymbol{\theta}},
\]
where $\mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)$ is an indicator function that is one for parameter values in the that obey the constrained and zero otherwise. The constrained posterior distribution of the parameters under the informed hypothesis can be represented in the same way,

\[
p(\boldsymbol{\theta} \mid \mathbf{x}\text{, }\mathcal{H}_r)
= \frac{p(\boldsymbol{\theta} \mid \mathbf{x}\text{, } \mathcal{H}_e)\, \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)}{\int_{\mathcal{R}_e}\, p(\mathbf{x} \mid \boldsymbol{\theta})\, p(\boldsymbol{\theta}\mid\mathcal{H}_r)\,\text{d}\boldsymbol{\theta}}.
\]
The Klugkist identity [@klugkist2005bayesian] can be derived from the marginal likelihoods of the two distributions as follows:

\begin{align}
\label{Eq:klugkistIdentity}
\text{BF}_{re} &= \frac{\overbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathbf{x}\text{, }\mathcal{H}_e)}^{\substack{\text{Marginal likelihood of}\\\text{constrained posterior distribution}}}}{\underbrace{p(\boldsymbol{\theta} \in \mathcal{R}_r \mid  \mathcal{H}_e)}_{\substack{\text{Marginal likelihood of}\\\text{constrained prior distribution}}}}.
\end{align}

The Klugkist identity made it possible to utilize numerical sampling methods such as bridge sampling to compute the Bayes factor. The following section provides a conceptual introduction to bridge sampling and how it is used in the context of evaluating informed hypotheses.

## Bridge Sampling Routine

The bridge sampling routine implemented in \textbf{multibridge} is a numerical method to estimate the marginal likelihood of a target density [cf., @gronau2017tutorial; @overstall2010default]. The identity used in bridge sampling is displayed in Equation \ref{Eq:bridgeidentity}; it considers the unnormalized target density, a proposal density with known normalizing constant, and an arbitrary bridge function. The numerator in Equation \ref{Eq:bridgeidentity} describes the expected value of the unnormalized target density evaluated with samples from the proposal density. The denominator is the expected value of the proposal density and a bridge function evaluated with samples from the target density. The bridge function serves the purpose of increasing the overlap between the two densities, thus increasing the efficiency and accuracy of the method. The bridge sampling identity can then be expressed as follows:

\begin{align}
    p(\boldsymbol{\theta} \in \mathcal{R}_r \mid \mathcal{H}_e) = \frac{\mathbb{E}_{g(\boldsymbol{\theta})}\left(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)h(\boldsymbol{\theta})\right)}{\mathbb{E}_{\text{prior}} \left(g(\boldsymbol{\theta})h(\boldsymbol{\theta})\right)},
    \label{Eq:bridgeidentity}
\end{align}
where the term \(h(\boldsymbol{\theta})\) refers to the bridge function proposed by @meng1996simulating, \(g(\boldsymbol{\theta})\) refers to a proposal density (in this application we choose the multivariate normal density), and \(p(\boldsymbol{\theta}\mid \mathcal{H}_e) \mathbb{I}(\boldsymbol{\theta}\in\mathcal{R}_r)\) is the unnormalized target density; in this case it represents the part of the prior parameter space under the encompassing hypothesis that is in accordance with the constraint. In the conventional application of bridge sampling, the marginal likelihoods of the two competing hypotheses are estimated, that is, the marginal likelihood of the informed hypothesis and the marginal likelihood of the encompassing hypothesis. But on the basis of  Equation \ref{Eq:klugkistIdentity}, the routine implemented in \textbf{multibridge} estimates the marginal likelihood of the restricted prior and restricted posterior density.

It should be noted that the bridge sampling algorithm implemented in \textbf{multibridge} is an adapted version of the algorithm implemented in the \texttt{R} package \textbf{bridgesampling} [@gronau2017bridgesampling] and allows for the specification of informed hypotheses on probability vectors.^[In addition, the function to compute the relative mean square error for bridge sampling estimates in \textbf{multibridge} is based on the code of the \texttt{error\_measures}-function from the \textbf{bridgesampling} package.]

A schematic representation of the bridge sampling routine is displayed in Figure \ref{fig:bridge}. To estimate the marginal likelihood, bridge sampling requires samples from the target distribution, that is, the constrained Dirichlet distribution for multinomial models and constrained beta distributions for binomial models, and samples from the proposal distribution which in principle can be any distribution with a known marginal likelihood; in \textbf{multibridge} the proposal distribution is the multivariate normal distribution. Samples from the target distribution are generated using the Gibbs sampling algorithms proposed by @damien2001sampling. For binomial models, we apply the suggested Gibbs sampling algorithm for constrained beta distributions. In the case of the multinomial models, we apply an algorithm that simulates values from constrained Gamma distributions which are then transformed into Dirichlet random variables. To sample efficiently from these distributions, \textbf{multibridge} provides a \texttt{C++} implementation of this algorithm. Samples from the proposal distribution are generated using the standard \texttt{rmvnorm}-function from the \texttt{R} package \textbf{mvtnorm} [@mvtnorm].

(ref:bridge-caption) A schematic illustration of the steps taken to estimate the marginal likelihood of the constrained prior distribution of two binomial probabilities under $\mathcal{H}_r: \theta_1 < \theta_2$. As starting point, the routine requires samples from the constrained prior distribution (red). Following a transformation to the real line, a multivariate normal distribution (blue) is fit to half of the samples. The results from evaluating the samples from the multivariate normal distribution and the constrained prior distribution at the respective other density are needed to compute the expected values displayed in Equation \ref{Eq:bridgeidentity}. As final step, the bridge sampling algorithm estimates the marginal likelihood of the constrained prior distribution using an iterative scheme.
```{r bridge, fig.cap='(ref:bridge-caption)', message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/bridge-sampling.pdf")
``` 

Despite the bridge function, the efficiency of the bridge sampling method is optimal only if the target and proposal distribution operate on the same parameter space and have sufficient overlap. We therefore probit transform the samples of the constrained distributions to move the samples from the probability space to the entire real line. Subsequently, we use half of these draws to construct the proposal distribution using the method of moments. Then, samples are drawn from the proposal density and transformed back into the probability space, ensuring that the samples correspond to the informed hypothesis. These transformed samples are then used to evaluate the unnormalized target density.

The numerator in Equation \ref{Eq:bridgeidentity} evaluates the unnormalized density for the constrained prior distribution with samples from the proposal distribution. The denominator evaluates the normalized proposal distribution with samples from the constrained prior distribution. Since the optimal bridge function proposed by @meng1996simulating contains the marginal likelihood of the target density --the quantity we wish to compute-- an iterative scheme is applied to obtain the estimate. \textbf{multibridge} then runs the iterative scheme until the tolerance criterion suggested by @gronau2017tutorial is reached. The sampling from the target and proposal distribution, the transformations and computational steps are performed automatically within the core functions of \textbf{multibridge}. The user only needs to provide the functions with the data, a prior and a specification of the informed hypothesis. As part of the standard output of \texttt{binom\_bf\_informed} and \texttt{mult\_bf\_informed}, the functions return the bridge sampling estimate for the log marginal likelihood of the target distribution, its associate relative mean square error and the number of iterations needed to until the bridge sampling estimator reached the tolerance criterion.

To summarize, in order to implement the bridge sampling method we only need to be able to sample from the constrained densities. Crucially, when using bridge sampling, it does not matter how small the constrained parameter space is in proportion to the encompassing density. This gives the method a decisive advantage over the encompassing prior approach in terms of accuracy and efficiency especially (1) when binomial and multinomial models with moderate to high number of categories (i.e., \(K > 10\)) are evaluated and (2) when relatively little posterior mass falls in the constrained parameter space.

## Stick-Breaking Transformation

The bridge sampling routine in \textbf{multibridge} uses the multivariate normal distribution as proposal distribution, which requires moving samples from target distribution to the real line and conversely, moving samples from the real line to the ordered probability space. Crucially, the transformation needs to retain the ordering of the parameters, that is, it needs to take into account the lower bound and the upper bound of each parameter. Elements from the real line to the ordered probability space are then transformed as follows: $$\theta_k = (u_k -l_k) \Phi(\xi_k)+l_k,$$ where \(\xi_k\) is \(k\)th the element on the real line, \(\Phi\) is the cumulative density function of a standard normal and \(u_k\) and \(l_k\) are the upper and lower bounds of \(\xi_k\), respectively. The largest element is simply the remainder of the stick. The inverse transformation is given by $$\xi_k = \Phi^{-1}\left(\frac{\theta_k - l_k}{u_k - l_k}\right),$$ where $\Phi^{-1}$ denote the inverse cumulative density function. To determine the bounds, \textbf{multibridge} uses a probit transformation, as proposed in @sarafoglou2020evaluatingPreprint, which transforms the elements by moving from the smallest to the largest value. A schematic illustration of the stick-breaking transformation is given in Figure \ref{fig:stick}, detailed technical details of the transformation are provided in the appendix.

(ref:stick-caption) The stick-breaking transformation of elements on the real line to the ordered probability space. The stick-breaking transformation moves from the smallest to the largest element to determine its lower and upper bounds.
```{r stick, fig.cap='(ref:stick-caption)',  message=FALSE, fig.align='center'}
knitr::include_graphics("scheme_multibridge/stick-breaking.pdf")
``` 

To perform the transformation from a parameter vector on the real line to an ordered probability vector, we need to determine the lower and upper bound of each parameter. Consider an increasing trend of four parameters, that is, $\theta_{1} < \theta_{2} < \theta_{3}$. The lower bound for for the smallest element in the parameter vector, $\theta_{1}$, is 0. For $\theta_{2}$ and $\theta_{3}$ the lower bound is the preceding element in the vector. That is, the lower bound for $\theta_{2}$ is $\theta_1$, lower bound for $\theta_{3}$ is $\theta_2$.

This definition holds for both binomial models and multinomial models. Differences in these two models appear only when determining the upper bound for each parameter. For binomial models, the upper bound for each parameter is $1$. For multinomial models, due to the sum-to-one constraint the upper bounds need to be computed differently. As proposed in @frigyik2010introduction and @stan2020 we represent $\boldsymbol{\theta}$ as unit-length stick which we subsequently divide into as many elements as there are parameters in the constraint [@stan2020]. In this approach, the upper bounds are derived from on the values of smaller elements as well as on the number of remaining larger parameters in the stick. Concretely, for the smallest element in the parameter vector, $\theta_{1}$, the upper bound is $\frac{1}{3}$; if this element were larger than that it would be impossible to create a probability vector with increasing values. For $\theta_{2}$ and $\theta_{3}$ the upper bound is the proportion of the unit-length stick that has not yet been accounted for in the transformation divided by the number of parameters in the remaining stick. For instance, the upper bound for $\theta_{2}$ is defined as $\frac{1 - \theta_1}{2}$. This transformation allows us to effectively transform elements from the real line to an constrained probability space and is therefore a main component of the bridge sampling algorithm.

The drawback of this transformation is, however, that it can only be performed if all elements in the constraint are arranged as a linearly ordered set, thus, only works for "stick hypotheses". For hypotheses in which elements in a constraint are arranfed as a partial order, the assumption is violated that for a given parameter smaller elements and the number of parameters in the remaining stick determine their upper bound.

## Poster Model Probabilites, and Bayes Factor Transitivity

Consider a scenario where researchers entertain more than two hypotheses that they wish to compare. For instance, they may entertain two informed hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) as well as a null hypothesis \(\mathcal{H}_{0}\) and the encompassing hypothesis \(\mathcal{H}_{e}\). An overview of the relative plausibility of all \(M=4\) models simultaneously may be obtained by presenting the posterior model probabilities for all hypotheses, \(p(\mathcal{H}_i \, | \, x)\), $i = 1, \cdots, 4$ @berger2005posterior. Posterior model probabilities are not automatically computed in \textbf{multibridge}; however, after computing the individual Bayes factors, the posterior model probabilities can be obtained easily. Denoting the prior model probability for hypothesis \(\mathcal{H}_{r1}\) by \(p(\mathcal{H}_{r1})\), the posterior model probability \(p(\mathcal{H}_{r1} \mid \mathbf{x})\) is given by:

\[ p(\mathcal{H}_{r1} \mid \mathbf{x}) = \frac{\frac{p(\mathbf{x} \mid \mathcal{H}_{r1})}{p(\mathbf{x} \mid \mathcal{H}_e)} \times p(\mathcal{H}_{r1})}{\displaystyle\sum\limits_{i = 1}^M \frac{p(\mathbf{x} \mid \mathcal{H}_i)}{p(\mathbf{x} \mid \mathcal{H}_e)} \times p(\mathcal{H}_i)}.\]

When all hypotheses are equally likely \emph{a priori}, this simplifies to:
\[
p(\mathcal{H}_{r1} \mid \mathbf{x}) = \frac{\text{BF}_{r1e}}{\text{BF}_{r1e} + \text{BF}_{r2e} + \text{BF}_{0e} + \text{BF}_{ee}},
\]
where $\text{BF}_{ee}$ equals 1. In R, the posterior model probabilities can be computed as follows:

```{r, eval = FALSE, echo = TRUE}
# posterior model probability of Hr1 given three alternative hypotheses
p_Hr1_x <- bfr1e/(bfr1e + bfr2e + bf0e + 1) # bfee = 1
```

Posterior model probabilities are useful for comparing multiple hypotheses; however, they are relative quantities that change depending on which other hypotheses are included in the comparison. Thus, hypotheses that describe the data poorly may have high posterior model probabilities if the other hypotheses in the comparison set provide even worse descriptions of the data. In order to gain insight into whether a hypothesis describes the data adequately, we therefore consider so-called bookend hypotheses along with theory-informed hypotheses. That is, we include a hypothesis that maximally constrains the parameter space (such as a point-null hypothesis \(\mathcal{H}_{0}\)) and the encompassing hypothesis \(\mathcal{H}_{e}\) that does not constrain the parameter space [in this case, that makes no ordinal predictions, @lee2018determining]. A hypothesis is then considered adequate if it outperforms these two bookend models.

In addition to posterior model probabilities, Bayes factors can also be calculated directly between two informed hypotheses. The comparison of any two informed hypotheses with one another follows from the fact that Bayes factors are transitive. For instance, the Bayes factor comparison between two informed hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) can be obtained by first computing \(\text{BF}_{r1e}\) and \(\text{BF}_{r2e}\), and then dividing out the common hypothesis \(\mathcal{H}_{e}\):
\[\text{BF}_{r1r2} = \frac{\text{BF}_{r1e}}{\text{BF}_{r2e}}.\] \rev{For this comparison to be feasible, the hypotheses of interest must be comparable, that is, the same prior distribution must be assigned to the category proportions.}

## Prior Sensitivity

Bayesian hypothesis testing has been criticised as the priors exert too much influence on the Bayes factors [e.g., @kass1995bayes]. That is, even if the data are informative enough to overwhelm the prior for parameter estimation, priors can still influence the Bayes factors. The development of suitable priors is thus an important part of Bayesian hypothesis testing.

But even priors that are justified by theory are to a certain degree arbitrary. For instance, if one expects an increasing trend in the data, the parameters in the prior can be chosen to reflect that trend. The exact number of \emph{a priori} category counts, however, is at the discretion of the analyst. It is therefore considered good research practice to conduct a sensitivity analysis on the final results [e.g., @sinharay2002sensitivity, @vanpaemel2010prior, @lee2018determining]. In a sensitivity analysis, a set of plausible priors are determined in addition to the prior chosen in the main analysis for which the Bayes factors are calculated. The range of Bayes factors then gives an indication of the extent to which the results are fragile or robust to different modeling choices. In general, the prior on which the final analysis is performed as well as the set of priors used to conduct the sensitivity analysis should be determined and preregistered before seeing the data to ensure a fair comparison of the hypotheses of interest. 

# Usage and Examples

In the following, we will outline three examples on how to use \textbf{multibridge} to compare an informed hypothesis to a null or encompassing hypothesis. The first example concerns multinomial data and the second and third example concerns independent binomial data. Additional examples are available as vignettes (see \texttt{vignette(package\ =\ "multibridge")). 


```{r, echo = FALSE}
library('multibridge')
```

The two core functions of \textbf{multibridge}---\texttt{mult\_bf\_informed} and the \texttt{binom\_bf\_informed}---can be illustrated schematically as follows:

```{r, eval=FALSE, echo=TRUE}
mult_bf_informed(x, Hr, a, factor_levels)
binom_bf_informed(x, n, Hr, a, b, factor_levels)
```

## Example 1: Applying A Benford Test to Greek Fiscal Data

The first-digit phenomenon, otherwise known as Benford's law [@benford1938law; @newcomb1881note] states that the expected proportion of leading digits in empirical data can be formalized as follows: for any given leading digit \(d, d = (1, \cdots, 9)\) the expected proportion is approximately equal to \[\mathbb{E}_{\theta_d}= \text{log}_{10}((d + 1)/d).\] This means that in an empirical data set, numbers with smaller leading digits are more common than numbers with larger leading digits. Specifically, a number has leading digit \(1\) in \(30.1 \%\) of the cases, and leading digit \(2\) in \(17.61 \%\) of the cases; leading digit \(9\) is the least frequent digit with an expected proportion of only \(4.58 \%\) (see Table \ref{Tab:benford} for an overview of the expected proportions). Empirical data for which this relationship holds include population sizes, death rates, baseball statistics, atomic weights of elements, and physical constants [@benford1938law]. In contrast, artificially generated data, such as telephone numbers, do in general not obey Benford's law [@hill1995statistical]. Given that Benford's law applies to empirical data but not artificially generated data, a so-called Benford test can be used in fields like accounting and auditing to check for indications for poor data quality [for an overview, see e.g., @durtschi2004effective; @nigrini1997use; @nigrini2012benford]. Data that do not pass the Benford test, should raise audit risk concerns, meaning that it is recommended that they undergo additional follow-up checks [@nigrini2019patterns].

Below we discuss four possible Bayesian adaptations of the Benford test. In a first scenario we simply conduct a Bayesian multinomial test in which we test the point-null hypothesis \(\mathcal{H}_0\) which predicts a Benford distribution. In a second scenario we test the informed hypothesis \(\mathcal{H}_{r1}\), which predicts a decreasing trend in the proportions of leading digits. The hypothesis \(\mathcal{H}_{r1}\) exerts considerably more constraint than \(\mathcal{H}_{e}\) and provides a more sensitive test if our primary goal is to test whether data comply with Benford's law or whether the data follow a similar but different trend. In the next two scenarios, our main goal is to identify fabricated data. The third scenario therefore tests the null hypothesis against the hypothesis that all proportions occur equally often. This hypothesis \(\mathcal{H}_{r2}\) could be considered if it is suspected that the data were generated randomly or could serve as a bookend comparison hypothesis as it maximally constraints the parameter space. In a fourth scenario we test a hypothesis which predicts a trend that is characteristic for manipulated data. This hypothesis, which we denote as \(\mathcal{H}_{r3}\), could be derived from empirical research on fraud or be based on observed patterns from former fraud cases. For instance, @hill1995statistical instructed students to produce a series of random numbers; in the resulting data the proportion of the leading digit \(1\) occurred most often and the digits \(8\) and \(9\) occurred least often which is consistent with the general pattern of Benford's law. However, the proportion for the remaining leading digits were approximately equal. Note that the predicted distribution derived from @hill1995statistical is not currently used as a test to detect fraud, however, for the sake of simplicity, we assume that this pattern could be an indication of manipulated auditing data. All hypotheses will be tested against the encompassing hypothesis \(\mathcal{H}_{e}\), which too serves as a bookend comparison hypothesis, and which imposes no constraints on the proportion of leading digits.

### Data and Hypothesis

The data we use to illustrate the computation of Bayes factors were originally published by the European statistics agency \enquote{Eurostat} and served as basis for reviewing the adherence to the Stability and Growth Pact of EU member states. @rauch2011fact conducted a Benford test on data related to budget deficit criteria, that is, public deficit, public dept and gross national products. The data used for this example features the proportion of first digits from Greek fiscal data in the years between \(1999\) and \(2010\); a total of \(N= 1{,}497\) numerical data were included in the analysis. We choose this data, since the Greek government deficit and debt statistics states has been repeatedly criticized by the European Commission in this time span [@europeanCommision2004; @europeanCommision2010]. In particular, the commission has accused the Greek statistical authorities to have misreported deficit and debt statistics. For further details on the data set see @rauch2011fact. The observed and expected proportions are displayed in Table \ref{Tab:benford}; the expected proportions versus the posterior parameter estimates under the encompassing hypothesis are displayed in Figure \ref{fig:benford-alt}.

\begin{table}[H]
	\centering
	\caption{Observed counts, observed proportions, and expected proportions of first digits in the Greek fiscal data set. The total sample size was $N = 1{,}497$ observations. Note that the observed proportions and counts deviate slightly from those reported in Rauch et al. (2011) (probably due to rounding errors).}
	\begin{tabular}{cccp{4cm}}
		\hline
Leading digit & Observed Counts & Observed Proportions & Expected Proportions: Benford's Law  \\
		\hline
		1 & 509 & 0.340 & 0.301  \\
		2 & 353 & 0.236 & 0.176  \\
		3 & 177 & 0.118 & 0.125  \\
		4 & 114 & 0.076 & 0.097  \\
		5 & 77 & 0.051 & 0.079  \\
		6 & 77 & 0.051 & 0.067  \\
		7 & 53 & 0.035 & 0.058  \\
		8 & 73 & 0.049 & 0.051  \\
		9 & 64 & 0.043 & 0.046  \\
		\hline
	\end{tabular}
    \label{Tab:benford}
\end{table}

In this example, the parameter vector of the multinomial model, \(\theta_1, \cdots, \theta_K\), reflects the probabilities of a leading digit in the Greek fiscal data being a number from \(1\) to \(9\). Each of the hypotheses above will be tested against the encompassing hypothesis $\mathcal{H}_e$ which imposes no constraints on the parameters. The hypotheses introduced above can then be formalized as follows:
\begin{align*}
\mathcal{H}_e &: \boldsymbol{\theta} \sim \text{Dirichlet}(\mathbf{1}) \\
\mathcal{H}_0 &: \boldsymbol{\theta}_0 = (0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046), \\
\mathcal{H}_{r1} &: \theta_1 > \theta_2 > \theta_3 > \theta_4 > \theta_5 > \theta_6 > \theta_7 > \theta_8 > \theta_9 \\
\mathcal{H}_{r2} &:  \boldsymbol{\theta}_0 = \left(\frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}, \frac{1}{9}\right)\\
\mathcal{H}_{r3} &:  \theta_1 > (\theta_2 = \theta_3 = \theta_4 = \theta_5 = \theta_6 = \theta_7) > (\theta_8, \ \theta_9).
\end{align*}

### Method

Both \(\text{BF}_{0e}\) and \(\text{BF}_{r2e}\) may be readily computed by means of a Bayesian multinomial test which is implemented in the function \texttt{mult\_bf\_equality}. This function requires (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution under $\mathcal{H}_e$, and (3) the vector of expected proportions under $\mathcal{H}_0$ and under $\mathcal{H}_{r2}$. In this example, we do not incorporate specific expectations about the distribution of leading digits in the Greek fiscal data and therefore assign a uniform Dirichlet distribution to the proportion of leading digits. That is, we set all concentration parameters under $\mathcal{H}_e$ to 1 (i.e., we assign $\boldsymbol{\theta}$ a uniform Dirichlet prior distribution). This prior supports all possible points equally, meaning that, if the data were completely random, none of the hypotheses under consideration should be favored over the other.

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Observed counts
x <- c(509, 353, 177, 114,  77,  77,  53,  73,  64)
# Prior specification for Dirichlet prior distribution under H_e
a <-  c(1, 1, 1, 1, 1, 1, 1, 1, 1)
# Expected proportions for H_0 and H_r2
p0  <- log10((1:9 + 1)/1:9)
pr2 <- c(1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9)
# Execute the analysis
results_H0_He   <- mult_bf_equality(x = x, a = a, p = p0)
results_Hr2_He  <- mult_bf_equality(x = x, a = a, p = pr2)
logBFe0  <- results_H0_He$bf$LogBFe0
logBFer2 <- results_Hr2_He$bf$LogBFe0
```

The hypotheses \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r3}\) contain inequality constraints, and this necessitates the use of the function \texttt{mult\_bf\_informed} to compute the Bayes factors \(\text{BF}_{r1e}\) and \(\text{BF}_{r3e}\). This function requires (1) a vector with observed counts, (2) a vector with concentration parameters of the Dirichlet prior distribution under $\mathcal{H}_e$, (3) labels for the categories of interest (i.e., leading digits), and (4) the informed hypothesis \(\mathcal{H}_{r1}\) or \(\mathcal{H}_{r3}\) (e.g., as a string). 
In addition to the basic required arguments, we use two additional arguments here. The first argument sets the Bayes factor type, that is, whether the output should print the Bayes factor in favor of the informed hypothesis (i.e., \(\text{BF}_{re}\)) or in favor of the encompassing hypothesis (i.e., \(\text{BF}_{er}\)). It is also possible to compute the log Bayes factor in favor of the hypothesis, which is the setting we choose for this example. The purpose of the second argument \texttt{seed} is to make the results reproducible:

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Observed counts
x <- c(509, 353, 177, 114,  77,  77,  53,  73,  64)
# Prior specification for Dirichlet prior distribution under H_e
a <-  c(1, 1, 1, 1, 1, 1, 1, 1, 1)
# Labels for categories of interest
factor_levels <- 1:9
# Specifying the informed hypotheses as a string
Hr1 <- c('1 > 2 > 3 > 4 > 5 > 6 > 7 > 8 > 9')
Hr3 <- c('1 > 2 = 3 = 4 = 5 = 6 = 7 > 8 , 9')
# Execute the analysis
results_He_Hr1 <- mult_bf_informed(x = x, Hr = Hr1, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
results_He_Hr3 <- mult_bf_informed(x = x, Hr = Hr3, a = a, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
logBFer1 <- summary(results_He_Hr1)$bf
logBFer3 <- summary(results_He_Hr3)$bf
```

We also compute the posterior model probabilities for all hypotheses. The results are shown in Table \ref{Tab:benfordResults}.

```{r, echo = FALSE}
bayes_factors <- data.frame(
   BFType = c('LogBF0e', 'LogBFr1e', 'LogBFr2e', 'LogBFr3e'), 
   LogBF  = c(-logBFe0, -logBFer1, -logBFer2, -logBFer3))

denominator <- c(1, exp(bayes_factors$LogBF))
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)', 'p(Hr2 | x)', 'p(Hr3 | x)'), 
   Prob = denominator/sum(denominator))
```

\begin{table}[H]
	\centering
	\caption{Prior model probabilities, posterior model probabilities, and Bayes factors for five rival accounts of first digit frequencies in the Greek fiscal data set.}
	\begin{tabular}{ccll}
		\hline Hypothesis &  $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{log}(\text{BF}_{.e})$ \\
		\hline
		$\mathcal{H}_{0}$  & $0.2$  &
		`r papaja::printnum(post_probs[2, 2], digits = 2, format = 'e')` & 
		`r papaja::printnum(bayes_factors[1, 2], digits = 2)` \\
		$\mathcal{H}_{r1}$ & $0.2$ &
		`r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')` & 
		`r papaja::printnum(bayes_factors[2, 2], digits = 2)`\\
		$\mathcal{H}_{e}$  & $0.2$ &
		`r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` & 
		$0$\\
		$\mathcal{H}_{r3}$ & $0.2$ &
		`r papaja::printnum(post_probs[5, 2], digits = 2, format = 'e')` &
		`r papaja::printnum(bayes_factors[4, 2], digits = 2)`\\
		$\mathcal{H}_{r2}$ & $0.2$ &
		`r papaja::printnum(post_probs[4, 2], digits = 2, format = 'e')` & 
		`r papaja::printnum(bayes_factors[3, 2], digits = 2)`\\
		\hline
	\end{tabular}
    \label{Tab:benfordResults}
\end{table}

The results indicate strong support for $\mathcal{H}_{r1}$ --the model in which the proportions are assumed to decrease monotonically-- over all other models. The log Bayes factor of $\mathcal{H}_{r1}$ against the encompassing hypothesis $\mathcal{H}_e$ is `r papaja::printnum(bayes_factors[2, 2], digits = 2, format = 'f')`, which equates to a Bayes factor of `r papaja::printnum(exp(bayes_factors[2, 2]), digits = 0, format = 'f')` on a natural scale. 

The strong Bayes factor support for $\mathcal{H}_{r1}$ translates to a relatively extreme posterior model probability of `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')`. By comparison, the posterior model probabilities for hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{r3}$, that is, the bookend null-hypothesis and the hypothesis predicting a data pattern typical of fraud, are only slightly greater than zero. The posterior model probability for $\mathcal{H}_{e}$ is `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')`. Thus, hypothesis $\mathcal{H}_{r1}$ can outperform the two bookend hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{e}$. That $\mathcal{H}_{r1}$ outperforms the unconstrained model $\mathcal{H}_{e}$ demonstrates how a parsimonious model that makes precise predictions can be favored over a model that is more complex [e.g., @jefferysberger1992]. 

(ref:benford-alt-caption) Predictions from Benford's law (in pink) show together with the posterior medians (black circles) for the category proportions estimated under the encompassing model $\mathcal{H}_e$. The circle skewers show the 95\% credible intervals. Only three of nine intervals encompass the expected proportions, suggesting that the data do not follow Benford's law. This plot was created using the \texttt{plot}-S3-method for \texttt{summary.bmult} objects in \textbf{multibridge}.

```{r benford-alt, echo = FALSE, message = FALSE, fig.cap = "(ref:benford-alt-caption)"}
first_digits <- 1:9
benford <- log10((first_digits + 1) / first_digits)

plot(
  summary(results_He_Hr1)
  , xlab = "Leading digit"
  # , ylab = "Proportion"
  , main = ""
  , panel.first = {
    lines(x = first_digits, y = benford, lty = "22", col = '#FB9A99')
    points(x = first_digits, y = benford, pch = 16, col = "white", cex = 2)
    points(x = first_digits, y = benford, pch = 16, bg = "white", col = '#FB9A99', cex = 0.8)
  }
)

points(x = first_digits, y = benford, pch = 16, bg = "white", col = '#FB9A99', cex = 0.8)

legend(
  "right"
  , legend = c("Benford", "Posterior")
  , col = c('#FB9A99', "black")
  , pch = c(16, 21)
  , pt.bg = c(NULL, "white")
  , lty = c("22", "solid")
  , lwd = c(1.25, 1)
  , bty = "n"
  , pt.cex = c(0.8, 1.5)
  , title = "Distribution"
  , seg.len = 1.5
)
```

### Sensitivity Analysis

In a sensitivity analysis we will determine whether our results are robust against different prior choices. In the main analysis we chose a uniform Dirichlet distribution on the category proportions as prior under $\mathcal{H}_{e}$. This prior assigns equal probability to all possible parameter values, but alternative prior distributions are seem also conceivable. Audit researchers may argue for the development of more informative and theory-driven priors that resemble one of the hypotheses under consideration. The Dirichlet parameters vectors specified below resemble the four hypotheses, assuming $N = 54$ prior observations.

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Alternative prior specifications
a0 <-  c(16, 10, 7, 5, 4, 3, 3, 3, 2) # Benford's law
a1 <-  c(10, 9, 8, 7, 6, 5, 4, 3, 2)  # Monotonically decreasing trend
a2 <-  c(6, 6, 6, 6, 6, 6, 6, 6, 6)   # Equal proportions
a3 <-  c(12, 6, 6, 6, 6, 6, 6, 3, 3)  # Fraud pattern
```

The sensitivity analysis is then carried out for each prior choice and will be compared to the main results. For this analysis, we are particularly interested in the Bayes factors of the hypothesis postulating a decreasing trend $\mathcal{H}_{r1}$ and Benford's law $\mathcal{H}_{0}$ to the encompassing hypothesis $\mathcal{H}_{e}$.

```{r, message=FALSE, echo = TRUE, results='hide', warning = FALSE}
# Sensitivity analysis for log(BFe_r1)
sensitivity0 <- mult_bf_informed(x = x, Hr = Hr1, a = a0, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity1 <- mult_bf_informed(x = x, Hr = Hr1, a = a1, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity2 <- mult_bf_informed(x = x, Hr = Hr1, a = a2, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)
sensitivity3 <- mult_bf_informed(x = x, Hr = Hr1, a = a3, 
                                 factor_levels = factor_levels, 
                                 bf_type = 'LogBFer', seed = 2020)

# Sensitivity analysis for log(BFe_0)
sensitivity4   <- mult_bf_equality(x = x, a = a0, p = p0)
sensitivity5   <- mult_bf_equality(x = x, a = a1, p = p0)
sensitivity6   <- mult_bf_equality(x = x, a = a2, p = p0)
sensitivity7   <- mult_bf_equality(x = x, a = a3, p = p0)
```

```{r, echo = FALSE}
m0 <- summary(sensitivity0)
m1 <- summary(sensitivity1)
m2 <- summary(sensitivity2)
m3 <- summary(sensitivity3)

m4 <- sensitivity4$bf$LogBFe0
m5 <- sensitivity5$bf$LogBFe0
m6 <- sensitivity6$bf$LogBFe0
m7 <- sensitivity7$bf$LogBFe0

bayes_factors1 <- data.frame(
   BFType = c('LogBFr1e_e', 'LogBFr1e_0', 'LogBFr1e_1', 'LogBFr1e_2', 'LogBFr1e_3'), 
   LogBF  = c(-logBFer1, -m0$bf, -m1$bf, -m2$bf, -m3$bf))

bayes_factors2 <- data.frame(
   BFType = c('LogBF0e_e', 'LogBF0e_0', 'LogBF0e_1', 'LogBF0e_2', 'LogBF0e_3'), 
   LogBF  = c(-logBFe0, -m4, -m5, -m6, -m7))
```

The results of the sensitivity analysis are displayed in Table \ref{Tab:benfordSensitivity}. The general direction of the sensitivity analysis agrees with our conclusions drawn from the main analysis. That is, for the Bayes factors of $\mathcal{H}_{r1}$ compared to $\mathcal{H}_{e}$, the evidence points towards the informed hypothesis. However, the prior exerts an influence on $\text{BF}_{r1e}$; the evidence in favor for the informed hypothesis ranges from weak to extreme evidence. Specifically, when we choose priors that resemble a decreasing trend for the frequency of leading digits, as we did with $\boldsymbol{\alpha_0}$ and $\boldsymbol{\alpha_1}$, the Bayes factor becomes smaller and the evidence weak (i.e., $( \text{BF}_{r1e} \mid \boldsymbol{\alpha_0} )$ = `r papaja::printnum(exp(bayes_factors1[2, 2]), digits = 2, format = 'f')` on the natural scale) and moderate (i.e., $( \text{BF}_{r1e} \mid \boldsymbol{\alpha_1})$ =   `r papaja::printnum(exp(bayes_factors1[3, 2]), digits = 2, format = 'f')` on the natural scale). When the prior contrasts the data, the evidence becomes very strong or extreme. Thus, a prior that closely resembles the predictive trend reduces to some degree the diagnostic value of the data.


By contrast, the Bayes factors for $\mathcal{H}_{0}$ compared to $\mathcal{H}_{e}$ are robust against different prior settings. Here too, the prior changes the Bayes factor estimate but in all cases the data suggests overwhelming evidence in favor of the encompassing hypothesis over Benford's law.

\begin{table}[H]
	\centering
	\caption{Results of a sensitivity analysis for the Greek fiscal data set.}
	\begin{tabular}{llcll}
		\hline Description & Prior & $\text{log}(\text{BF}_{r1e})$ & $\text{log}(\text{BF}_{0e})$ \\
		\hline
		Uniform &
		$\boldsymbol{\alpha_e} = (1, 1, 1, 1, 1, 1, 1, 1, 1)$  & 
		`r papaja::printnum(bayes_factors1[1, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[1, 2], digits = 2)` \\
		Benford's law &
		$\boldsymbol{\alpha_0} = (16, 10, 7, 5, 4, 3, 3, 3, 2)$ & 
		`r papaja::printnum(bayes_factors1[2, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[2, 2], digits = 2)` \\
		Montonically decreasing &
		$\boldsymbol{\alpha_1} = (10, 9, 8, 7, 6, 5, 4, 3, 2)$  & 
		`r papaja::printnum(bayes_factors1[3, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[3, 2], digits = 2)` \\
		Centered on mean &
		$\boldsymbol{\alpha_2} = (6, 6, 6, 6, 6, 6, 6, 6, 6)$ & 
		`r papaja::printnum(bayes_factors1[4, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[4, 2], digits = 2)` \\
		Fraud pattern &
		$\boldsymbol{\alpha_3} = (12, 6, 6, 6, 6, 6, 6, 3, 3)$ & 
		`r papaja::printnum(bayes_factors1[5, 2], digits = 2)` & 
		`r papaja::printnum(bayes_factors2[5, 2], digits = 2)` \\
		\hline
	\end{tabular}
    \label{Tab:benfordSensitivity}
\end{table}

To summarize, the data offer overwhelming support for hypothesis \(\mathcal{H}_{r1}\), which postulates a decreasing trend in the digit proportions. This model outperformed both simpler models (e.g., the Benford model and the bookend null-hypothesis) and a more complex model in which the proportions were free to vary. The results are sensitive to our prior choices as a sensitivity analysis showed: for moderately informative priors which resemble the predicted decreasing trend, the \(\mathcal{H}_{r1}\) cannot outperform the encompassing model. On the other hand, the conclusion that Benford's law does not offer a good description of the data was robust to different prior settings. Detailed follow-up analyses are needed to discover why the Greek fiscal data fail to adhere to Benford's law [@nigrini2019patterns].

## Example 2: Prevalence of Statistical Reporting Errors

This section illustrates how \textbf{multibridge} may be used to evaluate models for independent binomial data rather than multinomial data. Our example concerns the prevalence of statistical reporting errors across eight different psychology journals. In any article that uses null hypothesis significance testing, there is a chance that the reported test statistic and degrees of freedom do not match the reported \(p\)-value, possibly because of copy-paste errors. To flag these errors, @epskamp2014statcheck developed the \texttt{R} package \texttt{statcheck}, which scans the PDF of a given scientific article and automatically detects statistical inconsistencies. This package allowed @nuijten2016prevalence to estimate the prevalence of statistical reporting errors in the field of psychology. In total, the authors investigated a sample of \(30{,}717\) articles (which translates to over a quarter of a million \(p\)-values) published in eight major psychology journals between 1985 to 2013: \emph{Developmental Psychology} (DP), the \emph{Frontiers in Psychology} (FP), the \emph{Journal of Applied Psychology} (JAP), the \emph{Journal of Consulting and Clinical Psychology} (JCCP), \emph{Journal of Experimental Psychology: General} (JEPG), the \emph{Journal of Personality and Social Psychology} (JPSP), the \emph{Public Library of Science} (PLoS), \emph{Psychological Science} (PS).

Based on several background assumptions, @nuijten2016prevalence predicted that the proportion of statistical reporting errors is higher for articles published in the \emph{Journal of Personality and Social Psychology} (JPSP) than for articles published in the seven other journals.

### Data and Hypothesis

Here we reuse the original data published by @nuijten2016prevalence, which we also distribute with the package \textbf{multibridge} under the name \texttt{journals}.

```{r, echo = TRUE}
data(journals)
```

The @nuijten2016prevalence hypothesis of interest, \(\mathcal{H}_r\), states that the prevalence for statistical reporting errors is higher for JPSP than for the other journals.^[@nuijten2016prevalence did not report inferential tests because they had sampled the entire population. We do report inferential tests here because we wish to learn about the latent data-generating process.] We will consider two specific versions of the @nuijten2016prevalence \(\mathcal{H}_r\) hypothesis. The first hypothesis, \(\mathcal{H}_{r1}\), stipulates that JPSP has the highest prevalence of reporting inconsistencies, whereas the other seven journals share a prevalence that is lower. The second hypothesis, \(\mathcal{H}_{r2}\), also stipulates that JPSP has the highest prevalence of reporting inconsistencies, but does not commit to any particular structure on the prevalence for the other seven journals.

The \textbf{multibridge} package can be used to test \(\mathcal{H}_{r1}\) and \(\mathcal{H}_{r2}\) against the null hypothesis \(\mathcal{H}_0\) that all eight journals have the same prevalence of statistical reporting errors. In addition, we will compare \(\mathcal{H}_{r1}\), \(\mathcal{H}_{r2}\), and \(\mathcal{H}_0\) against the encompassing hypothesis \(\mathcal{H}_e\) that makes no commitment about the prevalence of reporting inconsistencies across the eight journals. In this example, the parameter vector of the binomial success probabilities, \(\boldsymbol{\theta}\), reflects the probabilities that articles contain at least one statistical reporting inconsistency across journals. Thus, the above hypotheses can be formalized as follows:

\begin{align*}
   \mathcal{H}_e &:  \theta_{\text{JAP}} \cdots \theta_{\text{JPSP}} \sim \prod_{k = 1}^K \text{Beta}(\alpha_k, \beta_k) \\
    \mathcal{H}_0 &:  \theta_{\text{JAP}} = \theta_{\text{PS}} = \theta_{\text{JCCP}} = \theta_{\text{PLOS}} = \theta_{\text{DP}} = \theta_{\text{FP}}= \theta_{\text{JEPG}} = \theta_{\text{JPSP}}\\
    \mathcal{H}_{r1} &:  (\theta_{\text{JAP}} = \theta_{\text{PS}} = \theta_{\text{JCCP}} = \theta_{\text{PLOS}} = \theta_{\text{DP}} = \theta_{\text{FP}}= \theta_{\text{JEPG}}) < \theta_{\text{JPSP}} \\
    \mathcal{H}_{r2} &: (\theta_{\text{JAP}} , \theta_{\text{PS}} , \theta_{\text{JCCP}} , \theta_{\text{PLOS}} , \theta_{\text{DP}} , \theta_{\text{FP}} , \theta_{\text{JEPG}}) < \theta_{\text{JPSP}}.
\end{align*}

### Method

To compute the Bayes factor \(\text{BF}_{0r}\) we need to specify (1) a vector with observed successes (i.e., the number of articles that contain a statistical inconsistency), (2) a vector containing the total number of observations (i.e., the number of articles), (3) a vector with prior parameter \(\alpha_k\) for each binomial proportion of the beta prior distribution under \(\mathcal{H}_e\), (4) a vector with prior parameter \(\beta_k\) for each binomial proportion of the beta prior distribution under \(\mathcal{H}_e\), (5) the category labels (i.e., journal names), and (6) the informed hypothesis \(\mathcal{H}_{r1}\) or \(\mathcal{H}_{r2}\) (e.g., as a string). We also change the Bayes factor type to \texttt{LogBFr0} so that the function returns the log Bayes factor in favor for the informed hypothesis compared to the null hypothesis. Since we have no specific expectations about the distribution of statistical reporting errors in any given journal, we set all parameters \(\alpha_k\) and \(\beta_k\) to one which corresponds to uniform beta distributions. With this information, we can now conduct the analysis with the function \texttt{binom\_bf\_informed}.

```{r, echo=TRUE, message=FALSE, results='hide'}
# Since percentages are rounded to two decimal values, we round the
# articles with an error to obtain integer values
x <- round(journals$articles_with_NHST  * 
             (journals$perc_articles_with_errors/100))
# Total number of articles
n <- journals$articles_with_NHST
# Prior specification for beta prior distributions under H_e
a <- c(1, 1, 1, 1, 1, 1, 1, 1)
b <- c(1, 1, 1, 1, 1, 1, 1, 1)
# Labels for categories of interest
journal_names <- journals$journal

# Specifying the informed Hypothesis
Hr1 <- c('JAP = PS = JCCP = PLOS = DP = FP = JEPG < JPSP')
Hr2 <- c('JAP , PS , JCCP , PLOS , DP , FP , JEPG < JPSP')

# Execute the analysis for Hr1
results_H0_Hr1 <- binom_bf_informed(x = x, n = n, Hr = Hr1, a = a, b = b,
                                factor_levels = journal_names,
                                bf_type = 'LogBFr0', seed = 2020)
# Execute the analysis for Hr2
results_H0_Hr2 <- binom_bf_informed(x = x, n = n, Hr = Hr2, a = a, b = b,
                                factor_levels = journal_names,
                                bf_type = 'LogBFr0', seed = 2020)
```

```{r, echo = TRUE}
LogBFe0  <- results_H0_Hr1$bf_list$bf0_table[['LogBFe0']]
LogBFr10 <- summary(results_H0_Hr1)$bf
LogBFr20 <- summary(results_H0_Hr2)$bf
```

```{r compute_post_probs, echo = FALSE}
LogBFr1e <- -results_H0_Hr1$bf_list$bfr_table[['LogBFer']]
LogBFr2e <- -results_H0_Hr2$bf_list$bfr_table[['LogBFer']]

bayes_factors <- data.frame(
   BFType = c('LogBFe0', 'LogBFr10', 'LogBFr20'), 
   BF = c(LogBFe0, LogBFr10, LogBFr20))

denominator <- sum(1, exp(-LogBFe0), exp(LogBFr1e), exp(LogBFr2e))
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)' , 'p(Hr2 | x)'), 
   Prob = c(1, exp(-LogBFe0), exp(LogBFr1e), exp(LogBFr2e))/denominator)
```


\begin{table}[H]
    \centering
    \caption{Prior model probabilities, posterior model probabilities, and Bayes factors for four hypotheses concerning the prevalence of statistical reporting errors across psychology journals.}
    \begin{tabular}{ccll}
        \hline Hypothesis & $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{log}(\text{BF}_{.0})$ \\
        \hline
        $\mathcal{H}_{0}$  & $0.25$ & 
        `r papaja::printnum(post_probs[2, 2], digits = 4, format = 'e')` & 
        $0$ \\
        $\mathcal{H}_{r2}$ & $0.25$ & 
        `r papaja::printnum(post_probs[4, 2], digits = 4, format = 'f')` &  
        `r papaja::printnum(bayes_factors[3, 2], digits = 2, format = 'f')`\\
        $\mathcal{H}_{e}$  &  $0.25$  & 
        `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` & 
        `r papaja::printnum(bayes_factors[1, 2], digits = 2, format = 'f')` \\
        $\mathcal{H}_{r1}$  &  $0.25$  & 
        `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'e')` & 
        `r papaja::printnum(bayes_factors[2, 2], digits = 2, format = 'f')` \\
        \hline
    \end{tabular}
    \label{Tab:journalsResults}
\end{table}

As the evidence is extreme in all four cases, we again report all Bayes factors on the log scale. The Bayes factor \(\text{log}(\text{BF}_{r20})\) indicates overwhelming evidence for the informed hypothesis that JPSP has the highest prevalence for statistical reporting inconsistencies compared to the null hypothesis that the statistical reporting errors are equal across all eight journals; \( \text{log}(\text{BF}_{r20}) = \) `r papaja::printnum(bayes_factors[3, 2], digits = 2, format = 'f')`.

For a clearer picture about the ordering of the journals we can investigate the posterior distributions for the prevalence rates obtained under the encompassing model. 

(ref:journals-caption) Posterior medians for the prevalence of statistical reporting inconsistencies across eight psychology journals, as obtained using the encompassing model. The circle skewers show the 95\% credible intervals. Analysis based on data from @nuijten2016prevalence. This plot was created using the \texttt{plot}-S3-method for \texttt{summary.bmult} objects.

```{r journals, echo = TRUE, message = FALSE, fig.cap = "(ref:journals-caption)"}
plot(summary(results_H0_Hr2), xlab = "Journal")
```

The posterior medians and 95\% credible intervals are returned by the \texttt{summary}-method and are shown in Figure \ref{fig:journals}. The figure strongly suggests that the prevalence of reporting inconsistencies is not equal across all eight journals. This impression may be quantified by comparing the null hypothesis \(\mathcal{H}_0\) to the encompassing hypothesis \(\mathcal{H}_e\). The corresponding Bayes factor equals \( \text{log}(\text{BF}_{e0}) = \) `r papaja::printnum(bayes_factors[1, 2], digits = 2, format = 'f')`, which confirms that the data dramatically undercut the null hypothesis that the prevalence of statistical reporting inconsistencies is equal across journals.

The data offer most support for the Nuijten hypothesis \(\mathcal{H}_{r2}\), which posits that JPSP has the highest prevalence but does not commit to any restriction on the prevalences for the remaining seven journals. This hypothesis may be compared to the encompassing hypothesis \(\mathcal{H}_e\), which yields \( \text{log}(\text{BF}_{r2e}) = \) `r papaja::printnum(LogBFr2e, digits=2, format = 'f')`. This means that the observed data are \(\exp(2.01) \approx 7.45\) times more likely under \(\mathcal{H}_{r2}\) than under \(\mathcal{H}_e\); this is moderate evidence for the restriction suggested by @nuijten2016prevalence. Under equal prior probability for the models, this Bayes factor translates to a posterior probability on \(\mathcal{H}_e\) of `r papaja::printnum(post_probs[1, 2], digits = 3, format = 'f')`, an amount that researchers may deem too large to discard in an all-or-none fashion.

To summarize, the data provide moderate evidence for the hypothesis stated by Nuijten et al. (2016) that the prevalence of statistical reporting inconsistencies in JPSP is higher than that in seven other psychology journals.

## Example 3: Effects of Gender and Education on the Violation of Stochastic Dominance

This section illustrates the comparison of four nested hypotheses concerning independent binomial probabilities. In his study, @birnbaum1999testing presented new possibilities of online testing for psychological science (in the late 1990s online testing was still novel and rarely used). To compare data collected from an online research to traditional lab research, @birnbaum1999testing collected experimental data from 1224 participants online and 124 participants in the lab. In his experiment participants played 20 rounds of a gambling game. In each round, they were presented with two gambles with different probabilities and monetary values and were asked to indicate which gamble they would rather play. The gamble chosen by the participants was then played once. @birnbaum1999testing examined the characteristics of the two samples, for instance, in terms of their risk aversion and their consistency with decision making axioms, such as stochastic violations, and correlated them with different demographics.

The author analyzed the proportion of stochastic violations for different demographic variables, noting a seemingly ordinal pattern for the probabilities to violate of stochastic dominance for the factors gender (m=male, f=female) and education (1=doctorate degree, 2=postgraduate degree, 3=bachelor's degree, 4=less than bachelor's degree). In a later study, @myung2005bayesian presented a Bayesian inference framework to test decision making axioms (using the ``Bayesian $p$-value'') and used Birnbaum's data as an example on how to assess violations of stochastic dominance and their relationship with covariates. Concretely, @myung2005bayesian reanalyzed the data from @birnbaum1999testing and tested the informed hypothesis that stochastic dominance is violated more frequently in women compared to men and more frequently in lower education levels than higher education levels.

### Data and Hypothesis

We will use data from @birnbaum1999testing as presented in @myung2005bayesian. The data show the stochastic violations of the online sample for one of the gambling rounds featuring 1212 valid responses (see Table 8).
<!-- (see Table \ref{Tab:decisionData}). -->

```{r, echo = TRUE}
dat <- data.frame(gender = rep(c('male', 'female'), each = 4),
                  education = rep(c('1', '2', '3', '4'), 2),
                  levels = paste0(rep(c('m', 'f'), each = 4), 1:4),
                  violation = c(0.487, 0.477, 0.523, 0.601,
                                0.407, 0.555, 0.650, 0.622),
                  n = c(80, 88, 195, 163,
                        54, 108, 206, 318),
                  x = c(39, 42, 102, 98, 
                        22, 60, 134, 198))

```


```{r, echo = FALSE}
descriptives_tab <- data.frame(
  education =  rep(c('Doctorate Degree', 'Postgraduate Degree', 'Bachelor\'s Degree', 'Less than Bachelor\'s degree'), 2),
  counts = paste0(dat$x, '/' , dat$n),
  props = dat$violation
)

colnames(descriptives_tab) <- c('Education', 'Observed Counts', 'Observed Proportions')
papaja::apa_table(
  descriptives_tab,
  label = 'Tab:decisionData',
  stub_indents = list(`Male` = 1:4, `Female` = 5:8),
  caption = "Observed counts and observed proportions of stochastic dominance violations for the N = 1,212 participants in Birnbaum (1999). The data are split by gender and education level of the participants.",
  escape = TRUE
)
```

The parameter vector of the binomial success probabilities, $\theta_1, \cdots, \theta_K$, contains the probabilities of observing a value in a particular category; here, it reflects the probabilities of violating stochastic dominance for a particular subgroup (e.g., women with a doctorate). We will compare three inequality-constrained hypotheses $\mathcal{H}_{r1}$, $\mathcal{H}_{r2}$, $\mathcal{H}_{r3}$ formulated by @myung2005bayesian. The first hypothesis $\mathcal{H}_{r1}$ encodes the main effect for gender and states that the probability to violate stochastic dominance is lower for men than for women. The second hypothesis $\mathcal{H}_{r2}$ encodes the main effect of education and states that the probability to violate stochastic dominance is lower for persons with higher education levels. The third hypothesis $\mathcal{H}_{r3}$ combines hypotheses $\mathcal{H}_{r1}$ and $\mathcal{H}_{r2}$. We will test this hypothesis against the encompassing hypothesis $\mathcal{H}_e$ without any constraints. In addition, we will include a bookend null-hypothesis $\mathcal{H}_{0}$ predicting that all probabilities are equal. The set of candidate hypotheses can therefore be written as follows:

\begin{align*}
\mathcal{H}_e &: (\theta_{\text{m1}}, \theta_{\text{m2}}, \theta_{\text{m3}}, \theta_{\text{m4}}, \theta_{\text{f1}}, \theta_{\text{f2}}, \theta_{\text{f3}}, \theta_{\text{f4}}) \\
\mathcal{H}_0 &:   \boldsymbol{\theta}_0 = \left(\frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}\right), \\
    \mathcal{H}_{r1} &: (\theta_{\text{m1}}, \theta_{\text{m2}}, \theta_{\text{m3}}, \theta_{\text{m4}}) < (\theta_{\text{f1}}, \theta_{\text{f2}}, \theta_{\text{f3}}, \theta_{\text{f4}})\\
    \mathcal{H}_{r2} &: (\theta_{\text{m1}}, \theta_{\text{f1}}) < (\theta_{\text{m2}}, \theta_{\text{f2}}) < (\theta_{\text{m3}}, \theta_{\text{f3}}) < (\theta_{\text{m4}}, \theta_{\text{f4}}) \\
    \mathcal{H}_{r3} &: \theta_{\text{m1}} < \theta_{\text{f1}} < \theta_{\text{m2}} < \theta_{\text{f2}} < \theta_{\text{m3}} < \theta_{\text{f3}} < \theta_{\text{m4}} < \theta_{\text{f4}}.
\end{align*}

### Method

To evaluate the inequality-constrained hypothesis, we need to specify (1) a vector with observed successes, and (2) a vector containing the total number of observations, (3) the informed hypothesis, (4) a vector with prior parameters alpha for each binomial proportion, (5) a vector with prior parameters beta for each binomial proportion, and (6) the labels of the categories of interest (i.e., gender and education level). As with the previous two example, we assign a uniform Beta prior to the binomial probabilities.

```{r, echo = TRUE}
# number of violations
x <- dat$x
# total number people in the category
n <- dat$n

# Specifying the informed hypotheses (step 3)

# null hypothesis
p0  <- c(1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8, 1/8)

# informed hypotheses
Hr1 <- c('m1, m2, m3, m4 < f1, f2, f3, f4')
Hr2 <- c('m1, f1 < m2, f2 < m3, f3 < m4, f4')
Hr3 <- c('m1 < f1 < m2 < f2 < m3 < f3 < m4 < f4')

# Prior specification (step 4 and 5)
# We assign a uniform beta distribution to each binomial propotion
a <- c(1, 1, 1, 1, 1, 1, 1, 1)
b <- c(1, 1, 1, 1, 1, 1, 1, 1)

# categories of interest (step 6)
gender_edu <- dat$levels
```

With this information, we can now conduct the analysis with the function \texttt{binom\_bf\_informed()}. Since we are interested in quantifying evidence in favor of the informed hypotheses compared to the encompassing hypothesis, we set the Bayes factor type to \texttt{BFre}. For reproducibility, we are also setting a seed.

```{r, message=FALSE, echo = TRUE}
results_H0_He <- multibridge::mult_bf_equality(x = x, a = a, p = p0)

results_Hr1_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr1, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)


results_Hr2_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr2, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)


results_Hr3_He <- multibridge::binom_bf_informed(x=x, n=n, Hr=Hr3, a=a, b=b,
                                             factor_levels=gender_edu,
                                             bf_type = 'BFre',
                                             seed = 2020)
```

```{r, message=FALSE, echo = FALSE}
BF0e <- results_H0_He$bf$BF0e
BFr1e <- summary(results_Hr1_He)$bf
BFr2e <- summary(results_Hr2_He)$bf
BFr3e <- summary(results_Hr3_He)$bf
```

The results are summarized in Table \ref{Tab:decisionResults}. We first inspect the Bayes factors for the three informed hypotheses compared to the encompassing hypothesis. For hypotheses $\mathcal{H}_{r1}$, the data suggest moderate evidence for the encompassing hypothesis compared to the informed hypothesis, with a Bayes factor of `r papaja::printnum(1/BFr1e, digits = 2, format = 'f')`. This hypothesis predicted a main effect of gender, that is, men should have a lower probability of violating stochastic dominance than women regardless of their education level. For hypotheses $\mathcal{H}_{r2}$ and $\mathcal{H}_{r3}$, the data provide strong evidence in favor of the informed hypothesis compared to the encompassing hypothesis, with Bayes factors of `r papaja::printnum(BFr2e, digits = 2, format = 'f')` and `r papaja::printnum(BFr3e, digits = 2, format = 'f')`, respectively. However, there is no predictive advantage of $\mathcal{H}_{r3}$ over $\mathcal{H}_{r2}$; the Bayes factor directly comparing these hypotheses is $\text{BF}_{r3r2} = \frac{\text{BF}_{r3e}}{\text{BF}_{r2e}} =$ `r papaja::printnum(BFr3e/BFr2e, digits = 2, format = 'f')`. The degree to which the data conforms to the predicted pattern from $\mathcal{H}_{r3}$ becomes apparent when we plot the posterior estimates.

```{r, echo = TRUE}
plot(summary(results_Hr3_He))
```

To compare all four hypotheses directly with each other, we computed the posterior model probabilities: 

```{r, echo = TRUE}
post_probs <- data.frame(
   Hyps = c('p(He | x)', 'p(H0 | x)', 'p(Hr1 | x)', 'p(Hr2 | x)', 'p(Hr3 | x)'),
   Prob = c(1, BF0e, BFr1e, BFr2e, BFr3e)/sum(c(1, BF0e, BFr1e, BFr2e, BFr3e)))
```

The model that predicts only a gender effect performs worse than the baseline model without any restrictions. Hypothesis $\mathcal{H}_{r3}$ outperforms all other models, including the bookend hypotheses, with a posterior model probability of 54\%. Here too, however, the posterior probability of hypothesis $\mathcal{H}_{r2}$ is with 43\% almost as high as $\mathcal{H}_{r3}$. To sum up, even though $\mathcal{H}_{r3}$ yield the biggest Bayes factor and the highest posterior model probability, the difference advantage to $\mathcal{H}_{r2}$ is slim. Note that @myung2005bayesian and @birnbaum1999testing concluded that hypothesis $\mathcal{H}_{r3}$ performs the best. In contrast, our analysis suggested that here is strong evidence for an effect of education, but it is inconclusive whether the effect is moderated by gender.



\begin{table}[H]
    \centering
    \caption{Prior model probabilities, posterior model probabilities, and Bayes factors for four hypotheses concerning the relationship between gender and education level on the probability of violating stochastic dominance.}
    \begin{tabular}{ccll}
        \hline Hypothesis & $p(\mathcal{H}_{.})$ & $p(\mathcal{H}_{.} \mid \mathbf{x})$ & $\text{BF}_{.e}$ \\
        \hline
        $\mathcal{H}_{e}$  & $0.25$ &
        `r papaja::printnum(post_probs[1, 2], digits = 4, format = 'f')` &
        $1$ \\
        $\mathcal{H}_{0}$ & $0.25$ &
        `r papaja::printnum(post_probs[2, 2], digits = 2, format = 'e')` &
        `r papaja::printnum(BF0e, digits = 2, format = 'e')`\\
        $\mathcal{H}_{r1}$ & $0.25$ &
        `r papaja::printnum(post_probs[3, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr1e, digits = 2, format = 'f')`\\
        $\mathcal{H}_{r2}$  &  $0.25$  &
        `r papaja::printnum(post_probs[4, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr2e, digits = 2, format = 'f')` \\
        $\mathcal{H}_{r3}$  &  $0.25$  &
        `r papaja::printnum(post_probs[5, 2], digits = 4, format = 'f')` &
        `r papaja::printnum(BFr3e, digits = 2, format = 'f')` \\
        \hline
    \end{tabular}
    \label{Tab:decisionResults}
\end{table}

# Discussion

The \texttt{R} package \textbf{multibridge} facilitates the estimation of Bayes factors for informed hypotheses in both multinomial and independent binomial models. The efficiency gains of \textbf{multibridge} are particularly pronounced when the parameter restrictions are highly informative or when the number of categories is large.

\textbf{multibridge} supports the evaluation of informed hypotheses that feature equality constraints, inequality constraints, and free parameters, as well as combinations between them. Moreover, users can choose to test the informative hypothesis against an encompassing hypothesis that lets all parameters vary freely or against the null hypothesis that states that category proportions are exactly equal.
Beyond the core functions currently implemented in \textbf{multibridge}, there are several natural extensions we aim to include in future versions of this package. For instance, to compare several models with each other we plan to implement functions that compute the posterior model probabilities. Another extension is to facilitate the specification of hierarchical binomial and multinomial models which would allow users to analyze data where responses are nested within a higher-order structure such as participants, schools, or countries. Hierarchical multinomial models can be found, for instance, in source memory research where people need to select a previously studied item from a list [e.g., @arnold2019testing]; a hierarchical binomial model was applied, for instance, in @hoogeveen2020laypeople, to evaluate laypeople's accuracy in predicting replication outcomes for social science studies.

Furthermore, to make the method accessible to a larger audience of users and students, the informed Bayesian multinomial test and the informed Bayesian test for multiple binomials will be made available in future versions of the software package JASP [@jasp]. JASP offers an intuitive graphical user interface and does not require extensive knowledge in programming. 

In addition, we plan to expand the types of hypotheses that can be evaluated in future versions of this package. Currently, \textbf{multibridge} only supports informed hypotheses which are "stick hypotheses", that is, hypotheses in which all elements within a constraint are linearly ordered. While the quantity shown in Equation \ref{Eq:klugkistIdentity} admits in principle any constraint imposed on a vector of category proportions, this requirement is necessary for the bridge sampling routine, in order to transform samples from the real line to the probability space. To be able to evaluate more general ordinal constrains including "branch-hypotheses" with bridge sampling in the future, the stick-breaking transformation needs to be further refined. Arguably, this refinement can be realized more easily for transformations of multiple binomials than for multinomials, since independent binomials live in probability space but are not constrained by the sum-to-one condition.

Finally, we aim to enable the specification of more general informed hypotheses, including hypotheses on the size ratios of the parameters (e.g., $\theta_1 < 2 \times \theta_2$) or on their odds ratios (e.g., $\frac{\theta_1}{(\theta_1 + \theta_2)} < \frac{\theta_3}{(\theta_3 + \theta_4)}$). A framework to evaluate these constraints using the unconditional encompassing approach has already been proposed [@klugkist2010bayesian]. We believe that the bridge sampling method could also be extended to test these hypotheses as in principle, as all the building blocks are already in place. Specifically, \textbf{multibridge} takes size ratios into account when it evaluates hypotheses featuring combinations of equality and inequality constraints. For these hypotheses, \textbf{multibridge} first evaluates the equality constraints separately and then evaluates the inequality constraints given the equality constraints hold. To do so, the algorithm merges equality-constrained categories but tracks their initial number to effectively sample from the constrained parameter space and to transform the parameters. For odds ratios, on the other hand, a suitable sampling method and transformation has not yet been developed. To facilitate the evaluation of these hypotheses, alternative methods to sample and transform the parameters are required.

# Declarations

## Availability of data and code
The source code of the \texttt{R} package is available at: \url{https://github.com/ASarafoglou/multibridge/}. In addition, readers can access the code for reproducing all analyses and plots via our project folder on the Open Science Framework: \url{https://osf.io/2wf5y/}.

## Funding
This research was supported by a Netherlands Organisation for Scientific Research (NWO) grant
to AS (406-17-568), a Veni grant from the NWO to MM (451-17-017), a Vici grant from the NWO to EJW (016.Vici.170.083), as well as a a European Research Council (ERC) grant to EJW (283876). This paper was written in Rmarkdown, using the \texttt{R} package \textbf{papaja} [@papaja].

## Author contributions
The authors made the following contributions. Alexandra Sarafoglou: Conceptualization, Data Curation, Formal Analysis, Funding Acquisition, Methodology, Project Administration, Software, Validation, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; Frederik Aust: Conceptualization, Software, Supervision, Validation, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; Maarten Marsman: Funding Acquisition, Conceptualization, Methodology, Supervision, Validation, Writing - Review & Editing; Frantisek Bartos: Software; Eric-Jan Wagenmakers: Funding Acquisition, Methodology, Supervision, Validation, Writing - Review & Editing; Julia M. Haaf: Conceptualization, Formal Analysis, Methodology, Software, Supervision, Validation, Writing - Original Draft Preparation, Writing - Review & Editing.

## Conflicts of interest
The authors declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## Ethical Approval
This is a methodological contribution which requires no ethical approval.

\clearpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

\clearpage

```{r echo = FALSE, results = 'asis', cache = FALSE, child = "Rpackage_appendix.Rmd"}
# papaja::render_appendix('Rpackage_appendix.Rmd')
```

